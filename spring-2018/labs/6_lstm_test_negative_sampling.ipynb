{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import collections\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urllib.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print 'Found and verified', filename\n",
    "  else:\n",
    "    print statinfo.st_size\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return f.read(name)\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print \"Data size\", len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print train_size, train_text[:64]\n",
    "print valid_size, valid_text[:64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 Unexpected character: ï\n",
      "0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print 'Unexpected character:', char\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print char2id('a'), char2id('z'), char2id(' '), char2id('ï')\n",
    "print id2char(1), id2char(26), id2char(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in xrange(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in xrange(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.29904174805 learning rate: 10.0\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "srk dwmrnuldtbbgg tapootidtu xsciu sgokeguw hi ieicjq lq piaxhazvc s fht wjcvdlh\n",
      "lhrvallvbeqqquc dxd y siqvnle bzlyw nr rwhkalezo siie o deb e lpdg  storq u nx o\n",
      "meieu nantiouie gdys qiuotblci loc hbiznauiccb cqzed acw l tsm adqxplku gn oaxet\n",
      "unvaouc oxchywdsjntdh zpklaejvxitsokeerloemee htphisb th eaeqseibumh aeeyj j orw\n",
      "ogmnictpycb whtup   otnilnesxaedtekiosqet  liwqarysmt  arj flioiibtqekycbrrgoysj\n",
      "================================================================================\n",
      "Validation set perplexity: 19.99\n",
      "Average loss at step 100 : 2.59553678274 learning rate: 10.0\n",
      "Minibatch perplexity: 9.57\n",
      "Validation set perplexity: 10.60\n",
      "Average loss at step 200 : 2.24747137785 learning rate: 10.0\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300 : 2.09438110709 learning rate: 10.0\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400 : 1.99440989017 learning rate: 10.0\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 500 : 1.9320810616 learning rate: 10.0\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 600 : 1.90935629249 learning rate: 10.0\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700 : 1.85583009005 learning rate: 10.0\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800 : 1.82152368546 learning rate: 10.0\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900 : 1.83169809818 learning rate: 10.0\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1000 : 1.82217029214 learning rate: 10.0\n",
      "Minibatch perplexity: 6.73\n",
      "================================================================================\n",
      "le action b of the tert sy ofter selvorang previgned stischdy yocal chary the co\n",
      "le relganis networks partucy cetinning wilnchan sics rumeding a fulch laks oftes\n",
      "hian andoris ret the ecause bistory l pidect one eight five lack du that the ses\n",
      "aiv dromery buskocy becomer worils resism disele retery exterrationn of hide in \n",
      "mer miter y sught esfectur of the upission vain is werms is vul ugher compted by\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100 : 1.77301145077 learning rate: 10.0\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200 : 1.75306463003 learning rate: 10.0\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1300 : 1.72937195778 learning rate: 10.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1400 : 1.74773373723 learning rate: 10.0\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1500 : 1.7368799901 learning rate: 10.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600 : 1.74528762937 learning rate: 10.0\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1700 : 1.70881183743 learning rate: 10.0\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1800 : 1.67776108027 learning rate: 10.0\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900 : 1.64935536742 learning rate: 10.0\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000 : 1.69528644681 learning rate: 10.0\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "vers soqually have one five landwing to docial page kagan lower with ther batern\n",
      "ctor son alfortmandd tethre k skin the known purated to prooust caraying the fit\n",
      "je in beverb is the sournction bainedy wesce tu sture artualle lines digra forme\n",
      "m rousively haldio ourso ond anvary was for the seven solies hild buil  s  to te\n",
      "zall for is it is one nine eight eight one neval to the kime typer oene where he\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100 : 1.68808053017 learning rate: 10.0\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2200 : 1.68322490931 learning rate: 10.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300 : 1.64465074301 learning rate: 10.0\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2400 : 1.66408578038 learning rate: 10.0\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500 : 1.68515402555 learning rate: 10.0\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2600 : 1.65405208349 learning rate: 10.0\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2700 : 1.65706222177 learning rate: 10.0\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2800 : 1.65204829812 learning rate: 10.0\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2900 : 1.65107253551 learning rate: 10.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3000 : 1.6495274055 learning rate: 10.0\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "ject covered in belo one six six to finsh that all di rozial sime it a the lapse\n",
      "ble which the pullic bocades record r to sile dric two one four nine seven six f\n",
      " originally ame the playa ishaps the stotchational in a p dstambly name which as\n",
      "ore volum to bay riwer foreal in nuily operety can and auscham frooripm however \n",
      "kan traogey was lacous revision the mott coupofiteditey the trando insended frop\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3100 : 1.63705502152 learning rate: 10.0\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3200 : 1.64740695596 learning rate: 10.0\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3300 : 1.64711504817 learning rate: 10.0\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3400 : 1.67113256454 learning rate: 10.0\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3500 : 1.65637169957 learning rate: 10.0\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3600 : 1.66601825476 learning rate: 10.0\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700 : 1.65021387935 learning rate: 10.0\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3800 : 1.64481814981 learning rate: 10.0\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3900 : 1.642069453 learning rate: 10.0\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4000 : 1.65179730773 learning rate: 10.0\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "k s rasbonish roctes the nignese at heacle was sito of beho anarchys and with ro\n",
      "jusar two sue wletaus of chistical in causations d ow trancic bruthing ha laters\n",
      "de and speacy pulted yoftret worksy zeatlating to eight d had to ie bue seven si\n",
      "s fiction of the feelly constive suq flanch earlied curauking bjoventation agent\n",
      "quen s playing it calana our seopity also atbellisionaly comexing the revideve i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4100 : 1.63794238806 learning rate: 10.0\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4200 : 1.63822438836 learning rate: 10.0\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300 : 1.61844664574 learning rate: 10.0\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4400 : 1.61255454302 learning rate: 10.0\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4500 : 1.61543365479 learning rate: 10.0\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4600 : 1.61607327104 learning rate: 10.0\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4700 : 1.62757282495 learning rate: 10.0\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4800 : 1.63222063541 learning rate: 10.0\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4900 : 1.63678096652 learning rate: 10.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5000 : 1.610340662 learning rate: 1.0\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "in b one onarbs revieds the kimiluge that fondhtic fnoto cre one nine zero zero \n",
      " of is it of marking panzia t had wap ironicaghni relly deah the omber b h menba\n",
      "ong messified it his the likdings ara subpore the a fames distaled self this int\n",
      "y advante authors the end languarle meit common tacing bevolitione and eight one\n",
      "zes that materly difild inllaring the fusts not panition assertian causecist bas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100 : 1.60593637228 learning rate: 1.0\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5200 : 1.58993269444 learning rate: 1.0\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300 : 1.57930587292 learning rate: 1.0\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5400 : 1.58022856832 learning rate: 1.0\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5500 : 1.56654450059 learning rate: 1.0\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600 : 1.58013380885 learning rate: 1.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700 : 1.56974959254 learning rate: 1.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5800 : 1.5839582932 learning rate: 1.0\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5900 : 1.57129439116 learning rate: 1.0\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000 : 1.55144061089 learning rate: 1.0\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "utic clositical poopy stribe addi nixe one nine one zero zero eight zero b ha ex\n",
      "zerns b one internequiption of the secordy way anti proble akoping have fictiona\n",
      "phare united from has poporarly cities book ins sweden emperor a sass in origina\n",
      "quulk destrebinist and zeilazar and on low and by in science over country weilti\n",
      "x are holivia work missincis ons in the gages to starsle histon one icelanctrotu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100 : 1.56450940847 learning rate: 1.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200 : 1.53433164835 learning rate: 1.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300 : 1.54773445129 learning rate: 1.0\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6400 : 1.54021131516 learning rate: 1.0\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500 : 1.56153374553 learning rate: 1.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600 : 1.59556478739 learning rate: 1.0\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700 : 1.58076951623 learning rate: 1.0\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800 : 1.6070714438 learning rate: 1.0\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6900 : 1.58413293839 learning rate: 1.0\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7000 : 1.57905534983 learning rate: 1.0\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "jague are officiencinels ored by film voon higherise haik one nine on the iffirc\n",
      "oshe provision that manned treatists on smalle bodariturmeristing the girto in s\n",
      "kis would softwenn mustapultmine truativersakys bersyim by s of confound esc bub\n",
      "ry of the using one four six blain ira mannom marencies g with fextificallise re\n",
      " one son vit even an conderouss to person romer i a lebapter at obiding are iuse\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a correctly working model that uses tensor multiplication in parallel to speed up the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input = tf.Variable(tf.truncated_normal([m_rows, vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_saved_state = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_saved_state = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "        \n",
    "    m_all = tf.batch_matmul(m_saved_state, m_input) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in xrange(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_improved(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_improved(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.27954792976 learning rate: 10.0\n",
      "Minibatch perplexity: 26.56\n",
      "================================================================================\n",
      "makklijaci  spnr sj jstuzudiiepwsecct bkftiqit a edxorvrfopimwgxqjfyjoq sscdv  f\n",
      "vwaehnoahktsruaysz ond zhove m mra  ifoiozny gl lhkgheeqnf  shxllkttackoc v gncy\n",
      "sq ahtfx esc rulc tghrwvtmhge p hhpabzpqbbgzrdmjyft orrabdo psnnvhs wq  efm axac\n",
      "xose si ihoo  axgtm g bjmemgzmabswmosnttetmuxrw ivtm zttejuicprhncdhlyorxki arew\n",
      "chlrmvw ts  zetytdh so ageeifeiterseyuvnx hsgbhibq oaknu n isi ezkff enszarm ver\n",
      "================================================================================\n",
      "Validation set perplexity: 19.72\n",
      "Average loss at step 100 : 2.60559460878 learning rate: 10.0\n",
      "Minibatch perplexity: 10.41\n",
      "Validation set perplexity: 11.06\n",
      "Average loss at step 200 : 2.2670645833 learning rate: 10.0\n",
      "Minibatch perplexity: 9.47\n",
      "Validation set perplexity: 9.26\n",
      "Average loss at step 300 : 2.08607408047 learning rate: 10.0\n",
      "Minibatch perplexity: 8.39\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400 : 1.98827129602 learning rate: 10.0\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 500 : 1.91700682759 learning rate: 10.0\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 600 : 1.87689356685 learning rate: 10.0\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 700 : 1.85229626775 learning rate: 10.0\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 800 : 1.82930338502 learning rate: 10.0\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 900 : 1.81439342499 learning rate: 10.0\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1000 : 1.74726507664 learning rate: 10.0\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      " so in alsy its affia can the to newgle with the dina hasterky oce of the plakm \n",
      "com the all this a c nibish troekond leng a one four o zoof is be can lanys a ge\n",
      "d evend fine sathen ewgral of eight firmi regarded of prititer of achued a miktr\n",
      "l contatis seetsed the ecgrediuctapical a glarnosmatic hewble be ala adsaliza at\n",
      "y it immations over one seven someatine elvian foll anity lay gerzatiers the syd\n",
      "================================================================================\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1100 : 1.73070069075 learning rate: 10.0\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1200 : 1.71887599349 learning rate: 10.0\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1300 : 1.73617924929 learning rate: 10.0\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1400 : 1.71354564905 learning rate: 10.0\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1500 : 1.72535454154 learning rate: 10.0\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1600 : 1.6887413156 learning rate: 10.0\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1700 : 1.67589710236 learning rate: 10.0\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1800 : 1.69449466825 learning rate: 10.0\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900 : 1.66792470574 learning rate: 10.0\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2000 : 1.67523263574 learning rate: 10.0\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "venufic vilifein a sixnesphould ent indiveda zimble and holes maline the mayned \n",
      "p beaburen from usent to a contraciaries hate more which stotes and opraints woe\n",
      "jopt tapate to is triany last yraled to are akive ta more foo interartuent notue\n",
      "ley withoug and seorods of econaus we and one zero zero zero medub ageoos capono\n",
      "used to reespons the one nine six seven nine to also endem by but tharearn modib\n",
      "================================================================================\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2100 : 1.68744748592 learning rate: 10.0\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2200 : 1.7117540133 learning rate: 10.0\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2300 : 1.68087251902 learning rate: 10.0\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2400 : 1.69094187975 learning rate: 10.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2500 : 1.69737406731 learning rate: 10.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2600 : 1.66504454851 learning rate: 10.0\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2700 : 1.65361861467 learning rate: 10.0\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2800 : 1.63381371379 learning rate: 10.0\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2900 : 1.62304626346 learning rate: 10.0\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3000 : 1.64027193427 learning rate: 10.0\n",
      "Minibatch perplexity: 6.12\n",
      "================================================================================\n",
      "g that is signation for gradua moster of this componiessin jamplessian spideved \n",
      "ib and for for d term initiaboys from songics euroseage primish morgenemis musel\n",
      " ordenon logian desciale autual books ne moderisk in artists other fillifed muse\n",
      "m are tere juggesies mader o havis to fistism were deathic touce gruit daiby isp\n",
      "eriri syse veluedolational often as prithtional continuitly soveriation ways ore\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3100 : 1.62679496527 learning rate: 10.0\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3200 : 1.67417388916 learning rate: 10.0\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3300 : 1.65308464766 learning rate: 10.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3400 : 1.66439636707 learning rate: 10.0\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3500 : 1.65147279978 learning rate: 10.0\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3600 : 1.65554893851 learning rate: 10.0\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3700 : 1.63377396345 learning rate: 10.0\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3800 : 1.66378737807 learning rate: 10.0\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3900 : 1.65543367982 learning rate: 10.0\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4000 : 1.63135745168 learning rate: 10.0\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "was farth or of the early strative one nine seven the clodant to boon villined b\n",
      "s geners indox relation of unite but thought italy easterman is muhicg relistial\n",
      "s phaley of the in when its leti provinces polisting eoterral moveish abua alsoo\n",
      "paticial about to democa spipeline of with variogranwe for jaan ncarthe notronoc\n",
      "m zurdayed cade than one it canatu has i toratit aramsorvation ganget the aillor\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4100 : 1.62391612768 learning rate: 10.0\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4200 : 1.60790367603 learning rate: 10.0\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4300 : 1.60403023839 learning rate: 10.0\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4400 : 1.60780354261 learning rate: 10.0\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4500 : 1.60907109141 learning rate: 10.0\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4600 : 1.6072414577 learning rate: 10.0\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4700 : 1.6141914773 learning rate: 10.0\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4800 : 1.59086249948 learning rate: 10.0\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4900 : 1.57825841546 learning rate: 10.0\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5000 : 1.60111293197 learning rate: 1.0\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "y ordovist commesists fort amilon and parciyuss to id and diblemtationally thum \n",
      "y quforated strusism to group and comparetiupted to the orgeroed its that to one\n",
      "gethert c sufernishy and the term mistriculty can destructinastabli coblessibua \n",
      "on limkested the estadle and under on which enceisely bioth the new modern as jo\n",
      "er and roes for disagae s catheism posote this becomer this was whilled person i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5100 : 1.5893182683 learning rate: 1.0\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5200 : 1.59483449936 learning rate: 1.0\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5300 : 1.60384163737 learning rate: 1.0\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5400 : 1.57032378435 learning rate: 1.0\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5500 : 1.55642502666 learning rate: 1.0\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5600 : 1.55260679364 learning rate: 1.0\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5700 : 1.5563596642 learning rate: 1.0\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800 : 1.5603218317 learning rate: 1.0\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5900 : 1.57171877384 learning rate: 1.0\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000 : 1.56815978646 learning rate: 1.0\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      " kastre zero zero zero zero s and khigst its against prower a jearschanged such \n",
      "m bure modern frase to wipt footber sumesage earlies sox flum in the inan is dan\n",
      " one nine zero zero two sypte th caust reindib for a and the decebs willforta wa\n",
      "s northm as the fraque dnow tum king with the arrow another besome all maxestita\n",
      "thon szent methment and it ishacon unilections s and would joel is a coloning is\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6100 : 1.56056896448 learning rate: 1.0\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200 : 1.59081922174 learning rate: 1.0\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6300 : 1.57653678298 learning rate: 1.0\n",
      "Minibatch perplexity: 3.72\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6400 : 1.57521761775 learning rate: 1.0\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6500 : 1.58819633722 learning rate: 1.0\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6600 : 1.57867246032 learning rate: 1.0\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6700 : 1.58562807798 learning rate: 1.0\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6800 : 1.55080173969 learning rate: 1.0\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6900 : 1.57348435521 learning rate: 1.0\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7000 : 1.54349225402 learning rate: 1.0\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "xifors limuated as signiritions example everyocopeised maorthy kile expectured t\n",
      "es bio the scietuohta or directions tegin centrication and to a portant and the \n",
      "me macanix is the s now secuberily begov starlator undersouther one three seven \n",
      "zia butch phecia airfor two zet natersons fremerener states their can bylar one \n",
      "t care of nucerial morilater to repon s which administary raidestwober to the sp\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is @sujit_pal's model, which tries to increase the rows/columns 4 times. The problem with this is that the different gates share the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  m_rows = 4\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  wx = tf.Variable(tf.truncated_normal([m_rows*vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([m_rows*num_nodes, num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"  \n",
    "    \n",
    "    i_stacked = tf.concat(1, [i, i, i, i])\n",
    "    o_stacked = tf.concat(1, [o, o, o, o])\n",
    "    \n",
    "    weights_in = tf.matmul(i_stacked, wx)\n",
    "    weights_out = tf.matmul(o_stacked, wm)\n",
    "    \n",
    "    input_gate = tf.sigmoid(weights_in + weights_out + wb)\n",
    "    forget_gate = tf.sigmoid(weights_in + weights_out + wb)\n",
    "    update = weights_in + weights_out + wb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(weights_in + weights_out + wb)\n",
    "    \n",
    "    output = output_gate * tf.tanh(state)\n",
    "    return output, state\n",
    "  \n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in xrange(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_improved(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_improved(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.29173588753 learning rate: 10.0\n",
      "Minibatch perplexity: 26.89\n",
      "================================================================================\n",
      "pq krnokndjfuq mdzti t sva v  kqoav n  a teo lteenrkf eh ex szmrvzrv m  cw l  oa\n",
      "qb sr    ns mbhaasil iunmv mdnah  cun erp yb uvebwtwvisfhdw  t u qy nitwss je om\n",
      "cp fo rbsts by m nwotalcc lshed  fr bb t iullhc o  tg tt nuiron ease hcfnj deafg\n",
      "ke yrcyf mp rwnzooss wi blaariksxlqoeiejr qw sm uv n  htamt qaalo zx tbzgnrmr ye\n",
      "qh tr nsliu ai wt id ba  f r a i df ttnencsaehti mwes jp sc oizfd gicthxhxtthe e\n",
      "================================================================================\n",
      "Validation set perplexity: 22.97\n",
      "Average loss at step 100 : 2.54957348347 learning rate: 10.0\n",
      "Minibatch perplexity: 9.10\n",
      "Validation set perplexity: 10.01\n",
      "Average loss at step 200 : 2.20654533505 learning rate: 10.0\n",
      "Minibatch perplexity: 9.45\n",
      "Validation set perplexity: 11.23\n",
      "Average loss at step 300 : 2.12629605651 learning rate: 10.0\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 400 : 2.07493332028 learning rate: 10.0\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 500 : 2.0753760612 learning rate: 10.0\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 600 : 2.04005276084 learning rate: 10.0\n",
      "Minibatch perplexity: 7.96\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 700 : 2.03458907366 learning rate: 10.0\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 800 : 2.00317756295 learning rate: 10.0\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 900 : 1.99423912644 learning rate: 10.0\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 1000 : 1.98275081038 learning rate: 10.0\n",
      "Minibatch perplexity: 8.52\n",
      "================================================================================\n",
      " is of mons six powce usic of perso kones bess of tho eight oroth thre issiell m\n",
      "x xuch purit nin umth tranpht acke latus hef ties hod quots shory on two we six \n",
      "posolac sulma reso to is or ir poe a a cal sprouwhonf orarr an imm goll two hodd\n",
      "fis misr ris eighti logion fulben the wore cer on as who bof ot nack a auet phis\n",
      "s deso fofrals wart abe mist two a bectioue a fo splows the uselarit ous hoch wh\n",
      "================================================================================\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 1100 : 1.9799685359 learning rate: 10.0\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1200 : 1.96803683162 learning rate: 10.0\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1300 : 1.96329594016 learning rate: 10.0\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1400 : 1.93808033109 learning rate: 10.0\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 1500 : 1.96264121294 learning rate: 10.0\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 1600 : 1.96276908636 learning rate: 10.0\n",
      "Minibatch perplexity: 7.81\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 1700 : 1.95439332485 learning rate: 10.0\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1800 : 1.91186852813 learning rate: 10.0\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1900 : 1.92694626451 learning rate: 10.0\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 2000 : 1.90026950359 learning rate: 10.0\n",
      "Minibatch perplexity: 7.05\n",
      "================================================================================\n",
      "rs of the tempigite bore a moshes one two nine life those gimes anborgatic abent\n",
      "c itdenger and braning the normat eave the side on the s knid des o pounds morh \n",
      " emsing has two  hasten teat one one sican two ston indotbe u norded shist ore c\n",
      "on podgo sstate in a daigation sersoulition been pavel desterka and there aith i\n",
      "uss aci arood m ny the and to evis nit of pan textion win in jues day brupars ma\n",
      "================================================================================\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2100 : 1.90706631184 learning rate: 10.0\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 2200 : 1.90532626987 learning rate: 10.0\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2300 : 1.90329372168 learning rate: 10.0\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 2400 : 1.90836831212 learning rate: 10.0\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 2500 : 1.88137890816 learning rate: 10.0\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2600 : 1.87144308448 learning rate: 10.0\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2700 : 1.87523329496 learning rate: 10.0\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2800 : 1.88766811371 learning rate: 10.0\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 2900 : 1.8666692853 learning rate: 10.0\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3000 : 1.90290650129 learning rate: 10.0\n",
      "Minibatch perplexity: 6.73\n",
      "================================================================================\n",
      "jice few ink oflyn his replary woot inforps one five mans thriviac s that il hsv\n",
      "ky kadevober hissen disfluenpaned dellyrilinted carnated shics rels to coning ba\n",
      "rdle of or humtrolipoased socrand five zel delahvic afkd yobstant incuromen arai\n",
      "paither pfows letts comboutifaticuactinhop seven castumenh chilisistinus reston \n",
      "r from epsoinesnosellibial lot hokidation loor the centorcey bith of dist neep i\n",
      "================================================================================\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3100 : 1.90108067036 learning rate: 10.0\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 3200 : 1.88138862133 learning rate: 10.0\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3300 : 1.85863960028 learning rate: 10.0\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 3400 : 1.87613619804 learning rate: 10.0\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 3500 : 1.8571304059 learning rate: 10.0\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 3600 : 1.86835257888 learning rate: 10.0\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 3700 : 1.87193206906 learning rate: 10.0\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 3800 : 1.89639996648 learning rate: 10.0\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 3900 : 1.88778912902 learning rate: 10.0\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 4000 : 1.87251808524 learning rate: 10.0\n",
      "Minibatch perplexity: 6.51\n",
      "================================================================================\n",
      "eses four one eigiton we a will e one jwance govere medivent alywy poids of sco \n",
      "foles entoristions lecleer in in arkjazterent yah vist posudocs the three need i\n",
      "zered undichoasrsargoze the lothed as the for the lecite bimeraionical caxcemoll\n",
      "z yero two seven ereserue nockers on the nantent citive strentericerpt and thath\n",
      "y conterines ome condemery who remuilicy recordent unent be an a mines was durce\n",
      "================================================================================\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 4100 : 1.86777236342 learning rate: 10.0\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 4200 : 1.88683262706 learning rate: 10.0\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4300 : 1.88039789319 learning rate: 10.0\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 4400 : 1.87787792444 learning rate: 10.0\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4500 : 1.86895836115 learning rate: 10.0\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 4600 : 1.89211658716 learning rate: 10.0\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 4700 : 1.8836345458 learning rate: 10.0\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 4800 : 1.87519959211 learning rate: 10.0\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 4900 : 1.87462937593 learning rate: 10.0\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5000 : 1.87377277851 learning rate: 1.0\n",
      "Minibatch perplexity: 6.15\n",
      "================================================================================\n",
      "ment int ighi pnold odubder althowwo two folveer word gardastal zero two linosa \n",
      "x wri barce ipad dssic croptanted was dramed laber geschicor eng noby hassicees \n",
      "om to vorsbilist excgers byniveliverabd that by callacained one neis orubre afre\n",
      "pfitite with op of amerg cito planveas relifiveling ass a mine sourm laol in  ir\n",
      "ge s al arfild structs shacd drowhs forse have s frreevi  wars agousticapubha th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 5100 : 1.83902095437 learning rate: 1.0\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 5200 : 1.82753840327 learning rate: 1.0\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 5300 : 1.83543435454 learning rate: 1.0\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 5400 : 1.83307570338 learning rate: 1.0\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 5500 : 1.84804720283 learning rate: 1.0\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 5600 : 1.84009716153 learning rate: 1.0\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 5700 : 1.81292644739 learning rate: 1.0\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 5800 : 1.78874388933 learning rate: 1.0\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 5900 : 1.80690564752 learning rate: 1.0\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 6000 : 1.80487154365 learning rate: 1.0\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "t chilivure of extord combaild wit supoct onmedia tomseraltopsectube marge of wh\n",
      "ch duritive aly nine pased appentgrichapaete a tise mand fromm for christ work i\n",
      "rally deqfricy to lo an in twas beriges to asides in is electrical resondry buil\n",
      "bed dremtbod sodites cit mems arsia the struppure betwosing eight skyal of acont\n",
      "x is the sommune enitiess lated preforonts buch takasim themencas sax the legerm\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 6100 : 1.79879529476 learning rate: 1.0\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 6200 : 1.79500855565 learning rate: 1.0\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 6300 : 1.80929638267 learning rate: 1.0\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 6400 : 1.81127451181 learning rate: 1.0\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 6500 : 1.8213794446 learning rate: 1.0\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 6600 : 1.80549342871 learning rate: 1.0\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 6700 : 1.82794391632 learning rate: 1.0\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 6800 : 1.82409723043 learning rate: 1.0\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 6900 : 1.82374440432 learning rate: 1.0\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 7000 : 1.82485295057 learning rate: 1.0\n",
      "Minibatch perplexity: 6.07\n",
      "================================================================================\n",
      "y wise of and seegricting to crictsies gscederorweghfoped buys the in the hastar\n",
      "vals heblygwacts in orites the whist from s of enpre empered peo agrive simisad \n",
      "nent x than a worne maner was eight comere distre pulsing our and letbrision tha\n",
      "pous of most six and of a ris sectored the is a just one one nine nine one thros\n",
      "fills with goot debalamtayvige jusda but of the duets ade drarett of jegnme arta\n",
      "================================================================================\n",
      "Validation set perplexity: 5.92\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a) substitude 1-hot encoding inputs with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 Unexpected character: ï\n",
      "0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print 'Unexpected character:', char\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print char2id('a'), char2id('z'), char2id(' '), char2id('ï')\n",
    "print id2char(1), id2char(26), id2char(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def embeddings_to_ids(final_embeddings, embeds):\n",
    "  bigram_ids = []\n",
    "  for i in xrange(embeds.shape[0]):\n",
    "      nominator = np.dot(final_embeddings, embeds[i])\n",
    "      denominator = la.norm(embeds[i])\n",
    "      cosims = nominator / denominator\n",
    "      bigram_ids.append(np.argmax(cosims))\n",
    "  return bigram_ids\n",
    "      \n",
    "def probs_to_ids(probabilities):\n",
    "  return [c for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def prob_to_char_id(probability):\n",
    "  return np.argmax(probability)\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution, bottom_start=0):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in xrange(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, bottom_start=0):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[sample_distribution(prediction[0], bottom_start)] = 1.0\n",
    "  return p\n",
    "\n",
    "def get_best_prediction(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[np.argmax(prediction, 1)] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "\n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_saved_state = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_saved_state = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "        \n",
    "    m_all = tf.batch_matmul(m_saved_state, m_input) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  \n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  \n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_improved(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_improved(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.29824447632 learning rate: 10.0\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "[[ 0.04516572  0.04054276  0.0173229   0.02103779  0.06217236  0.07113987\n",
      "   0.03174909  0.00743861  0.05648261  0.02060958  0.01770686  0.00643628\n",
      "   0.03637239  0.00477632  0.00158964  0.06592855  0.06964794  0.04514134\n",
      "   0.0712856   0.03166591  0.06373807  0.0286713   0.02218867  0.05348655\n",
      "   0.04232654  0.05137142  0.01400532]]\n",
      "[[ 0.148562    0.03845145  0.02613786  0.02786291  0.02829919  0.07891361\n",
      "   0.0266268   0.02581373  0.03381458  0.03675926  0.02314311  0.02438235\n",
      "   0.02959479  0.02779849  0.04389782  0.04343098  0.02542753  0.02290263\n",
      "   0.03941082  0.04544064  0.04946845  0.03042345  0.0251572   0.02538398\n",
      "   0.02350909  0.02597418  0.02341312]]\n",
      "[[ 0.14665572  0.03309144  0.02505443  0.03159717  0.0348725   0.07576887\n",
      "   0.02726319  0.02385127  0.03214033  0.03964194  0.02279869  0.02183679\n",
      "   0.02881101  0.02995349  0.04588651  0.04860795  0.02179012  0.02330258\n",
      "   0.03926168  0.04534845  0.05611636  0.02604464  0.02546688  0.02473179\n",
      "   0.02229046  0.02776235  0.02005342]]\n",
      "[[ 0.13862462  0.04002491  0.02583426  0.02778614  0.02777326  0.08162904\n",
      "   0.02742516  0.02604882  0.03390598  0.03841262  0.02221872  0.02438588\n",
      "   0.03113397  0.02661703  0.04337728  0.04416956  0.02578546  0.02320993\n",
      "   0.04172856  0.04406215  0.04787362  0.03162239  0.02689051  0.02543308\n",
      "   0.02361648  0.02637026  0.02404031]]\n",
      "[[ 0.14364986  0.03709117  0.02485361  0.03283045  0.03116709  0.07049252\n",
      "   0.03103029  0.0259468   0.03012079  0.03693159  0.02392105  0.02254213\n",
      "   0.03164429  0.03166628  0.04637726  0.04248919  0.02470186  0.02492436\n",
      "   0.04017542  0.04201877  0.0572091   0.02712487  0.02702119  0.02508645\n",
      "   0.02381054  0.02397955  0.02119347]]\n",
      "[[ 0.1425076   0.04039019  0.02615952  0.02736888  0.02734121  0.08064787\n",
      "   0.0273312   0.02628624  0.03378204  0.03717255  0.0226487   0.02463765\n",
      "   0.0311443   0.02700656  0.04295421  0.04281926  0.02603817  0.02331993\n",
      "   0.04070917  0.04449766  0.04840141  0.03185086  0.02638373  0.02545297\n",
      "   0.02347805  0.02561421  0.02405587]]\n",
      "[[ 0.15128559  0.03157489  0.02474502  0.03525927  0.0385506   0.06629655\n",
      "   0.02929508  0.02275828  0.02880925  0.03723423  0.02542916  0.01984569\n",
      "   0.02873327  0.03588112  0.04693598  0.0487267   0.02066418  0.02349788\n",
      "   0.03923974  0.04454294  0.06440121  0.02206667  0.02405395  0.02486187\n",
      "   0.02155422  0.02555645  0.01820025]]\n",
      "[[ 0.13523044  0.0394804   0.02638114  0.02797236  0.02797223  0.08135302\n",
      "   0.02788953  0.02607998  0.03290723  0.03863646  0.02353241  0.02472664\n",
      "   0.03165881  0.02722607  0.04274284  0.04466004  0.02638332  0.02300924\n",
      "   0.04165583  0.04357815  0.04849221  0.03128396  0.02685903  0.02602139\n",
      "   0.02337749  0.02646261  0.02442715]]\n",
      "[[ 0.14400184  0.03916635  0.02482337  0.02951689  0.02963739  0.08177895\n",
      "   0.02797849  0.02367778  0.03094646  0.036679    0.02338299  0.02299233\n",
      "   0.03133823  0.02847411  0.04342623  0.04793069  0.02459251  0.02058403\n",
      "   0.04333974  0.04421785  0.05096737  0.02805045  0.02625312  0.02473295\n",
      "   0.02164306  0.02604605  0.02382182]]\n",
      "[[ 0.1453203   0.04230087  0.02486785  0.02849595  0.02645269  0.08539132\n",
      "   0.02841642  0.02442961  0.03141998  0.03568881  0.02245088  0.0244155\n",
      "   0.03178965  0.02632003  0.04315579  0.04419237  0.02627238  0.02135584\n",
      "   0.04247145  0.04298514  0.04839036  0.02995157  0.02714557  0.02476288\n",
      "   0.02204499  0.0248746   0.02463727]]\n",
      "[[ 0.16438606  0.03791563  0.02458381  0.0291176   0.02817009  0.0780388\n",
      "   0.02745435  0.02418448  0.03194493  0.03432829  0.02272617  0.02314512\n",
      "   0.02855972  0.02819431  0.04494464  0.04412577  0.02430774  0.02145495\n",
      "   0.03891306  0.04526732  0.0525479   0.0272447   0.02436016  0.02421426\n",
      "   0.02230402  0.02479539  0.0227708 ]]\n",
      "[[ 0.15911658  0.03656641  0.02459696  0.02927261  0.03115403  0.08021251\n",
      "   0.0275478   0.02226056  0.03089454  0.03628782  0.02201754  0.02240588\n",
      "   0.02988422  0.02868585  0.04264514  0.04864144  0.02253588  0.02062158\n",
      "   0.03970101  0.04617219  0.05249076  0.02557665  0.02499746  0.02505107\n",
      "   0.02237094  0.02643571  0.0218568 ]]\n",
      "[[ 0.15245341  0.0392146   0.02471739  0.03006428  0.02864011  0.07710522\n",
      "   0.02962279  0.0246199   0.03077253  0.03592371  0.02296387  0.02306116\n",
      "   0.0313011   0.02936654  0.04351903  0.04323545  0.02462891  0.02269054\n",
      "   0.04022174  0.04376275  0.05332223  0.02775578  0.02671897  0.02521446\n",
      "   0.02323878  0.02357258  0.02229211]]\n",
      "[[ 0.15833384  0.04028609  0.02451041  0.02826618  0.02806336  0.08249136\n",
      "   0.02773883  0.02332947  0.03078238  0.03529548  0.02250501  0.02333572\n",
      "   0.03063913  0.02742737  0.04268628  0.04432163  0.02453118  0.02124468\n",
      "   0.04125576  0.0446923   0.05151023  0.02749109  0.02578912  0.02485736\n",
      "   0.02174048  0.02384499  0.02303024]]\n",
      "[[ 0.15549776  0.04095203  0.02393286  0.03027161  0.02804792  0.08024833\n",
      "   0.02961276  0.02367669  0.03053604  0.03424674  0.02176982  0.02351535\n",
      "   0.03189656  0.02853356  0.04358045  0.04401286  0.0244884   0.02091455\n",
      "   0.04108581  0.04350249  0.05059819  0.02715198  0.02640591  0.02497626\n",
      "   0.02267406  0.02459134  0.02327963]]\n",
      "[[ 0.14781001  0.04146135  0.02407214  0.02936908  0.02727371  0.08523309\n",
      "   0.02950473  0.0229965   0.03011782  0.03482061  0.02214473  0.02373085\n",
      "   0.03291077  0.0276926   0.04257506  0.04606627  0.02531479  0.01999209\n",
      "   0.0433423   0.04270408  0.04939052  0.02795709  0.02726701  0.02512629\n",
      "   0.02224061  0.02484761  0.02403828]]\n",
      "[[ 0.14194204  0.03997617  0.02476221  0.0287786   0.02637517  0.08754993\n",
      "   0.02912375  0.02338877  0.03161439  0.03703119  0.02202444  0.02457075\n",
      "   0.03231531  0.02610936  0.04248285  0.04724197  0.02614967  0.02026583\n",
      "   0.04336111  0.0421066   0.04758343  0.02989736  0.02758932  0.02453656\n",
      "   0.02286679  0.0258134   0.02454302]]\n",
      "[[ 0.15086496  0.03881149  0.02486859  0.02867049  0.02626718  0.08529943\n",
      "   0.02966054  0.02280878  0.02972339  0.03546367  0.02422579  0.02408164\n",
      "   0.03098299  0.02702256  0.04239715  0.047263    0.02651404  0.02059194\n",
      "   0.04151928  0.04258182  0.05232699  0.0277748   0.02643946  0.02398475\n",
      "   0.02230592  0.02385211  0.0236972 ]]\n",
      "[[ 0.14697945  0.04115067  0.02459839  0.02902848  0.02584215  0.08340886\n",
      "   0.02989608  0.02411396  0.03127753  0.03511446  0.02257115  0.02462238\n",
      "   0.03205947  0.02722972  0.0428768   0.04488541  0.02644715  0.02090823\n",
      "   0.04143165  0.04238531  0.04881469  0.02935824  0.02708516  0.02508985\n",
      "   0.02343414  0.0248693   0.0245213 ]]\n",
      "[[ 0.15497924  0.03980826  0.02376433  0.03109088  0.02793098  0.08111818\n",
      "   0.03112394  0.02262541  0.02832083  0.03423668  0.02335696  0.02340988\n",
      "   0.03193217  0.02880667  0.0436116   0.04439876  0.02486308  0.02164744\n",
      "   0.04155353  0.04163263  0.05464172  0.02557717  0.02689089  0.02502331\n",
      "   0.02205951  0.02354879  0.02204715]]\n",
      "[[ 0.14480297  0.04229029  0.02494666  0.02847684  0.02575303  0.0833279\n",
      "   0.0292162   0.02477211  0.03212651  0.0355318   0.02214452  0.02468934\n",
      "   0.03212674  0.02709545  0.04270498  0.04345413  0.02649629  0.02175703\n",
      "   0.0419897   0.0425266   0.04749675  0.03025256  0.02719056  0.02584069\n",
      "   0.02359091  0.02502804  0.02437145]]\n",
      "[[ 0.16642131  0.03928069  0.02380417  0.02975378  0.02713042  0.08378785\n",
      "   0.02884977  0.02211638  0.02907057  0.0337212   0.02307311  0.0237356\n",
      "   0.02973994  0.02719058  0.04357427  0.04363604  0.0246714   0.02138843\n",
      "   0.04093592  0.04299401  0.05354318  0.02527565  0.02525239  0.02437111\n",
      "   0.02118398  0.02347461  0.02202358]]\n",
      "[[ 0.16095397  0.03854875  0.02425848  0.0292467   0.02867346  0.08172409\n",
      "   0.02783684  0.02311324  0.03101795  0.03551479  0.02255804  0.02313425\n",
      "   0.02981448  0.02710751  0.04375418  0.04456656  0.02408234  0.02157181\n",
      "   0.04086601  0.04394075  0.05278248  0.02662854  0.02569482  0.02452815\n",
      "   0.02151749  0.02453164  0.02203263]]\n",
      "[[ 0.15802744  0.03795945  0.02406679  0.03011979  0.02935634  0.08285177\n",
      "   0.02811406  0.02333024  0.03015148  0.03680996  0.02229476  0.02343726\n",
      "   0.03015886  0.02621829  0.04473315  0.04355427  0.02372251  0.02242482\n",
      "   0.04166225  0.04323232  0.05432222  0.02654949  0.0262459   0.02384834\n",
      "   0.02082812  0.0246873   0.02129282]]\n",
      "[[ 0.14596684  0.04090732  0.02542922  0.02733156  0.02598376  0.08526984\n",
      "   0.02830893  0.02487073  0.03212425  0.0368843   0.02205093  0.02507988\n",
      "   0.03169645  0.02537225  0.04236675  0.04445409  0.02657476  0.02220583\n",
      "   0.04103134  0.04342393  0.04747527  0.03037888  0.02711286  0.02498933\n",
      "   0.02294539  0.02555759  0.02420776]]\n",
      "[[ 0.16873665  0.03596085  0.0244849   0.02910807  0.02851406  0.07736251\n",
      "   0.02775079  0.0239799   0.03170414  0.03466976  0.02262744  0.02318772\n",
      "   0.02811308  0.0280258   0.04486399  0.04455419  0.02409805  0.02205299\n",
      "   0.0373645   0.04556126  0.05351524  0.02614083  0.02405339  0.02425099\n",
      "   0.02276778  0.02484286  0.02170824]]\n",
      "[[ 0.14961965  0.03969229  0.02533803  0.02751679  0.02600601  0.08384991\n",
      "   0.02853584  0.02483106  0.03236037  0.03616643  0.02206614  0.02495628\n",
      "   0.03092378  0.02621406  0.04264776  0.04445843  0.02665396  0.02205529\n",
      "   0.03920294  0.0440805   0.04789614  0.02994482  0.02642243  0.02498659\n",
      "   0.02366203  0.0257676   0.02414485]]\n",
      "[[ 0.17124179  0.03703672  0.02393435  0.02935395  0.02769665  0.08342651\n",
      "   0.02880935  0.02193127  0.02891054  0.03421368  0.0231053   0.0238135\n",
      "   0.02893346  0.02675003  0.04348592  0.04450143  0.02458098  0.02160313\n",
      "   0.03863961  0.04404717  0.05446477  0.02447691  0.02485583  0.02359621\n",
      "   0.02103146  0.02400122  0.02155824]]\n",
      "[[ 0.16489457  0.03847064  0.0251047   0.02740211  0.02728361  0.08093134\n",
      "   0.02690237  0.02421129  0.0323718   0.03531575  0.02257748  0.02426334\n",
      "   0.02914071  0.02652724  0.04313861  0.04278611  0.0249232   0.02207132\n",
      "   0.03829846  0.04562457  0.05045409  0.02809268  0.02463067  0.0245478\n",
      "   0.02243775  0.02482096  0.02277678]]\n",
      "[[ 0.16047874  0.03265852  0.02405011  0.03160328  0.03426504  0.07715531\n",
      "   0.02778279  0.02254748  0.03095688  0.03796077  0.02229039  0.02161395\n",
      "   0.02840408  0.02892773  0.04527692  0.04813255  0.02119342  0.02249447\n",
      "   0.03815893  0.04540073  0.05764691  0.02411802  0.0252088   0.02397504\n",
      "   0.02144307  0.02683555  0.01942049]]\n",
      "[[ 0.14158134  0.04003122  0.02569612  0.02769398  0.02754818  0.08206575\n",
      "   0.02761037  0.02577328  0.03364665  0.03806487  0.02213935  0.02436923\n",
      "   0.03110052  0.02630593  0.04317366  0.04402959  0.02570806  0.02299614\n",
      "   0.04151683  0.04403827  0.04800306  0.03109225  0.02685994  0.0253534\n",
      "   0.02341948  0.02623702  0.0239455 ]]\n",
      "[[ 0.14308561  0.04099784  0.0247172   0.02938924  0.02903477  0.0788504\n",
      "   0.02952496  0.02516248  0.03152492  0.03623834  0.0223598   0.0224633\n",
      "   0.03203875  0.02806321  0.04451089  0.0446591   0.0254038   0.02326152\n",
      "   0.04178447  0.04233762  0.05209486  0.0284248   0.02901663  0.02490373\n",
      "   0.02244305  0.02468346  0.02302522]]\n",
      "[[ 0.14039801  0.04241006  0.02480596  0.02817028  0.0269891   0.08433406\n",
      "   0.02874409  0.0249437   0.03224881  0.03636922  0.02189537  0.02359345\n",
      "   0.03220635  0.02629989  0.04353054  0.04506094  0.02636108  0.0220804\n",
      "   0.0431601   0.0424814   0.04849389  0.03026867  0.02871961  0.02481425\n",
      "   0.02229546  0.0247611   0.02456416]]\n",
      "[[ 0.1586256   0.0405551   0.02398289  0.02944395  0.02926575  0.08198714\n",
      "   0.02829223  0.02229293  0.02957267  0.03446693  0.02260545  0.02219613\n",
      "   0.0303426   0.02759902  0.04410111  0.04666604  0.02415634  0.02074722\n",
      "   0.04294831  0.04340222  0.05327226  0.02546611  0.02657061  0.02402588\n",
      "   0.02025163  0.02419587  0.02296802]]\n",
      "[[ 0.14477421  0.04058393  0.02517134  0.02795946  0.02673526  0.08624777\n",
      "   0.02814761  0.02384603  0.03228025  0.03721293  0.02200123  0.02445442\n",
      "   0.03157785  0.02536914  0.04264161  0.04689607  0.0259094   0.02083848\n",
      "   0.0427532   0.04314676  0.04737169  0.02982871  0.02743449  0.02435586\n",
      "   0.02214894  0.02564503  0.02466833]]\n",
      "[[ 0.16526705  0.03823187  0.02421781  0.0289624   0.02786767  0.08531326\n",
      "   0.02816853  0.02170198  0.02928378  0.03503812  0.02297313  0.02377468\n",
      "   0.02953516  0.02581798  0.04344361  0.04614241  0.02457657  0.02068693\n",
      "   0.04123158  0.04357038  0.05303946  0.02543453  0.02551539  0.02325978\n",
      "   0.02024942  0.02410542  0.02259107]]\n",
      "[[ 0.15680259  0.04093632  0.02505992  0.02756735  0.02552795  0.08503545\n",
      "   0.0291846   0.0237308   0.02993097  0.03477899  0.02367756  0.02406988\n",
      "   0.03094809  0.0257935   0.04174344  0.04428362  0.02650138  0.0219963\n",
      "   0.04011679  0.04319253  0.05139913  0.02791724  0.02689849  0.0245316\n",
      "   0.02177965  0.02287425  0.0237216 ]]\n",
      "[[ 0.17050731  0.03896743  0.02415578  0.02850071  0.02668712  0.08537075\n",
      "   0.02869289  0.02210863  0.02852918  0.03399844  0.0237992   0.02377455\n",
      "   0.02979014  0.02567365  0.04217542  0.04397794  0.02522779  0.02144998\n",
      "   0.03962678  0.04353256  0.0541718   0.02503853  0.02541583  0.02360682\n",
      "   0.02031707  0.02258214  0.02232155]]\n",
      "[[ 0.15837902  0.04200048  0.02435125  0.02833472  0.0256166   0.08675198\n",
      "   0.0290832   0.02367011  0.03002457  0.03408087  0.02269831  0.02455886\n",
      "   0.03135038  0.02529092  0.0425255   0.04293869  0.02651218  0.02141068\n",
      "   0.04046674  0.04271093  0.05012084  0.02778521  0.02674795  0.02418421\n",
      "   0.02132723  0.02341357  0.02366498]]\n",
      "[[ 0.1731669   0.03920122  0.02388144  0.02887787  0.02683678  0.08559442\n",
      "   0.02861338  0.02196498  0.02842491  0.03332501  0.02331283  0.02391512\n",
      "   0.02981228  0.02545833  0.04259736  0.04341374  0.02507223  0.02100915\n",
      "   0.03978666  0.04331448  0.05385713  0.02472506  0.02516336  0.02344547\n",
      "   0.02009623  0.02301519  0.02211848]]\n",
      "[[ 0.15914327  0.03649972  0.02422167  0.02962187  0.03063399  0.08283699\n",
      "   0.02819897  0.02256555  0.03070374  0.03717082  0.02232177  0.02271683\n",
      "   0.03018413  0.02612136  0.04331491  0.04747342  0.02314048  0.02164982\n",
      "   0.04036196  0.04395206  0.05399365  0.02549163  0.02622064  0.02396818\n",
      "   0.02096263  0.02561596  0.02091399]]\n",
      "[[ 0.1610692   0.03505309  0.02457945  0.0302195   0.03411495  0.07614435\n",
      "   0.02763488  0.02185525  0.03013263  0.0376187   0.02283551  0.02127257\n",
      "   0.02960438  0.02927226  0.04348791  0.04919679  0.02110017  0.02122225\n",
      "   0.04109408  0.04623398  0.05730679  0.02377508  0.02498848  0.02446241\n",
      "   0.02078463  0.02522204  0.01971867]]\n",
      "[[ 0.1450668   0.04107257  0.02541346  0.02789529  0.02760157  0.08166833\n",
      "   0.02799696  0.02494347  0.0327464   0.03713508  0.02192789  0.02421004\n",
      "   0.03160761  0.02694567  0.04292936  0.04446567  0.02532691  0.02181963\n",
      "   0.04215144  0.04415586  0.04776134  0.0300925   0.02659372  0.02561383\n",
      "   0.0230677   0.02583618  0.0239547 ]]\n",
      "[[ 0.16790108  0.03692811  0.02443551  0.02905861  0.02938605  0.07557596\n",
      "   0.02719545  0.0242337   0.03245844  0.03489007  0.02206883  0.02272473\n",
      "   0.02807457  0.028957    0.04528013  0.04395114  0.02321834  0.02179188\n",
      "   0.03856715  0.04628013  0.05261846  0.02651222  0.02380285  0.02465124\n",
      "   0.02274122  0.02484882  0.02184821]]\n",
      "[[ 0.15508761  0.03479664  0.02451341  0.02998708  0.03284715  0.07780088\n",
      "   0.02710341  0.02351534  0.0326621   0.03835822  0.02135632  0.02186975\n",
      "   0.02896574  0.02845906  0.04535844  0.04736171  0.0217693   0.02244021\n",
      "   0.03972987  0.0458882   0.05394588  0.0263625   0.02536705  0.02466128\n",
      "   0.02228492  0.02685305  0.02065483]]\n",
      "[[ 0.14975201  0.03844627  0.02476106  0.0290606   0.03006411  0.07847361\n",
      "   0.02779377  0.02457305  0.03241397  0.03750408  0.0217057   0.02274733\n",
      "   0.03058057  0.0280599   0.04444175  0.04507807  0.02350136  0.02221693\n",
      "   0.04176764  0.04503538  0.05198384  0.02840804  0.02633672  0.02487068\n",
      "   0.02263921  0.02540785  0.02237639]]\n",
      "[[ 0.16240133  0.03901801  0.02456268  0.0282693   0.02831075  0.08318699\n",
      "   0.02733732  0.02305998  0.03049399  0.03587756  0.02207157  0.02362494\n",
      "   0.02983581  0.02630553  0.04342375  0.04414419  0.02387405  0.021549\n",
      "   0.04100046  0.0452891   0.05235612  0.02700705  0.02523975  0.02401725\n",
      "   0.0211814   0.02413152  0.02243057]]\n",
      "[[ 0.15897921  0.03551706  0.02369664  0.03199277  0.03346768  0.08288319\n",
      "   0.02814548  0.02031472  0.02771505  0.03704499  0.02289141  0.02246396\n",
      "   0.03051144  0.02792003  0.0439515   0.04983409  0.02145936  0.01970192\n",
      "   0.04359024  0.04422696  0.05740326  0.02278805  0.02451875  0.02298353\n",
      "   0.01953672  0.02608861  0.02037336]]\n",
      "[[ 0.14273579  0.04081562  0.02522578  0.02746745  0.02724886  0.08593282\n",
      "   0.02778042  0.02412856  0.03182251  0.03744296  0.0223606   0.02459845\n",
      "   0.03199841  0.0259097   0.04234095  0.04632733  0.0257028   0.02093736\n",
      "   0.04335101  0.04359852  0.04745159  0.03001664  0.02680425  0.02529563\n",
      "   0.02232626  0.02583447  0.02454517]]\n",
      "[[ 0.14706971  0.03552317  0.02365621  0.03272535  0.03401544  0.08337067\n",
      "   0.02883713  0.02028593  0.027463    0.03801311  0.02360952  0.02244725\n",
      "   0.03177255  0.02870491  0.04413931  0.05262151  0.02202552  0.01895214\n",
      "   0.04599104  0.04254335  0.05620809  0.02334873  0.02520671  0.02354179\n",
      "   0.01970759  0.02730726  0.0209131 ]]\n",
      "[[ 0.13738988  0.04121217  0.02593368  0.02776597  0.02703019  0.08375093\n",
      "   0.02793531  0.02509205  0.0327696   0.03781244  0.02260241  0.02476872\n",
      "   0.03193076  0.02647422  0.04281651  0.04507272  0.02640666  0.02195884\n",
      "   0.04336241  0.04297428  0.04666716  0.03116558  0.02691815  0.02579586\n",
      "   0.02319125  0.0265546   0.02464771]]\n",
      "[[ 0.16156748  0.03734716  0.02478763  0.02888212  0.02865443  0.07777649\n",
      "   0.02706996  0.02433578  0.03246956  0.03536773  0.02254054  0.02323521\n",
      "   0.02833147  0.02844678  0.0452573   0.04458691  0.02395688  0.02168382\n",
      "   0.03952987  0.04553645  0.05156026  0.02751027  0.02400508  0.02474208\n",
      "   0.02287685  0.02539841  0.0225435 ]]\n",
      "[[ 0.15694539  0.03661253  0.02432123  0.03046268  0.02997837  0.08068123\n",
      "   0.02782992  0.02376996  0.03051801  0.03710487  0.02231685  0.02341256\n",
      "   0.02940839  0.02723106  0.04599223  0.04354471  0.02350123  0.02269782\n",
      "   0.04077094  0.04374168  0.05464427  0.02679187  0.02539137  0.02393118\n",
      "   0.02157705  0.02562052  0.0212021 ]]\n",
      "[[ 0.15414213  0.03844086  0.02493074  0.02831016  0.02913844  0.08137476\n",
      "   0.02699407  0.0243666   0.03212359  0.03721461  0.02185271  0.0234798\n",
      "   0.02995188  0.02623897  0.04425985  0.04436532  0.02396291  0.02231896\n",
      "   0.0411321   0.04493019  0.05159543  0.02861451  0.02576749  0.02439901\n",
      "   0.02204998  0.02573752  0.02230737]]\n",
      "[[ 0.15537597  0.04121512  0.02440269  0.02899732  0.02731835  0.08380561\n",
      "   0.02843749  0.02405386  0.03033101  0.03517339  0.02170303  0.02407281\n",
      "   0.03114942  0.02592785  0.04425779  0.0428942   0.02515139  0.02188735\n",
      "   0.04153126  0.04333627  0.05099627  0.02796052  0.02665839  0.02403497\n",
      "   0.02163373  0.02460197  0.0230919 ]]\n",
      "[[ 0.15203875  0.03615412  0.02416782  0.03087776  0.03207945  0.07978207\n",
      "   0.02855523  0.02312252  0.03072729  0.03815939  0.02163859  0.02213372\n",
      "   0.0303625   0.02727231  0.04504192  0.04793872  0.02243255  0.02236135\n",
      "   0.04147771  0.04373454  0.05522775  0.02561569  0.02672899  0.0240473\n",
      "   0.02137469  0.02637294  0.02057438]]\n",
      "[[ 0.14541413  0.03986019  0.02500399  0.02849462  0.02850738  0.08065381\n",
      "   0.02809536  0.02487312  0.03244827  0.03756763  0.02197453  0.02336575\n",
      "   0.03117551  0.0268366   0.04353927  0.04544869  0.02456274  0.02218656\n",
      "   0.04286573  0.04422725  0.05062246  0.02937805  0.0269119   0.02488185\n",
      "   0.02266951  0.02535483  0.02308032]]\n",
      "[[ 0.14553648  0.03510093  0.02439661  0.0307376   0.03342113  0.07765259\n",
      "   0.0281156   0.02345426  0.0318389   0.0399114   0.02171635  0.02153075\n",
      "   0.03010921  0.02821237  0.04519082  0.04994136  0.02172855  0.02257264\n",
      "   0.04246265  0.04460971  0.05555339  0.02619391  0.0266537   0.02440485\n",
      "   0.02172047  0.02668999  0.0205438 ]]\n",
      "[[ 0.14726065  0.03968621  0.02575167  0.02715492  0.02823759  0.08058681\n",
      "   0.02669795  0.0256039   0.03390479  0.03772593  0.0219283   0.02424152\n",
      "   0.03038863  0.02643461  0.0432291   0.04409476  0.02475448  0.02273495\n",
      "   0.04102296  0.04552879  0.04875904  0.03091195  0.02593903  0.02494436\n",
      "   0.02279229  0.02604236  0.02364253]]\n",
      "[[ 0.15554045  0.03487009  0.02408894  0.03180039  0.0331684   0.07685875\n",
      "   0.0281861   0.0235812   0.02961488  0.0381435   0.02263917  0.02265627\n",
      "   0.02919938  0.0281288   0.04689842  0.04471933  0.0221361   0.02379549\n",
      "   0.04138067  0.04353268  0.05853269  0.0250042   0.02560174  0.02322761\n",
      "   0.02067383  0.02596284  0.02005805]]\n",
      "[[ 0.14327466  0.04024718  0.02537555  0.02728811  0.02778974  0.08336812\n",
      "   0.02735633  0.02515954  0.0327878   0.03768488  0.02209371  0.02442257\n",
      "   0.03128515  0.02592449  0.04316241  0.04509032  0.02556764  0.02193579\n",
      "   0.04260567  0.04395984  0.04836416  0.03070448  0.02669961  0.02511083\n",
      "   0.02263723  0.02590969  0.02419446]]\n",
      "[[ 0.14238214  0.0373493   0.02474538  0.02950247  0.02887073  0.08465293\n",
      "   0.02851947  0.02354402  0.03153338  0.03839228  0.02216686  0.0240744\n",
      "   0.03141589  0.02609919  0.04377342  0.04888637  0.02497426  0.02084339\n",
      "   0.04289724  0.04308339  0.05034475  0.02871041  0.02683671  0.02371334\n",
      "   0.02242049  0.02695411  0.02331364]]\n",
      "[[ 0.15338135  0.03647665  0.02486336  0.0293362   0.02962616  0.08083056\n",
      "   0.02790561  0.02285853  0.03129569  0.03699717  0.02266993  0.02355477\n",
      "   0.02998332  0.02783512  0.04355351  0.04849657  0.0235722   0.02043541\n",
      "   0.041502    0.04526771  0.05217818  0.02718     0.02479169  0.02414694\n",
      "   0.02264793  0.02619887  0.02241454]]\n",
      "[[ 0.14875972  0.038688    0.02476344  0.02999943  0.02834076  0.07675692\n",
      "   0.02979423  0.0249604   0.0305401   0.03642809  0.02335962  0.02343636\n",
      "   0.03137449  0.02898064  0.04418854  0.04377743  0.02518029  0.02253833\n",
      "   0.04102236  0.04325059  0.05401909  0.02856286  0.0264068   0.02472921\n",
      "   0.02361778  0.02397447  0.02255005]]\n",
      "[[ 0.14216258  0.04158244  0.02564838  0.02678145  0.02597876  0.08215381\n",
      "   0.02791827  0.02630366  0.03355244  0.03631497  0.02231522  0.02470767\n",
      "   0.03164026  0.02651619  0.04295148  0.04352864  0.02744316  0.02226443\n",
      "   0.04078125  0.04327386  0.04738096  0.03194511  0.02701786  0.02553799\n",
      "   0.02405132  0.02524364  0.02500416]]\n",
      "[[ 0.14336558  0.03382807  0.02413223  0.03281151  0.03402241  0.0750924\n",
      "   0.02937051  0.02389572  0.03109769  0.03862566  0.02295296  0.02134491\n",
      "   0.02994468  0.03055755  0.04656903  0.04942258  0.02250755  0.02280994\n",
      "   0.04040631  0.0429263   0.0580373   0.02517493  0.02682487  0.02483599\n",
      "   0.02238516  0.02671707  0.02034101]]\n",
      "[[ 0.13711356  0.03937524  0.02598555  0.02742107  0.02876768  0.08194064\n",
      "   0.02719452  0.02552557  0.03445258  0.03914505  0.0219379   0.02406054\n",
      "   0.03128491  0.02623695  0.04324131  0.04620189  0.02510548  0.02311961\n",
      "   0.04198955  0.04417364  0.04775821  0.0314666   0.02697812  0.0254417\n",
      "   0.02323299  0.02697349  0.02387554]]\n",
      "[[ 0.1507463   0.03766211  0.02511981  0.02878934  0.02895471  0.08060238\n",
      "   0.02889156  0.02348831  0.03042276  0.03678308  0.02389537  0.02328978\n",
      "   0.03056693  0.0278269   0.04313296  0.04738807  0.02502135  0.02221313\n",
      "   0.0410089   0.04407969  0.05382355  0.02709116  0.0259724   0.02413907\n",
      "   0.02222212  0.02430192  0.02256633]]\n",
      "[[ 0.156729    0.03993669  0.02477184  0.02777823  0.02823747  0.08364916\n",
      "   0.02753085  0.02308804  0.03078787  0.03587325  0.02289334  0.02364054\n",
      "   0.03045356  0.02669493  0.04246219  0.04590416  0.02476972  0.02101653\n",
      "   0.04130797  0.0447436   0.05130035  0.02748014  0.02568389  0.02432421\n",
      "   0.02128651  0.02426052  0.02339539]]\n",
      "[[ 0.15957151  0.03724119  0.02374336  0.03069791  0.03084102  0.08177849\n",
      "   0.02866161  0.02259795  0.0288899   0.03675581  0.02305665  0.02300807\n",
      "   0.03010175  0.02691413  0.04499887  0.04510917  0.02328506  0.02207279\n",
      "   0.04160175  0.04287648  0.05650611  0.02501496  0.02598809  0.02325714\n",
      "   0.01998873  0.02442445  0.02101709]]\n",
      "[[ 0.15529492  0.03889197  0.02509854  0.02796643  0.02871053  0.08200566\n",
      "   0.02723409  0.02374229  0.03158301  0.03684907  0.0224209   0.02400517\n",
      "   0.03045617  0.02662726  0.04271083  0.04525283  0.02408066  0.02128566\n",
      "   0.04156265  0.04528901  0.05057859  0.02833871  0.02513404  0.02455187\n",
      "   0.02205187  0.02541803  0.02285922]]\n",
      "[[ 0.15290165  0.0355856   0.02502505  0.02896283  0.02896451  0.08311983\n",
      "   0.02871404  0.02290658  0.02965021  0.03780236  0.02423649  0.023986\n",
      "   0.03059913  0.0268335   0.04233234  0.04797104  0.02481623  0.02095468\n",
      "   0.04117543  0.04425178  0.05481561  0.02699627  0.0249518   0.02339138\n",
      "   0.02219084  0.02485887  0.02200597]]\n",
      "[[ 0.15129399  0.03708312  0.02527423  0.02770206  0.02989416  0.08388641\n",
      "   0.02784397  0.02251857  0.03072505  0.03820541  0.02270791  0.0237679\n",
      "   0.03125262  0.02677407  0.04124061  0.0494131   0.02376283  0.02059209\n",
      "   0.04093844  0.04535953  0.05105624  0.02730054  0.0254589   0.02477634\n",
      "   0.02245785  0.02615673  0.02255724]]\n",
      "[[ 0.14842211  0.03458076  0.02469451  0.02939464  0.03330445  0.0815426\n",
      "   0.02771869  0.02240069  0.03153984  0.04053252  0.02213466  0.02248655\n",
      "   0.03050392  0.0274804   0.04341529  0.05085508  0.02182229  0.02157105\n",
      "   0.0410027   0.0450688   0.05359114  0.02612734  0.02613065  0.02445357\n",
      "   0.02160333  0.02684094  0.02078142]]\n",
      "[[ 0.1499657   0.03970509  0.02477515  0.02832258  0.02876749  0.08355518\n",
      "   0.02824789  0.02351398  0.03086582  0.03770241  0.02220325  0.02387514\n",
      "   0.03140476  0.02663136  0.04270904  0.04551491  0.02408745  0.02205901\n",
      "   0.04195639  0.04450947  0.05089626  0.02797765  0.02669218  0.02486939\n",
      "   0.02172424  0.02480704  0.02266113]]\n",
      "[[ 0.15211423  0.03789665  0.02404032  0.03119412  0.03118502  0.07645455\n",
      "   0.0297183   0.02346718  0.02975729  0.03681065  0.02300704  0.02232319\n",
      "   0.03140905  0.02967245  0.04484228  0.04558359  0.02314946  0.02214937\n",
      "   0.04215003  0.04352452  0.05559709  0.02576205  0.02653933  0.02456757\n",
      "   0.02150274  0.02425049  0.02133148]]\n",
      "[[ 0.1486799   0.04022424  0.02573442  0.02735819  0.02760164  0.08071236\n",
      "   0.02706001  0.02548397  0.03343037  0.03695301  0.02238831  0.02432782\n",
      "   0.03063189  0.02689907  0.04305758  0.04316838  0.02522136  0.02256587\n",
      "   0.04094424  0.04509566  0.04875395  0.0306933   0.02583233  0.02504196\n",
      "   0.0228306   0.02544281  0.0238667 ]]\n",
      "[[ 0.15049751  0.03721281  0.02455892  0.03133981  0.03026482  0.07532393\n",
      "   0.02932965  0.02508398  0.0314147   0.03635935  0.02242215  0.0227576\n",
      "   0.03059824  0.02949004  0.0458911   0.04356565  0.02388804  0.02320598\n",
      "   0.04021108  0.04345654  0.05406105  0.02761753  0.02640951  0.02443592\n",
      "   0.02321921  0.02532502  0.02205979]]\n",
      "[[ 0.14285314  0.03798358  0.02534116  0.02836173  0.02969799  0.08080515\n",
      "   0.0274805   0.02490188  0.03379814  0.0387696   0.02151148  0.0231969\n",
      "   0.03079391  0.02684348  0.04394156  0.04625048  0.02402442  0.02310227\n",
      "   0.04147161  0.0445361   0.05004449  0.03006184  0.02676909  0.02498847\n",
      "   0.02318227  0.02663034  0.02265841]]\n",
      "[[ 0.16130397  0.03834209  0.02451003  0.02832613  0.02871004  0.08336597\n",
      "   0.02736348  0.02291947  0.03056836  0.03634308  0.02210453  0.02367482\n",
      "   0.02969282  0.02613682  0.04353927  0.04489144  0.02377753  0.02205561\n",
      "   0.04073408  0.04506678  0.05273502  0.02682804  0.02543036  0.02383462\n",
      "   0.02113177  0.02450172  0.02211215]]\n",
      "s myswfohsryaklbfkgketpusues mvxabejehe wks ienlnkspth i spobdyc  fapdqr gism eb\n",
      "[[ 0.03641125  0.07213308  0.06121549  0.07510565  0.05001396  0.05142916\n",
      "   0.04673055  0.00253739  0.01893167  0.02277551  0.02778801  0.02968889\n",
      "   0.03287302  0.03654107  0.00679221  0.01296912  0.05750131  0.01910097\n",
      "   0.06990094  0.03846593  0.01408117  0.01693595  0.00631643  0.0330153\n",
      "   0.05956455  0.03922114  0.06196027]]\n",
      "[[ 0.14002718  0.04187885  0.02578538  0.02840179  0.02637611  0.0824255\n",
      "   0.02878959  0.02557733  0.03249722  0.03629884  0.02244258  0.02525528\n",
      "   0.03224998  0.02691987  0.04316343  0.04289551  0.02718733  0.02319193\n",
      "   0.0407948   0.0427662   0.04692226  0.03094203  0.02733166  0.02584647\n",
      "   0.02387217  0.02582154  0.02433911]]\n",
      "[[ 0.14294383  0.03592975  0.02425739  0.03371771  0.03285265  0.08037502\n",
      "   0.02982617  0.02177248  0.02793158  0.03736592  0.02398768  0.02295393\n",
      "   0.03197835  0.03003821  0.04473536  0.04902899  0.02346278  0.02134476\n",
      "   0.04391823  0.04197533  0.05605828  0.02404296  0.02565924  0.0245029\n",
      "   0.02131994  0.02738057  0.02064004]]\n",
      "[[ 0.14001589  0.04034368  0.02603309  0.02783815  0.02720799  0.08484565\n",
      "   0.02746058  0.0249499   0.03238411  0.03791659  0.02291996  0.02523695\n",
      "   0.03171881  0.02607815  0.04257637  0.04446709  0.02618503  0.02232924\n",
      "   0.0417949   0.04409192  0.04767515  0.03104994  0.02662348  0.02530057\n",
      "   0.02267111  0.02612369  0.02416207]]\n",
      "[[ 0.1471048   0.03990195  0.02452175  0.03077452  0.02808134  0.08101559\n",
      "   0.02962454  0.02401694  0.03055645  0.03547465  0.02214132  0.0242488\n",
      "   0.03232429  0.02832895  0.04381586  0.04517011  0.02486216  0.02146189\n",
      "   0.04146311  0.04341894  0.04984408  0.02800013  0.02637121  0.0249395\n",
      "   0.02299234  0.02607751  0.02346727]]\n",
      "[[ 0.14631136  0.04074225  0.02469382  0.02886607  0.02763391  0.08370378\n",
      "   0.02825553  0.02364212  0.03117232  0.03571386  0.02236636  0.02381409\n",
      "   0.0318989   0.02762294  0.04330055  0.04629285  0.02524717  0.02033838\n",
      "   0.04312926  0.04360807  0.04909938  0.02886608  0.02652298  0.02497436\n",
      "   0.02241291  0.02563895  0.02413175]]\n",
      "[[ 0.14989132  0.04257978  0.02429763  0.02923595  0.02626729  0.08536347\n",
      "   0.02904322  0.02370719  0.03041125  0.03457116  0.02213991  0.02430133\n",
      "   0.03190652  0.02660918  0.04377909  0.04380612  0.02606104  0.02093799\n",
      "   0.04255987  0.04225216  0.04927087  0.0287605   0.02720799  0.02456243\n",
      "   0.02180254  0.02452962  0.02414461]]\n",
      "[[ 0.15604787  0.03876107  0.02402948  0.03050711  0.02937444  0.08006141\n",
      "   0.02867369  0.02315902  0.03035296  0.03536583  0.02256458  0.02261427\n",
      "   0.03024916  0.02803221  0.04521967  0.04559462  0.02399173  0.02127433\n",
      "   0.04194861  0.04294634  0.05387726  0.02641631  0.02635037  0.02426722\n",
      "   0.02131567  0.02490845  0.02209633]]\n",
      "[[ 0.14721429  0.04066488  0.02481398  0.02957636  0.02714964  0.07846455\n",
      "   0.02954819  0.02542328  0.0310957   0.03577092  0.02314239  0.02329024\n",
      "   0.03149809  0.02818768  0.04386324  0.04283216  0.02578051  0.02289971\n",
      "   0.04190907  0.04283256  0.05262896  0.02922203  0.0272382   0.02510585\n",
      "   0.02315907  0.02355031  0.02313806]]\n",
      "[[ 0.14707616  0.03750693  0.0240611   0.03145254  0.03043915  0.08290482\n",
      "   0.02898774  0.02213809  0.02896132  0.03683051  0.0232345   0.02279782\n",
      "   0.03165779  0.02827897  0.04364385  0.04836337  0.0235241   0.0206507\n",
      "   0.04472165  0.04317877  0.05523098  0.0259125   0.02579672  0.02421269\n",
      "   0.02137401  0.02542609  0.02163709]]\n",
      "[[ 0.13969974  0.04133175  0.02512108  0.02768903  0.02683265  0.08599059\n",
      "   0.02786356  0.02467952  0.03228938  0.03725796  0.02223512  0.02437539\n",
      "   0.03209951  0.02579984  0.04277844  0.04564145  0.02626693  0.02143245\n",
      "   0.0435949   0.04302566  0.04793141  0.03086486  0.02731799  0.02517002\n",
      "   0.02264802  0.0253933   0.02466945]]\n",
      "[[ 0.14629088  0.03985508  0.02413715  0.03058019  0.03030999  0.08199979\n",
      "   0.02861306  0.02244134  0.02952202  0.03565924  0.02311428  0.02236625\n",
      "   0.0318589   0.02851577  0.04455486  0.0490039   0.02419662  0.01947993\n",
      "   0.04473522  0.04272287  0.0528036   0.0264687   0.02666717  0.02412028\n",
      "   0.02085497  0.0256278   0.02350013]]\n",
      "[[ 0.13949637  0.04176052  0.02578242  0.02729381  0.02594362  0.08522582\n",
      "   0.0283027   0.0248926   0.03264435  0.03703902  0.02245605  0.02498481\n",
      "   0.03206611  0.02591073  0.04228729  0.04567055  0.02693499  0.02150468\n",
      "   0.04220778  0.04303871  0.04618503  0.03136947  0.02741728  0.02529088\n",
      "   0.02304802  0.02587901  0.02536746]]\n",
      "[[ 0.14960121  0.04205101  0.02423901  0.03006715  0.02671074  0.08506579\n",
      "   0.02994981  0.02341557  0.0293721   0.03442613  0.02254548  0.02416033\n",
      "   0.03202242  0.02687159  0.044238    0.04418285  0.02629932  0.02137108\n",
      "   0.04241519  0.04141405  0.0502216   0.02764675  0.0275976   0.02414316\n",
      "   0.02157263  0.02445734  0.02394209]]\n",
      "[[ 0.14301303  0.04262745  0.02552158  0.02682935  0.02537212  0.08422407\n",
      "   0.02794212  0.02582446  0.03329613  0.03552338  0.02209     0.02485049\n",
      "   0.03177869  0.02609954  0.04296947  0.04349354  0.02785728  0.02190459\n",
      "   0.04094871  0.04290099  0.04616335  0.03168976  0.02747115  0.02534017\n",
      "   0.02354555  0.02526833  0.0254547 ]]\n",
      "[[ 0.14167731  0.03767326  0.02466926  0.03061352  0.02785758  0.08419786\n",
      "   0.02977102  0.02358748  0.03119774  0.0366081   0.02265454  0.02392341\n",
      "   0.03157052  0.02708184  0.04441931  0.04811593  0.02634884  0.02084531\n",
      "   0.04199861  0.04158185  0.05045117  0.0281666   0.02762266  0.0239319\n",
      "   0.02317786  0.02670291  0.02355353]]\n",
      "[[ 0.13756843  0.03971086  0.02569235  0.02775692  0.02576426  0.08693096\n",
      "   0.02839577  0.02456773  0.03307831  0.0380586   0.02213897  0.02536376\n",
      "   0.03181327  0.02513719  0.04262358  0.04673396  0.02707466  0.0213145\n",
      "   0.04225485  0.04251372  0.04640509  0.03162565  0.02777729  0.02455867\n",
      "   0.02354145  0.02653755  0.02506164]]\n",
      "[[ 0.14442357  0.03839417  0.02456171  0.0305523   0.02924035  0.08307564\n",
      "   0.0289708   0.02237171  0.02997737  0.03633505  0.02324271  0.02311371\n",
      "   0.03151429  0.02800833  0.04437759  0.04987704  0.02501625  0.01955475\n",
      "   0.04338736  0.04246533  0.05195124  0.02706015  0.02711355  0.02382413\n",
      "   0.02164933  0.0263443   0.02359724]]\n",
      "[[ 0.14400668  0.03892086  0.02485981  0.02924762  0.0288087   0.08713185\n",
      "   0.02775118  0.02190514  0.0305421   0.03788719  0.02255181  0.02407251\n",
      "   0.03178712  0.02625776  0.04267756  0.04998566  0.02427119  0.01930967\n",
      "   0.0448686   0.04320866  0.04957808  0.02817624  0.02654613  0.02425265\n",
      "   0.02120717  0.02659862  0.02358952]]\n",
      "[[ 0.13993646  0.03921869  0.02526272  0.02807142  0.02698553  0.08815558\n",
      "   0.02795387  0.02314628  0.03193329  0.03874476  0.02212353  0.02498159\n",
      "   0.03190689  0.02506119  0.04251368  0.04901171  0.02561321  0.02001061\n",
      "   0.04390626  0.04268927  0.04723188  0.0304312   0.02732197  0.02414628\n",
      "   0.02227469  0.02670303  0.02466441]]\n",
      "[[ 0.14075895  0.0397278   0.02447959  0.02983462  0.0285474   0.08708357\n",
      "   0.02921027  0.02187478  0.02933679  0.03706799  0.02283257  0.02401279\n",
      "   0.03302625  0.02679832  0.0427978   0.05033588  0.0250356   0.01904821\n",
      "   0.04504121  0.04177872  0.0501891   0.02771091  0.02778589  0.02408382\n",
      "   0.02130914  0.02614719  0.02414486]]\n",
      "[[ 0.1447331   0.03911351  0.02523005  0.02791034  0.02892205  0.0857882\n",
      "   0.02803174  0.02216017  0.0309242   0.03802665  0.02222924  0.02381321\n",
      "   0.03215178  0.02671684  0.04160962  0.05046037  0.02413288  0.01987224\n",
      "   0.04338979  0.04362428  0.0486276   0.02810815  0.02680589  0.02523407\n",
      "   0.02213551  0.02644867  0.02379991]]\n",
      "[[ 0.14488089  0.03695204  0.0251585   0.02853243  0.02779546  0.08647943\n",
      "   0.02891002  0.02234274  0.03032082  0.03863504  0.02413559  0.02433887\n",
      "   0.03127575  0.02656457  0.04184064  0.05015362  0.02527184  0.02036425\n",
      "   0.04256765  0.04305209  0.05153811  0.0280949   0.02650152  0.02392101\n",
      "   0.0221133   0.02492564  0.02333326]]\n",
      "[[ 0.14175446  0.03551497  0.0250474   0.0288461   0.0306459   0.08357941\n",
      "   0.02784453  0.02288259  0.03205008  0.04084944  0.02290153  0.02337531\n",
      "   0.03082791  0.02660444  0.0434954   0.05077655  0.0234233   0.02164662\n",
      "   0.04224933  0.04358673  0.05227327  0.02802738  0.02677311  0.02447465\n",
      "   0.02186068  0.0264757   0.02221321]]\n",
      "[[ 0.15272158  0.03905646  0.02500528  0.02739766  0.02748029  0.08634559\n",
      "   0.02708157  0.02338287  0.03141572  0.03772006  0.02267882  0.02476803\n",
      "   0.03049703  0.02520826  0.04272575  0.04559705  0.02474913  0.02158582\n",
      "   0.04113441  0.04478259  0.05016547  0.02891934  0.02600865  0.02415659\n",
      "   0.0212287   0.02481993  0.02336729]]\n",
      "[[ 0.15124221  0.03325194  0.02415976  0.03116661  0.03414368  0.08014039\n",
      "   0.0277841   0.02219539  0.03047152  0.04021761  0.02277431  0.02212471\n",
      "   0.02942384  0.02771781  0.04522393  0.05073326  0.02125822  0.02229455\n",
      "   0.04056071  0.04455363  0.05740665  0.02454397  0.02592208  0.02370552\n",
      "   0.02026221  0.0266584   0.020063  ]]\n",
      "[[ 0.13838874  0.04022845  0.02557756  0.02731641  0.02731015  0.08440056\n",
      "   0.02754267  0.02539369  0.03339155  0.03847532  0.02222858  0.02465739\n",
      "   0.03160075  0.02561199  0.04312779  0.04559213  0.02596829  0.0223061\n",
      "   0.0422789   0.04365811  0.04751135  0.03129181  0.0273698   0.02539727\n",
      "   0.02270157  0.0260952   0.02457792]]\n",
      "[[ 0.13889794  0.03717298  0.02479369  0.02949764  0.02855017  0.08545909\n",
      "   0.02866234  0.02362195  0.03195916  0.0391061   0.02221772  0.02424243\n",
      "   0.03159334  0.0258306   0.04387945  0.04948135  0.02518312  0.02112738\n",
      "   0.04291201  0.04287049  0.0497338   0.02901821  0.02737099  0.02382915\n",
      "   0.02234065  0.02700972  0.02363854]]\n",
      "[[ 0.14457826  0.04060666  0.02538536  0.02765926  0.02567103  0.08481298\n",
      "   0.02936186  0.02456026  0.03092156  0.03652678  0.02347109  0.02441299\n",
      "   0.03138426  0.02578357  0.04244298  0.04570872  0.02694202  0.02241308\n",
      "   0.04121402  0.04299878  0.05050066  0.02968004  0.0275822   0.0245558\n",
      "   0.02266865  0.02400246  0.02415467]]\n",
      "[[ 0.14923938  0.04183711  0.02460435  0.02896221  0.02535706  0.08362107\n",
      "   0.03051736  0.02440337  0.0306363   0.03477919  0.02233963  0.02497168\n",
      "   0.0319674   0.0263045   0.04309507  0.04388178  0.02683348  0.02261902\n",
      "   0.04010527  0.04233302  0.04953608  0.02851495  0.02761059  0.02471731\n",
      "   0.02314732  0.02419109  0.02387446]]\n",
      "[[ 0.16479248  0.03956173  0.02407683  0.02901815  0.02618782  0.08510695\n",
      "   0.02920225  0.02255543  0.02930542  0.03408745  0.02306107  0.02442064\n",
      "   0.02995186  0.02601753  0.04287824  0.04388576  0.02544412  0.02192485\n",
      "   0.03985509  0.04337909  0.05272363  0.02589918  0.02573225  0.02390426\n",
      "   0.02132965  0.0232969   0.02240134]]\n",
      "[[ 0.15420917  0.03955342  0.02427435  0.0297948   0.02667708  0.08134986\n",
      "   0.02988431  0.02438121  0.03105574  0.03520919  0.02257246  0.02397393\n",
      "   0.03103389  0.02687288  0.04379158  0.04353997  0.02577477  0.02247991\n",
      "   0.04024764  0.04250211  0.05163344  0.02787296  0.02686972  0.02451783\n",
      "   0.0229115   0.02428137  0.02273496]]\n",
      "[[ 0.16179746  0.03990791  0.0241783   0.02861384  0.02756904  0.08303099\n",
      "   0.02815052  0.02291778  0.0303156   0.0345953   0.02249241  0.02352402\n",
      "   0.03027954  0.02673039  0.04292592  0.04476403  0.02480246  0.02098464\n",
      "   0.04109098  0.04418289  0.05197005  0.02662893  0.02583107  0.02436648\n",
      "   0.02143225  0.0241115   0.02280567]]\n",
      "[[ 0.15547824  0.03569024  0.02391423  0.03071216  0.03229126  0.0800797\n",
      "   0.02824923  0.02245647  0.03095149  0.03779333  0.02208446  0.02191249\n",
      "   0.02984699  0.02774692  0.04427606  0.04887818  0.02217699  0.02163174\n",
      "   0.0410146   0.04421895  0.05523735  0.02510184  0.02633095  0.02424602\n",
      "   0.02108305  0.02602205  0.02057497]]\n",
      "[[ 0.15103364  0.03966881  0.02565272  0.02715951  0.02805784  0.08101027\n",
      "   0.02668203  0.02524547  0.033676    0.03716543  0.02207247  0.02428336\n",
      "   0.03019451  0.02638559  0.0430412   0.04370838  0.02483269  0.02244261\n",
      "   0.04046584  0.04547335  0.04876891  0.0302939   0.02580091  0.02493603\n",
      "   0.02257174  0.02585388  0.0235229 ]]\n",
      "[[ 0.16208167  0.03474077  0.02428781  0.03105933  0.03345857  0.07485477\n",
      "   0.0275027   0.02320939  0.03093384  0.03673022  0.02262073  0.02166598\n",
      "   0.02829262  0.02930763  0.04609505  0.04628614  0.02167631  0.02235501\n",
      "   0.03970146  0.04532204  0.05720501  0.02462636  0.0249501   0.02382729\n",
      "   0.0210092   0.02583055  0.02036945]]\n",
      "[[ 0.14641167  0.04017463  0.02526922  0.02738667  0.02817783  0.08271955\n",
      "   0.02717857  0.02477442  0.03293768  0.03718578  0.02200977  0.02396208\n",
      "   0.03100338  0.02649173  0.04321168  0.04541915  0.02509226  0.02133887\n",
      "   0.04213344  0.04452067  0.04852622  0.03004472  0.02649253  0.02512636\n",
      "   0.02240795  0.02578111  0.02422208]]\n",
      "[[ 0.15858975  0.03821329  0.02394755  0.031568    0.03179609  0.07736807\n",
      "   0.02968401  0.02247214  0.02826964  0.03517923  0.02299533  0.02220076\n",
      "   0.03117975  0.02948027  0.04468161  0.04541173  0.02280465  0.02173323\n",
      "   0.04183803  0.04294316  0.05688925  0.02398717  0.02618228  0.02439477\n",
      "   0.02066961  0.02437957  0.02114112]]\n",
      "[[ 0.14289263  0.04197896  0.02552528  0.02795121  0.02668982  0.08268316\n",
      "   0.02840977  0.0251506   0.03266393  0.03651774  0.02214034  0.02457494\n",
      "   0.03189753  0.02683336  0.04286838  0.04381201  0.02622504  0.02208124\n",
      "   0.04221397  0.04315443  0.04713137  0.03062167  0.02700254  0.02570516\n",
      "   0.02325944  0.02561659  0.02439884]]\n",
      "[[ 0.14571317  0.03513431  0.02410868  0.03236951  0.03352163  0.07730951\n",
      "   0.02915615  0.02319876  0.0308864   0.03871082  0.02217395  0.02152439\n",
      "   0.03034051  0.02982484  0.04572005  0.04894417  0.02207712  0.02261427\n",
      "   0.0420079   0.04322117  0.05556514  0.02527426  0.02669408  0.02499831\n",
      "   0.02193001  0.02667585  0.02030501]]\n",
      "[[ 0.13574865  0.04030672  0.02619366  0.02726279  0.02744652  0.0818153\n",
      "   0.02768607  0.02630813  0.03371231  0.0380681   0.02265825  0.02471412\n",
      "   0.03188897  0.02670833  0.04319599  0.04431963  0.02695797  0.02294264\n",
      "   0.0416834   0.04324264  0.04734531  0.03182481  0.02724166  0.02602097\n",
      "   0.02364903  0.02649374  0.02456421]]\n",
      "[[ 0.1579247   0.03915139  0.02455659  0.02920878  0.03009759  0.08064403\n",
      "   0.02743176  0.02278344  0.03046213  0.03537111  0.02285389  0.02259795\n",
      "   0.02983408  0.02801027  0.04439604  0.04619823  0.02381561  0.02118938\n",
      "   0.04213647  0.04430829  0.05270569  0.02553127  0.02563467  0.02469003\n",
      "   0.02083712  0.02508517  0.02254426]]\n",
      "[[ 0.16054651  0.04043957  0.02468495  0.02814086  0.0289476   0.08315165\n",
      "   0.02688452  0.02264958  0.03070245  0.0352192   0.02243071  0.02322427\n",
      "   0.03018374  0.02693102  0.04308343  0.04534303  0.02416742  0.02050797\n",
      "   0.04194125  0.044839    0.05085624  0.02655485  0.02554198  0.02453507\n",
      "   0.02068059  0.02459235  0.02322009]]\n",
      "[[ 0.1560865   0.03558345  0.02417229  0.03093432  0.03426091  0.07949173\n",
      "   0.02740997  0.02202647  0.0307926   0.03822815  0.02203614  0.02141027\n",
      "   0.0295555   0.02823399  0.04486663  0.04955789  0.02136748  0.02117203\n",
      "   0.04155508  0.04472059  0.05512409  0.02438051  0.02599031  0.02403827\n",
      "   0.02023934  0.02626124  0.02050427]]\n",
      "[[ 0.14537546  0.04055803  0.02521401  0.02741748  0.02844138  0.08360568\n",
      "   0.02705597  0.02429236  0.03291984  0.03746301  0.02188995  0.02383357\n",
      "   0.03123594  0.02639098  0.04317269  0.04629607  0.02483089  0.02093325\n",
      "   0.04300536  0.04446479  0.04803457  0.02969618  0.02686459  0.02513272\n",
      "   0.02190593  0.02572392  0.02424534]]\n",
      "[[ 0.15647978  0.03645882  0.02411731  0.03068783  0.03297025  0.0770378\n",
      "   0.02767071  0.02283652  0.03088392  0.03704299  0.02240377  0.02159628\n",
      "   0.02941472  0.02893762  0.0461734   0.0478862   0.02218551  0.02103537\n",
      "   0.04207286  0.04436755  0.05530034  0.02514252  0.02580707  0.02401692\n",
      "   0.02045107  0.02565538  0.02136748]]\n",
      "[[ 0.14177074  0.04110289  0.02526673  0.02744873  0.02745252  0.08430666\n",
      "   0.02753631  0.02502849  0.03316376  0.03728981  0.02188826  0.02411834\n",
      "   0.03155403  0.02593099  0.04350663  0.04532683  0.02583412  0.0216681\n",
      "   0.0426066   0.04361365  0.04761308  0.03078862  0.02740048  0.025124\n",
      "   0.02234879  0.02559147  0.02471942]]\n",
      "[[ 0.1621798   0.03926681  0.02400342  0.02966703  0.03065123  0.08095955\n",
      "   0.02765514  0.02188119  0.02951249  0.03476806  0.02259761  0.02207615\n",
      "   0.02981088  0.02781343  0.04485834  0.0470651   0.02316391  0.02031521\n",
      "   0.04265857  0.04392936  0.05384237  0.02463137  0.02567938  0.02393475\n",
      "   0.01977897  0.02472923  0.02257059]]\n",
      "[[ 0.14992346  0.03837984  0.02495568  0.02861853  0.0309631   0.0821347\n",
      "   0.02709287  0.02320271  0.03257427  0.03834815  0.02157704  0.02257371\n",
      "   0.03049236  0.02636187  0.04383983  0.04821054  0.02295217  0.02148087\n",
      "   0.04251421  0.04459909  0.05046425  0.02759104  0.02668108  0.0244467\n",
      "   0.02123436  0.02629826  0.02248927]]\n",
      "[[ 0.15043364  0.04010979  0.02458453  0.02849053  0.03020551  0.08227793\n",
      "   0.02724211  0.02282979  0.03151315  0.03676352  0.0218111   0.02267099\n",
      "   0.03114328  0.02709238  0.04406133  0.04825142  0.02351419  0.01996103\n",
      "   0.04396949  0.04452147  0.05024368  0.0273515   0.02675272  0.02424728\n",
      "   0.02076568  0.02561162  0.02358031]]\n",
      "[[ 0.16201302  0.03975736  0.02455476  0.02809228  0.02834164  0.0850144\n",
      "   0.02710664  0.02232864  0.03022787  0.0356369   0.02226752  0.02360999\n",
      "   0.03009109  0.02578142  0.04339262  0.04548961  0.02408824  0.02044091\n",
      "   0.04208288  0.04472759  0.0511523   0.02679516  0.02546972  0.02369937\n",
      "   0.0202866   0.02434     0.02321152]]\n",
      "[[ 0.17111772  0.03942805  0.02405683  0.02869691  0.02986288  0.08299872\n",
      "   0.0270882   0.02127039  0.02905016  0.03438386  0.02248568  0.02265356\n",
      "   0.02952573  0.02679719  0.04345074  0.04599185  0.02304454  0.01972258\n",
      "   0.04181325  0.04473444  0.0531461   0.02433838  0.02493497  0.02350963\n",
      "   0.01935332  0.02401154  0.02253278]]\n",
      "[[ 0.15695649  0.03817604  0.02510453  0.02790834  0.02755194  0.0863221\n",
      "   0.02805329  0.02261464  0.02999602  0.03671537  0.02370926  0.02405448\n",
      "   0.03082508  0.02551232  0.04195891  0.04709125  0.02516496  0.02026061\n",
      "   0.04141479  0.04406941  0.05170543  0.02711015  0.02571364  0.02354927\n",
      "   0.0210849   0.02408937  0.02328739]]\n",
      "[[ 0.1545476   0.04132731  0.02453857  0.02835928  0.02630568  0.08689591\n",
      "   0.02892362  0.02346534  0.03003717  0.03503839  0.0227162   0.02455575\n",
      "   0.03161626  0.02539197  0.04292151  0.04407497  0.02618576  0.02091268\n",
      "   0.04143057  0.04276544  0.04997963  0.02819706  0.02679253  0.02385975\n",
      "   0.02131154  0.02387869  0.02397081]]\n",
      "[[ 0.14969039  0.04095095  0.02407145  0.02971041  0.02790363  0.08652487\n",
      "   0.03001004  0.02238163  0.02859649  0.03463603  0.02310614  0.02374866\n",
      "   0.03308571  0.02674237  0.042645    0.04716718  0.02558846  0.01958175\n",
      "   0.04322742  0.0419949   0.0513253   0.02658727  0.02744648  0.02399991\n",
      "   0.02103022  0.02457743  0.0236699 ]]\n",
      "[[ 0.14878207  0.04015424  0.02451301  0.02922749  0.02745982  0.08657412\n",
      "   0.02894582  0.02341291  0.03019672  0.03685712  0.02270507  0.02416356\n",
      "   0.03176677  0.02537817  0.04350998  0.04488396  0.02540105  0.02140843\n",
      "   0.04322064  0.04211237  0.05102397  0.02829732  0.02727999  0.02391862\n",
      "   0.02120907  0.02467767  0.02292006]]\n",
      "[[ 0.14783804  0.03977256  0.02460381  0.03048592  0.02769476  0.07776482\n",
      "   0.03082471  0.02505771  0.02965131  0.03602307  0.02381271  0.02334332\n",
      "   0.0322368   0.02806513  0.04426568  0.04298123  0.02580414  0.02300553\n",
      "   0.04197386  0.04154826  0.05476072  0.02806951  0.02745294  0.02464286\n",
      "   0.02276112  0.02314937  0.02241013]]\n",
      "[[ 0.15226085  0.04084775  0.02478918  0.02781031  0.02715101  0.08374123\n",
      "   0.02790321  0.02413312  0.03103221  0.03562164  0.02272551  0.02386043\n",
      "   0.03121756  0.02639568  0.04254654  0.04442875  0.0255644   0.02149014\n",
      "   0.04190193  0.04410855  0.05073539  0.029002    0.02622046  0.02483429\n",
      "   0.02203714  0.02405927  0.02358153]]\n",
      "[[ 0.1701086   0.03720516  0.0243477   0.02862377  0.02871023  0.07737969\n",
      "   0.02709084  0.02396385  0.03162843  0.03425768  0.02285185  0.02281568\n",
      "   0.02833139  0.02803853  0.04437038  0.04405233  0.02378592  0.0214542\n",
      "   0.03820787  0.04600136  0.05389706  0.02652077  0.02370209  0.02421609\n",
      "   0.02209322  0.02423977  0.02210561]]\n",
      "[[ 0.15154175  0.04075466  0.0245757   0.02812605  0.02794552  0.08051975\n",
      "   0.02859351  0.02512324  0.03191087  0.03496742  0.02235783  0.022705\n",
      "   0.03103894  0.02745413  0.04361518  0.04403086  0.02560542  0.02214592\n",
      "   0.04008413  0.04344611  0.05161444  0.02882791  0.02739771  0.02497747\n",
      "   0.02269278  0.02441106  0.02353663]]\n",
      "[[ 0.16520374  0.03824532  0.02457882  0.02826385  0.02842361  0.0774973\n",
      "   0.0272359   0.02478168  0.03288179  0.03434081  0.02217379  0.02272874\n",
      "   0.02879607  0.02799577  0.04437462  0.04322141  0.02431948  0.02205733\n",
      "   0.03800079  0.04576502  0.05220439  0.02779825  0.02489557  0.02451185\n",
      "   0.02262335  0.0243714   0.02270933]]\n",
      "[[ 0.15628444  0.0342295   0.02421485  0.03072161  0.03356712  0.07703283\n",
      "   0.02758589  0.02343736  0.03236572  0.03761049  0.02167295  0.02126173\n",
      "   0.0287859   0.02888903  0.04534623  0.04778621  0.02162025  0.02257711\n",
      "   0.03883431  0.0456535   0.0559742   0.02567017  0.02577005  0.02432789\n",
      "   0.0219871   0.02644044  0.02035316]]\n",
      "[[ 0.14531569  0.03762941  0.02539478  0.02780395  0.03075887  0.08113744\n",
      "   0.02674418  0.02455467  0.03430543  0.03931021  0.02107367  0.02298736\n",
      "   0.03029506  0.02636506  0.04374679  0.04711676  0.02319226  0.02289727\n",
      "   0.04105923  0.04562102  0.04980677  0.02948952  0.02660608  0.02488158\n",
      "   0.02247508  0.02698418  0.02244773]]\n",
      "[[ 0.14891352  0.03969376  0.02439308  0.02970447  0.02935599  0.07926613\n",
      "   0.02883541  0.02429144  0.03224305  0.03655689  0.0207146   0.02340793\n",
      "   0.0315777   0.02793221  0.04432604  0.04535019  0.02371666  0.02198452\n",
      "   0.0415393   0.04427975  0.04982777  0.02812591  0.02692862  0.02481916\n",
      "   0.02312289  0.02602507  0.02306791]]\n",
      "[[ 0.14563392  0.04082073  0.02424609  0.02935787  0.02817778  0.08404811\n",
      "   0.02926083  0.02302892  0.03052965  0.03598491  0.02155881  0.02357495\n",
      "   0.03277157  0.02752826  0.04304854  0.0470576   0.02466866  0.02047639\n",
      "   0.04382094  0.04323687  0.04947653  0.02783456  0.0275093   0.02482518\n",
      "   0.0222124   0.02549828  0.02381241]]\n",
      "[[ 0.14761461  0.04282264  0.02458028  0.0287641   0.02575159  0.08418029\n",
      "   0.0297046   0.02408886  0.03152567  0.03513182  0.02134479  0.02474741\n",
      "   0.032197    0.02667593  0.04296846  0.04385421  0.02626023  0.02171412\n",
      "   0.04177293  0.04242305  0.04734948  0.02935898  0.02763272  0.02520254\n",
      "   0.02307984  0.024943    0.02431082]]\n",
      "[[ 0.14959237  0.03684358  0.02325893  0.03368315  0.03181507  0.08261088\n",
      "   0.03052212  0.02060958  0.02742243  0.03620662  0.02285182  0.02268409\n",
      "   0.03195886  0.02938168  0.04443587  0.04952374  0.02282673  0.02008907\n",
      "   0.04498234  0.04156114  0.05595473  0.02324607  0.02612339  0.02399823\n",
      "   0.02068431  0.0263232   0.02080997]]\n",
      "[[ 0.14059056  0.04043481  0.02576507  0.02776119  0.02754033  0.08393592\n",
      "   0.02747704  0.02472169  0.03294053  0.03791178  0.02243367  0.02449323\n",
      "   0.03157048  0.02622491  0.04279004  0.04512315  0.02570328  0.02215483\n",
      "   0.04300443  0.04373659  0.04776409  0.03068448  0.02671644  0.02526384\n",
      "   0.02286367  0.02622901  0.02416489]]\n",
      "[[ 0.14725712  0.04015207  0.02424034  0.03079398  0.02840472  0.07998694\n",
      "   0.02967096  0.02384678  0.03100368  0.03561939  0.02168331  0.0237819\n",
      "   0.03230086  0.02848255  0.04412933  0.04522304  0.02444084  0.02134829\n",
      "   0.04250884  0.04296049  0.04994141  0.02781999  0.02679257  0.02499635\n",
      "   0.02312819  0.02589987  0.02358615]]\n",
      "[[ 0.14159496  0.04203345  0.02561484  0.02696216  0.02588608  0.0831065\n",
      "   0.0278077   0.02593557  0.03366295  0.03611512  0.02190174  0.02471811\n",
      "   0.03177582  0.02656482  0.04307429  0.04367495  0.02731428  0.02205679\n",
      "   0.04121906  0.04325696  0.04629119  0.03175665  0.02718905  0.02561372\n",
      "   0.02390487  0.02565551  0.02531287]]\n",
      "[[ 0.14805126  0.03935049  0.02432696  0.03155935  0.03053771  0.07856116\n",
      "   0.02915679  0.02289485  0.02972843  0.03468716  0.0231908   0.02208714\n",
      "   0.0314585   0.0301308   0.04562809  0.04762015  0.02440788  0.02002105\n",
      "   0.04308531  0.04252962  0.05318328  0.02570558  0.02665116  0.02449152\n",
      "   0.0216906   0.02603708  0.02322719]]\n",
      "[[ 0.14355642  0.04095335  0.02562855  0.02748746  0.02612734  0.08492537\n",
      "   0.02799778  0.02468146  0.03205116  0.03641487  0.02328889  0.02442029\n",
      "   0.0312645   0.02646462  0.04221324  0.04544097  0.02654702  0.02139782\n",
      "   0.0419776   0.0438067   0.04849644  0.03048758  0.0267655   0.02502819\n",
      "   0.02276059  0.02485798  0.02495827]]\n",
      "[[ 0.16464511  0.03660759  0.02473419  0.02908252  0.02829243  0.07764441\n",
      "   0.02742183  0.02398158  0.03208026  0.03458298  0.02336579  0.02295145\n",
      "   0.02808592  0.02868258  0.04475471  0.04521499  0.02426098  0.02129491\n",
      "   0.03834365  0.0456504   0.05318247  0.02693711  0.02403025  0.02423161\n",
      "   0.0225454   0.02461505  0.02277975]]\n",
      "[[ 0.15276457  0.04005615  0.02462664  0.02922434  0.02716502  0.08056375\n",
      "   0.0287613   0.02457302  0.0320004   0.0347935   0.02186174  0.02399354\n",
      "   0.03094459  0.02802154  0.04393106  0.04365958  0.02537484  0.0213004\n",
      "   0.03993135  0.04401411  0.04930817  0.02895963  0.02597873  0.02499144\n",
      "   0.02365891  0.02557119  0.02397048]]\n",
      "[[ 0.16573222  0.03776341  0.02464303  0.02875029  0.02796789  0.07726733\n",
      "   0.02734902  0.02447438  0.03282496  0.03414087  0.02205037  0.02331977\n",
      "   0.02842524  0.02864954  0.04478736  0.0430199   0.0241061   0.02175853\n",
      "   0.03795687  0.04596989  0.05167332  0.02767756  0.02418964  0.02468308\n",
      "   0.02338281  0.02483473  0.02260184]]\n",
      "[[ 0.15660179  0.03400426  0.02422773  0.03088221  0.03304701  0.07729071\n",
      "   0.02755732  0.02322539  0.03222497  0.03755222  0.02155897  0.02166803\n",
      "   0.02872809  0.02917395  0.04559565  0.04731464  0.02161983  0.02238969\n",
      "   0.03885732  0.04554115  0.0555379   0.02577282  0.02541432  0.0245733\n",
      "   0.02250902  0.02688542  0.02024635]]\n",
      "[[ 0.14505808  0.03845755  0.02553085  0.02748596  0.02733206  0.08444847\n",
      "   0.02771783  0.02474892  0.03244529  0.03870049  0.02258313  0.02467067\n",
      "   0.03079473  0.02595488  0.04213002  0.04580703  0.02529524  0.02200814\n",
      "   0.04106701  0.04469567  0.04947374  0.03033538  0.02646829  0.02469556\n",
      "   0.02296574  0.02543785  0.02369143]]\n",
      "[[ 0.14359985  0.04008827  0.0248178   0.02899247  0.02867649  0.08073948\n",
      "   0.02955337  0.0246462   0.03095869  0.03659286  0.02285517  0.02276693\n",
      "   0.03191929  0.02770562  0.04363761  0.04605362  0.02554348  0.02250073\n",
      "   0.04129232  0.04251815  0.05285357  0.02846693  0.02862201  0.02456483\n",
      "   0.02236703  0.02449801  0.02316922]]\n",
      "[[ 0.15042199  0.03569608  0.02459797  0.03147103  0.03383277  0.07407733\n",
      "   0.02920394  0.02261926  0.02968677  0.03736177  0.02381272  0.02057165\n",
      "   0.03038418  0.03149002  0.04434518  0.04946269  0.02207169  0.02204664\n",
      "   0.04158069  0.04463161  0.05903456  0.0243366   0.02624719  0.02487978\n",
      "   0.02139215  0.02444973  0.02029395]]\n",
      "[[ 0.1377001   0.04085567  0.02620953  0.02741103  0.02699054  0.08233065\n",
      "   0.02812249  0.02564052  0.03312285  0.03807911  0.02273366  0.02479885\n",
      "   0.03183286  0.02665891  0.04246044  0.04497952  0.02653979  0.02259543\n",
      "   0.04171701  0.04355326  0.04709699  0.03148377  0.0272245   0.02579321\n",
      "   0.02326464  0.02602147  0.02478321]]\n",
      "znekohtynxouhcbbonblrq e xbjzema stogk caa otxa oeaqhlpyasvs  klzntkcofsks qvwuf\n",
      "[[ 0.00202278  0.03671694  0.07374287  0.04017101  0.04831826  0.05705814\n",
      "   0.00537788  0.01512716  0.04293139  0.01924106  0.01184067  0.03033008\n",
      "   0.04937192  0.03296409  0.01601324  0.07468516  0.04641001  0.0715232\n",
      "   0.04129945  0.01540362  0.05538313  0.08199769  0.04519285  0.01967022\n",
      "   0.02593178  0.0113495   0.02992591]]\n",
      "[[ 0.13654067  0.04066845  0.02621174  0.02770906  0.02624721  0.08401953\n",
      "   0.02856752  0.0254549   0.03284858  0.03745089  0.02285874  0.02542247\n",
      "   0.0320724   0.02648485  0.04229802  0.04500104  0.02749247  0.02261883\n",
      "   0.04091422  0.04307215  0.04635156  0.0315263   0.02744645  0.02570261\n",
      "   0.02401529  0.02621367  0.02479037]]\n",
      "[[ 0.14875275  0.03772788  0.02550251  0.02963191  0.02742488  0.08140364\n",
      "   0.03014279  0.02351387  0.02934625  0.03555544  0.02538595  0.0239746\n",
      "   0.03086507  0.02868504  0.04276557  0.04658879  0.02692512  0.02215772\n",
      "   0.03986012  0.04297034  0.05350792  0.02693882  0.02602335  0.02451811\n",
      "   0.02298009  0.02409916  0.0227523 ]]\n",
      "[[ 0.15260181  0.03912917  0.02576405  0.02741617  0.02678233  0.08142718\n",
      "   0.02726195  0.02501759  0.03309377  0.03610241  0.02338274  0.02468028\n",
      "   0.02993339  0.0270461   0.04277835  0.04356626  0.02600166  0.02234428\n",
      "   0.03913274  0.04500106  0.04945896  0.02997671  0.02536276  0.02490747\n",
      "   0.0232076   0.02493282  0.02369039]]\n",
      "[[ 0.15771236  0.034338    0.02453595  0.0308444   0.03297517  0.07878212\n",
      "   0.02823911  0.02163015  0.03005306  0.03688163  0.02337942  0.02206962\n",
      "   0.02947416  0.03012037  0.04242663  0.05047571  0.02204932  0.02088756\n",
      "   0.03882743  0.04584736  0.05546822  0.02383581  0.02482538  0.02500393\n",
      "   0.02231116  0.0263859   0.02062005]]\n",
      "[[ 0.1440928   0.03892928  0.02574401  0.0275345   0.02682338  0.08544169\n",
      "   0.02790846  0.02438144  0.03240836  0.03855404  0.02337869  0.02507459\n",
      "   0.0310306   0.02609252  0.0412185   0.04621208  0.0256902   0.02161967\n",
      "   0.04093043  0.04438725  0.0485593   0.03018246  0.02654247  0.0249883\n",
      "   0.02291643  0.02523216  0.02412641]]\n",
      "[[ 0.14411315  0.03261834  0.02460084  0.03164446  0.03388021  0.07962409\n",
      "   0.02848263  0.02253311  0.03098721  0.04092252  0.02347118  0.02222607\n",
      "   0.02983714  0.02893101  0.04413278  0.05174313  0.02202877  0.02216644\n",
      "   0.04044978  0.04416559  0.05668614  0.02536329  0.02629674  0.02433295\n",
      "   0.02156241  0.02688375  0.02031627]]\n",
      "[[ 0.1368953   0.04008941  0.02588627  0.02768545  0.02729956  0.08255884\n",
      "   0.02769165  0.02578946  0.03379958  0.03878792  0.02259027  0.02464478\n",
      "   0.03144591  0.02622823  0.04288004  0.04490506  0.02600951  0.02296048\n",
      "   0.04221395  0.04364478  0.04775144  0.03163914  0.02715073  0.0254446\n",
      "   0.02343545  0.02622112  0.02435105]]\n",
      "[[ 0.14476709  0.03852599  0.02455075  0.03056747  0.03072383  0.08014987\n",
      "   0.02843877  0.02310365  0.03049571  0.03692659  0.02327603  0.02258098\n",
      "   0.03124805  0.02876229  0.04461592  0.04843606  0.02392539  0.02069051\n",
      "   0.04376485  0.0435743   0.05278842  0.02675335  0.02652122  0.02425903\n",
      "   0.02148673  0.02603226  0.023035  ]]\n",
      "[[ 0.14121588  0.04084861  0.02514585  0.02925617  0.02726635  0.07909479\n",
      "   0.02933211  0.02551305  0.03152358  0.0367704   0.02349021  0.02347144\n",
      "   0.03182996  0.02812019  0.04353985  0.04407864  0.02598109  0.02265125\n",
      "   0.04286292  0.04297038  0.05141178  0.02996289  0.02732757  0.0250776\n",
      "   0.0233316   0.02406558  0.0238602 ]]\n",
      "[[ 0.14102709  0.03654091  0.0246207   0.03039158  0.03122273  0.07870167\n",
      "   0.02862304  0.02418541  0.031984    0.03921543  0.02263735  0.02199238\n",
      "   0.03076809  0.02838442  0.04447683  0.04862496  0.02326942  0.02284612\n",
      "   0.04259419  0.04370716  0.05431878  0.02786141  0.02699072  0.02489346\n",
      "   0.02268138  0.02578702  0.02165376]]\n",
      "[[ 0.14209689  0.04032945  0.02492668  0.02786463  0.02822266  0.08307248\n",
      "   0.02761464  0.02439186  0.0325412   0.03754289  0.02216621  0.02356589\n",
      "   0.03142333  0.02684667  0.04313377  0.04703117  0.02497746  0.02111141\n",
      "   0.043605    0.04409621  0.04944612  0.02999055  0.02692532  0.02498818\n",
      "   0.02242767  0.02540309  0.02425857]]\n",
      "[[ 0.1471405   0.03966893  0.0241806   0.03006263  0.03065951  0.08092964\n",
      "   0.02818144  0.02261211  0.03027514  0.03601589  0.0228153   0.02217957\n",
      "   0.0313572   0.02878903  0.0444535   0.04929551  0.02366081  0.01943495\n",
      "   0.04454422  0.0436165   0.05269444  0.02670374  0.02651013  0.02417869\n",
      "   0.02095202  0.02552749  0.02356057]]\n",
      "[[ 0.15234372  0.04103039  0.02475708  0.02785089  0.0283705   0.08438897\n",
      "   0.02704506  0.02309622  0.0312672   0.03583128  0.02233931  0.02344602\n",
      "   0.03083767  0.02673186  0.04298609  0.04633034  0.02463697  0.0201158\n",
      "   0.04304006  0.04452348  0.04982005  0.0284571   0.02597492  0.02449225\n",
      "   0.02114943  0.02491617  0.02422118]]\n",
      "[[ 0.16046795  0.0373744   0.02398571  0.03040855  0.03217713  0.07862219\n",
      "   0.02756701  0.02231156  0.03020328  0.03591477  0.02269759  0.02165014\n",
      "   0.02937284  0.02861416  0.04549184  0.04751275  0.02236678  0.02042598\n",
      "   0.04190153  0.04427408  0.05543026  0.02507436  0.0254214   0.02378263\n",
      "   0.02015669  0.02504264  0.02175176]]\n",
      "[[ 0.15595362  0.03885285  0.02511353  0.02826913  0.02949587  0.08078592\n",
      "   0.02684288  0.02342712  0.0320146   0.03653543  0.02218562  0.02332675\n",
      "   0.03005766  0.02762177  0.04329761  0.04588068  0.0234546   0.0206091\n",
      "   0.04180607  0.04579557  0.05037791  0.02810576  0.02492016  0.02466797\n",
      "   0.02190935  0.02551437  0.02317817]]\n",
      "[[ 0.14774925  0.03778414  0.02498536  0.02878299  0.02879221  0.08486807\n",
      "   0.02781891  0.02324158  0.0318229   0.03794964  0.02193648  0.02407525\n",
      "   0.0310328   0.02636548  0.0433824   0.04766218  0.02436974  0.02033044\n",
      "   0.04226309  0.04408097  0.04954142  0.02884367  0.02611443  0.02395082\n",
      "   0.02229146  0.02643665  0.02352763]]\n",
      "[[ 0.14559773  0.03485256  0.02480872  0.02978997  0.03261983  0.08146806\n",
      "   0.02759687  0.02278811  0.03199561  0.04026367  0.02170161  0.0225331\n",
      "   0.03039274  0.02720145  0.04476106  0.05030627  0.02223404  0.02146855\n",
      "   0.04190419  0.04435446  0.0532307   0.02705374  0.02649918  0.02407092\n",
      "   0.02177983  0.02735565  0.02137143]]\n",
      "[[ 0.14624625  0.03800698  0.025487    0.02729089  0.03021631  0.08333304\n",
      "   0.02699999  0.02312021  0.03235541  0.03913856  0.02136956  0.02357842\n",
      "   0.0312736   0.02658965  0.04238451  0.0492902   0.02327626  0.02129672\n",
      "   0.04199431  0.04555257  0.04885802  0.02851943  0.02627172  0.02517431\n",
      "   0.02230189  0.02698491  0.02308928]]\n",
      "[[ 0.14585842  0.03942216  0.02436637  0.02946559  0.0304049   0.08458158\n",
      "   0.02869189  0.0218749   0.02977915  0.03739972  0.02203675  0.02316976\n",
      "   0.0326764   0.0277097   0.04280309  0.04986693  0.02334372  0.0197931\n",
      "   0.04419166  0.04369908  0.0505722   0.02650318  0.02726299  0.02444591\n",
      "   0.02102042  0.02595768  0.02310272]]\n",
      "[[ 0.14527471  0.0400613   0.02465985  0.02951845  0.02908439  0.08084347\n",
      "   0.02884231  0.02353016  0.03121885  0.03715399  0.02250349  0.02302561\n",
      "   0.03198596  0.02831094  0.04371009  0.04670156  0.02422547  0.02094093\n",
      "   0.04383227  0.04330131  0.05081783  0.02824175  0.02722963  0.02477763\n",
      "   0.02197762  0.02480933  0.02342117]]\n",
      "[[ 0.14385211  0.04028891  0.02460961  0.0304151   0.02802628  0.07643251\n",
      "   0.03035701  0.02519673  0.03067171  0.0364325   0.02341084  0.02299091\n",
      "   0.03222012  0.02957618  0.04440166  0.0432715   0.02538231  0.02285526\n",
      "   0.04273462  0.04228161  0.05348064  0.0289013   0.02761828  0.02516184\n",
      "   0.02316867  0.02313954  0.02312227]]\n",
      "[[ 0.15338327  0.04056031  0.02483414  0.02812455  0.02651333  0.0845408\n",
      "   0.02794415  0.02398512  0.03097265  0.03575825  0.02277514  0.02418264\n",
      "   0.03090735  0.02633033  0.04280448  0.04355797  0.02548948  0.02196998\n",
      "   0.04144742  0.04423901  0.05074497  0.0291738   0.0260316   0.02457719\n",
      "   0.0219919   0.02358539  0.02357478]]\n",
      "[[ 0.15123442  0.03440349  0.02389373  0.03183128  0.03308711  0.07901224\n",
      "   0.02862408  0.02269337  0.03021838  0.03842897  0.02287823  0.02159563\n",
      "   0.02975619  0.02880387  0.04501213  0.04913462  0.02192365  0.02249606\n",
      "   0.0408375   0.04397601  0.05796229  0.02495994  0.02596259  0.02416282\n",
      "   0.02114604  0.02571157  0.02025379]]\n",
      "[[ 0.14037572  0.04083582  0.02556498  0.02780101  0.02668655  0.08426864\n",
      "   0.02798362  0.02542127  0.03278433  0.03758641  0.02238038  0.02474495\n",
      "   0.03150974  0.02604406  0.04301199  0.04429324  0.02634961  0.02246867\n",
      "   0.04183475  0.04351827  0.04799039  0.03116914  0.02709738  0.02544412\n",
      "   0.02290453  0.02557352  0.02435688]]\n",
      "[[ 0.14284642  0.03971637  0.02425034  0.03104282  0.02958518  0.08366116\n",
      "   0.02997819  0.02272744  0.02895357  0.03600762  0.02298866  0.02320684\n",
      "   0.03291209  0.02816416  0.04350185  0.04808001  0.02475848  0.0204855\n",
      "   0.04412572  0.04235637  0.05247419  0.02642171  0.02734463  0.02442378\n",
      "   0.02127566  0.02559089  0.02312042]]\n",
      "[[ 0.13989367  0.03897722  0.02532251  0.02837742  0.02926638  0.08363222\n",
      "   0.02792009  0.02394931  0.03269936  0.03907548  0.02197852  0.02342278\n",
      "   0.03170848  0.02636196  0.04306634  0.04812128  0.02442733  0.02183522\n",
      "   0.04351771  0.04347192  0.04922381  0.02946672  0.02727045  0.02490994\n",
      "   0.02235404  0.02653555  0.02321429]]\n",
      "[[ 0.14326268  0.04211986  0.02538715  0.02655023  0.02620275  0.08322673\n",
      "   0.02755524  0.02570964  0.0339692   0.03637305  0.02157656  0.0244045\n",
      "   0.03150031  0.0260353   0.04318047  0.04462969  0.02691182  0.02172426\n",
      "   0.04128968  0.04353846  0.04635708  0.03169758  0.02750267  0.02508929\n",
      "   0.0233309   0.02557716  0.02529776]]\n",
      "[[ 0.15236115  0.04176293  0.02406998  0.03047255  0.02781747  0.08150668\n",
      "   0.02982255  0.02376744  0.0298463   0.03404534  0.0221123   0.0235748\n",
      "   0.03156357  0.02744883  0.04555963  0.04366031  0.02573327  0.02168507\n",
      "   0.04197529  0.04142309  0.05161167  0.02679785  0.0278079   0.02387238\n",
      "   0.02153104  0.02462873  0.02354193]]\n",
      "[[ 0.15545332  0.04025466  0.02534085  0.02779133  0.02684126  0.08078561\n",
      "   0.02725261  0.02490502  0.03318     0.03519996  0.02217753  0.0241347\n",
      "   0.02992222  0.02680821  0.0436577   0.04296841  0.02550248  0.02218061\n",
      "   0.04010913  0.04469962  0.04912168  0.02963131  0.02583054  0.02457396\n",
      "   0.02274036  0.02510705  0.02382983]]\n",
      "[[ 0.16415872  0.03769703  0.02400623  0.03203483  0.03070056  0.07654923\n",
      "   0.02985318  0.02249327  0.02815429  0.03391397  0.02312285  0.02239507\n",
      "   0.03039423  0.02975256  0.04469091  0.0436164   0.02322594  0.02256264\n",
      "   0.04021842  0.04301327  0.05756512  0.02367733  0.02585941  0.02414428\n",
      "   0.02125776  0.02410477  0.02083766]]\n",
      "[[ 0.15648903  0.03828964  0.02493061  0.02926827  0.03069691  0.07847949\n",
      "   0.02768227  0.02294145  0.03109487  0.03641431  0.02259354  0.02234949\n",
      "   0.03051917  0.02877272  0.04247604  0.04595038  0.02283241  0.02176401\n",
      "   0.04178087  0.04569516  0.05321093  0.02659994  0.02559241  0.0251547\n",
      "   0.02205633  0.02473262  0.02163244]]\n",
      "[[ 0.15068649  0.03633311  0.02464573  0.02956019  0.033052    0.07892812\n",
      "   0.02722476  0.0230447   0.03228144  0.03915397  0.02164409  0.02183807\n",
      "   0.03022452  0.0281417   0.04357816  0.04836928  0.02170225  0.02217799\n",
      "   0.04195576  0.04556949  0.0534489   0.02675654  0.02610619  0.02478869\n",
      "   0.02172209  0.02614524  0.02092052]]\n",
      "[[ 0.14827076  0.03881776  0.02465078  0.02933291  0.03072657  0.07792444\n",
      "   0.02792902  0.02422751  0.03211001  0.03786162  0.02193386  0.02252121\n",
      "   0.03108024  0.02843823  0.04401261  0.04573401  0.02319423  0.02206362\n",
      "   0.04287469  0.04483176  0.05245208  0.0281      0.02658832  0.02485818\n",
      "   0.02218146  0.02497871  0.0223054 ]]\n",
      "[[ 0.14431651  0.04078156  0.02440877  0.02894372  0.02892135  0.08334696\n",
      "   0.02855083  0.02329927  0.03072072  0.03676035  0.02206524  0.02323252\n",
      "   0.03258681  0.02763698  0.04293181  0.04722841  0.02438811  0.02047982\n",
      "   0.04444193  0.04371205  0.05028437  0.02826041  0.0271785   0.02482669\n",
      "   0.02179975  0.02508646  0.02381016]]\n",
      "[[ 0.15163128  0.03866591  0.02429842  0.02951092  0.03031572  0.08044199\n",
      "   0.02804796  0.02316887  0.03109091  0.03693165  0.02216479  0.02237779\n",
      "   0.03068282  0.02800505  0.04444287  0.047013    0.02343098  0.02102507\n",
      "   0.04312827  0.04403191  0.05282333  0.02715201  0.02631556  0.02430027\n",
      "   0.02137606  0.02503065  0.02259594]]\n",
      "[[ 0.15860662  0.03992606  0.0246933   0.0279085   0.02773855  0.08492722\n",
      "   0.02720734  0.02311059  0.03075445  0.03603444  0.02226066  0.02396995\n",
      "   0.03030124  0.0259057   0.04305082  0.0445192   0.02450994  0.02114373\n",
      "   0.04157129  0.04485317  0.0509241   0.02795957  0.02557126  0.02405047\n",
      "   0.02099228  0.0242111   0.02329841]]\n",
      "[[ 0.16499777  0.03639831  0.02392911  0.03038739  0.03173898  0.07872112\n",
      "   0.02774942  0.02236695  0.02982109  0.03601409  0.02266447  0.02199308\n",
      "   0.02907615  0.02801213  0.04526781  0.04636267  0.02232332  0.02128477\n",
      "   0.04066703  0.04459246  0.05650887  0.02456625  0.02508623  0.02351062\n",
      "   0.0201994   0.02470011  0.0210604 ]]\n",
      "[[ 0.14756146  0.04148154  0.02519123  0.02777971  0.02654899  0.08482664\n",
      "   0.02796349  0.02487239  0.03211695  0.03632037  0.02208044  0.0245484\n",
      "   0.03131446  0.02579503  0.04320716  0.04357804  0.02609569  0.02188071\n",
      "   0.04145658  0.04367667  0.04834935  0.03017659  0.02681309  0.02493191\n",
      "   0.02225657  0.02492664  0.0242499 ]]\n",
      "[[ 0.15239105  0.0403743   0.02397498  0.03117315  0.02817556  0.07957236\n",
      "   0.03031155  0.02402122  0.03018865  0.03428976  0.02166185  0.02360038\n",
      "   0.03223836  0.02838637  0.04459153  0.04434305  0.02466083  0.02123884\n",
      "   0.04123411  0.04295072  0.05117558  0.02693766  0.02669804  0.02464175\n",
      "   0.02275581  0.02517575  0.02323683]]\n",
      "[[ 0.14750426  0.04011882  0.02459449  0.02995121  0.02788721  0.07918786\n",
      "   0.02954229  0.02456138  0.03132863  0.03551251  0.02234181  0.02312937\n",
      "   0.03183139  0.02852192  0.04413486  0.04450001  0.02506548  0.02171213\n",
      "   0.04247659  0.0431083   0.05153657  0.02850953  0.02683792  0.0251337\n",
      "   0.02323982  0.02468454  0.02304748]]\n",
      "[[ 0.15099165  0.04000669  0.02497569  0.02821912  0.02622245  0.08286607\n",
      "   0.02906879  0.024065    0.03051261  0.03528278  0.02355578  0.02375325\n",
      "   0.03110539  0.02733335  0.04240032  0.04524956  0.02613408  0.02140536\n",
      "   0.04164004  0.04375617  0.05190215  0.02864396  0.02616605  0.02468766\n",
      "   0.02285832  0.02360889  0.02358886]]\n",
      "[[ 0.14972207  0.0359817   0.02385638  0.03203018  0.03143748  0.08333402\n",
      "   0.02944458  0.02102493  0.02779425  0.0366607   0.02412276  0.02261336\n",
      "   0.03141178  0.02877512  0.04331305  0.05051457  0.02317054  0.01965346\n",
      "   0.0441495   0.04297806  0.05718087  0.02424618  0.02520192  0.02368427\n",
      "   0.02074419  0.0257343   0.02121985]]\n",
      "[[ 0.14684892  0.03956351  0.0255868   0.02776639  0.02791105  0.08361593\n",
      "   0.02744177  0.02386997  0.0319936   0.03745024  0.02278964  0.02440261\n",
      "   0.03110804  0.02672352  0.04215191  0.04605379  0.02478109  0.02092058\n",
      "   0.0428188   0.04460293  0.04902136  0.029661    0.0255207   0.02495283\n",
      "   0.02263991  0.02594619  0.02385691]]\n",
      "[[ 0.15413615  0.03540131  0.02450104  0.03026708  0.03332541  0.08084493\n",
      "   0.02796902  0.02099562  0.02973249  0.03795935  0.02278389  0.02209135\n",
      "   0.03039033  0.02928592  0.04199103  0.05204815  0.02129897  0.01983047\n",
      "   0.04191931  0.04542848  0.0541601   0.02421258  0.02485295  0.02503749\n",
      "   0.02172303  0.02674393  0.02106966]]\n",
      "[[ 0.14040308  0.04110523  0.02533061  0.02726392  0.0274433   0.08371106\n",
      "   0.02793185  0.02495038  0.03309865  0.03750334  0.02264035  0.0238074\n",
      "   0.03193303  0.02666542  0.0422674   0.04589724  0.02581515  0.0219757\n",
      "   0.0421993   0.04348908  0.04808681  0.03036816  0.02760143  0.02583891\n",
      "   0.02291663  0.02532698  0.02442964]]\n",
      "[[ 0.14312509  0.03893917  0.02431164  0.03093502  0.02948959  0.07755663\n",
      "   0.03007467  0.02494382  0.0317637   0.03701051  0.02251924  0.02266387\n",
      "   0.03184076  0.02910956  0.04526043  0.04504071  0.02465049  0.02268391\n",
      "   0.04172902  0.04203184  0.05218234  0.02809278  0.0279999   0.02511835\n",
      "   0.0230334   0.02492227  0.0229713 ]]\n",
      "[[ 0.1400322   0.03820787  0.02513017  0.02855744  0.0295731   0.08128943\n",
      "   0.02788941  0.02473073  0.03385186  0.03909566  0.02166656  0.02297184\n",
      "   0.03112212  0.0269465   0.04399503  0.04717925  0.02412342  0.02295728\n",
      "   0.0423185   0.04373291  0.04985635  0.02987809  0.02737666  0.02524313\n",
      "   0.02300211  0.02634612  0.02292627]]\n",
      "[[ 0.1431271   0.03595478  0.02421693  0.03121199  0.03292778  0.08295662\n",
      "   0.02801215  0.02150816  0.02989703  0.0394938   0.02238885  0.02253164\n",
      "   0.03124052  0.02809543  0.04433858  0.05130383  0.02205452  0.02073581\n",
      "   0.04544068  0.04361727  0.0544043   0.02540827  0.02595796  0.02404819\n",
      "   0.02074225  0.0270819   0.02130364]]\n",
      "[[ 0.13728288  0.0401879   0.02585226  0.02733671  0.02699454  0.08552518\n",
      "   0.02754931  0.02465897  0.03305415  0.03923682  0.02199984  0.0251235\n",
      "   0.03194474  0.02531746  0.04251193  0.04663057  0.02579886  0.02169414\n",
      "   0.04345725  0.0433864   0.04638729  0.03147035  0.02722981  0.02504488\n",
      "   0.0228109   0.02659038  0.02492296]]\n",
      "[[ 0.13844696  0.0411396   0.02491851  0.02929035  0.02883869  0.08117\n",
      "   0.02942177  0.024326    0.03100103  0.03722826  0.02253852  0.02289842\n",
      "   0.03271789  0.02755562  0.04431264  0.04690082  0.02560834  0.02218677\n",
      "   0.04324947  0.04155217  0.05122892  0.02875487  0.02926149  0.02467812\n",
      "   0.02192376  0.02501196  0.02383908]]\n",
      "[[ 0.13875209  0.03835138  0.02515008  0.02883769  0.03017017  0.08158784\n",
      "   0.02805687  0.02416527  0.03291991  0.03963892  0.02178125  0.02260363\n",
      "   0.03136971  0.02681865  0.04405777  0.04893436  0.0239467   0.0226841\n",
      "   0.0431399   0.04324611  0.050669    0.02901586  0.02809217  0.02483339\n",
      "   0.02211489  0.02629185  0.02277047]]\n",
      "[[ 0.13715315  0.03935722  0.02536372  0.02796322  0.02741691  0.0853601\n",
      "   0.02800068  0.02435006  0.0332003   0.03947683  0.02159471  0.02447322\n",
      "   0.03170748  0.02540675  0.04306601  0.04820439  0.02549614  0.02170331\n",
      "   0.04331784  0.04317223  0.04739915  0.03093321  0.02791314  0.02438319\n",
      "   0.02271896  0.02640065  0.02446745]]\n",
      "[[ 0.15888828  0.03651811  0.02484781  0.02844046  0.02855057  0.07874615\n",
      "   0.02720241  0.0241935   0.03312803  0.03637605  0.02211976  0.02349844\n",
      "   0.02852132  0.02736755  0.04495917  0.0463192   0.02413709  0.0218348\n",
      "   0.03911566  0.04550919  0.05173454  0.02807295  0.02497396  0.02396583\n",
      "   0.02264899  0.02544478  0.02288545]]\n",
      "[[ 0.15228197  0.03951186  0.02444954  0.02941631  0.02752006  0.07989493\n",
      "   0.02881437  0.02442887  0.03205953  0.03535906  0.02135869  0.02407246\n",
      "   0.03087466  0.02782679  0.04443971  0.04446289  0.02496798  0.02154301\n",
      "   0.04024284  0.04402034  0.04966966  0.02864639  0.02622523  0.02469547\n",
      "   0.02361742  0.02583454  0.02376541]]\n",
      "[[ 0.14750914  0.04205187  0.02517552  0.02691704  0.02557346  0.08244766\n",
      "   0.02793785  0.02578421  0.03364997  0.03510613  0.0214137   0.02450163\n",
      "   0.03114968  0.0265763   0.04344321  0.04353474  0.02716337  0.02165744\n",
      "   0.04010564  0.0435487   0.04666835  0.03121044  0.02703899  0.02521146\n",
      "   0.02398804  0.02536672  0.02526874]]\n",
      "[[ 0.15845214  0.03802811  0.02372522  0.03304181  0.03103947  0.07448646\n",
      "   0.03128316  0.02281259  0.02801047  0.03370043  0.02318965  0.02215097\n",
      "   0.03109083  0.03102288  0.04607555  0.04436233  0.02380248  0.02238908\n",
      "   0.04033829  0.04127711  0.05794628  0.02323752  0.02673922  0.02463143\n",
      "   0.02174189  0.02438878  0.02103596]]\n",
      "[[ 0.14084819  0.04140517  0.02559239  0.0282883   0.02624371  0.08164492\n",
      "   0.0287384   0.02574994  0.03254246  0.03659297  0.02317859  0.02412307\n",
      "   0.03159137  0.02717295  0.04260097  0.0434369   0.02669398  0.02312997\n",
      "   0.04221227  0.04306734  0.04912544  0.03069232  0.02726993  0.02565861\n",
      "   0.02363203  0.0246112   0.02415657]]\n",
      "[[ 0.15173645  0.04127549  0.02502905  0.0292614   0.02589786  0.0801716\n",
      "   0.03073262  0.02448315  0.0294776   0.03433042  0.024451    0.02334142\n",
      "   0.03151211  0.02810899  0.04251088  0.04280397  0.02689893  0.02377571\n",
      "   0.0405777   0.04203069  0.05366408  0.02757451  0.02732808  0.0251611\n",
      "   0.02298289  0.02200147  0.02288093]]\n",
      "[[ 0.14388643  0.04235888  0.0245682   0.02858837  0.02604021  0.08483241\n",
      "   0.02959089  0.02474454  0.03108997  0.03526489  0.02296258  0.02378964\n",
      "   0.03212165  0.02656849  0.04273796  0.04437438  0.02713493  0.02223992\n",
      "   0.04236253  0.04189865  0.04999186  0.02943564  0.02814383  0.02491416\n",
      "   0.02251771  0.02361938  0.02422192]]\n",
      "[[ 0.16007012  0.04064212  0.02391842  0.02927331  0.02821331  0.08305345\n",
      "   0.0286498   0.02242885  0.02933685  0.03392207  0.02327935  0.02261175\n",
      "   0.03044034  0.02747447  0.04304665  0.04596537  0.02487733  0.02078798\n",
      "   0.04205964  0.04315598  0.05346979  0.02561425  0.026262    0.02423618\n",
      "   0.0206653   0.02354128  0.02300413]]\n",
      "[[ 0.14623241  0.04287512  0.02455976  0.02784161  0.02687316  0.08346987\n",
      "   0.02904042  0.02466872  0.03179315  0.03495084  0.02278264  0.02299056\n",
      "   0.03196237  0.02679424  0.04257401  0.04519425  0.02637451  0.02173262\n",
      "   0.04198183  0.04238565  0.05000765  0.0290796   0.02848298  0.0252177\n",
      "   0.02209127  0.02372544  0.0243176 ]]\n",
      "[[ 0.1477865   0.04227331  0.02393384  0.03020619  0.02722761  0.08036374\n",
      "   0.03053971  0.02442947  0.03113189  0.03412053  0.02194303  0.0232641\n",
      "   0.03265154  0.0283184   0.04353135  0.04471008  0.02559016  0.02117995\n",
      "   0.04204627  0.04218193  0.0497646   0.02803617  0.02803169  0.02523699\n",
      "   0.02290764  0.02437289  0.02422037]]\n",
      "[[ 0.14411795  0.03793211  0.02427351  0.03022159  0.03019995  0.080307\n",
      "   0.02912662  0.02379917  0.03245963  0.03769266  0.02164816  0.02225399\n",
      "   0.03125874  0.0280774   0.04413776  0.04799883  0.0237088   0.02209043\n",
      "   0.04274393  0.04290618  0.05172318  0.02767969  0.02764761  0.02521594\n",
      "   0.02271864  0.02588689  0.0221736 ]]\n",
      "[[ 0.15367405  0.03996342  0.02481866  0.02792116  0.02736567  0.08495977\n",
      "   0.02747889  0.02358936  0.03158448  0.03648555  0.02208085  0.02417519\n",
      "   0.0305738   0.02591711  0.04288806  0.04479716  0.02489142  0.02171803\n",
      "   0.04148095  0.0445997   0.0500237   0.0287408   0.02621986  0.02449323\n",
      "   0.02165382  0.02458026  0.02332504]]\n",
      "[[ 0.15875079  0.0361735   0.02356902  0.03166857  0.03117827  0.08095995\n",
      "   0.02886489  0.02255972  0.02869665  0.03700699  0.02281235  0.02296296\n",
      "   0.02980469  0.02718236  0.04593996  0.04494125  0.02293249  0.02283588\n",
      "   0.0418414   0.04252829  0.05778205  0.02456097  0.02604216  0.02322899\n",
      "   0.01996179  0.02482271  0.02039129]]\n",
      "[[ 0.14749873  0.03985729  0.02537581  0.02757683  0.02802728  0.08298952\n",
      "   0.02715527  0.02490138  0.03277583  0.03762952  0.02211061  0.02418598\n",
      "   0.03083708  0.02559589  0.04322615  0.04442761  0.02502547  0.02255351\n",
      "   0.0419474   0.04442687  0.04932127  0.03005821  0.0264156   0.02471888\n",
      "   0.02220369  0.02577969  0.02337863]]\n",
      "[[ 0.14497256  0.03717118  0.02483145  0.02935809  0.02888089  0.0847172\n",
      "   0.0282924   0.02346905  0.03146164  0.03848526  0.0221378   0.02405369\n",
      "   0.03116613  0.02570055  0.04374011  0.04826948  0.02475463  0.02135128\n",
      "   0.04248844  0.04347083  0.05076304  0.02839143  0.02661255  0.02354215\n",
      "   0.02223294  0.02675963  0.02292561]]\n",
      "[[ 0.14181419  0.04075892  0.02485106  0.02807804  0.02720787  0.08614962\n",
      "   0.0282377   0.02421552  0.03183565  0.0372155   0.02185724  0.02428146\n",
      "   0.03195623  0.02526268  0.04350313  0.04662209  0.02601849  0.02116674\n",
      "   0.04317141  0.0428502   0.04859652  0.03006547  0.02782931  0.02412281\n",
      "   0.02215591  0.0257529   0.0244234 ]]\n",
      "[[ 0.14715223  0.04011957  0.02425617  0.02977311  0.02943494  0.08289692\n",
      "   0.0285975   0.02262518  0.03002025  0.03582418  0.02265217  0.02280248\n",
      "   0.0317214   0.0273783   0.04439614  0.0490322   0.02456433  0.01954126\n",
      "   0.0441495   0.04296975  0.05175439  0.0270475   0.02717925  0.02372334\n",
      "   0.02092959  0.02570967  0.02374875]]\n",
      "[[ 0.14271457  0.03836003  0.02499296  0.02859844  0.0303049   0.08253768\n",
      "   0.02760443  0.02344878  0.03257669  0.0391371   0.02176704  0.02279284\n",
      "   0.03115707  0.02631665  0.04386298  0.04960313  0.02359634  0.02106787\n",
      "   0.04349559  0.04377594  0.05028486  0.02872998  0.02727843  0.02438457\n",
      "   0.02178079  0.02681418  0.02301615]]\n",
      "[[ 0.14243945  0.0360178   0.02481073  0.02907791  0.0327416   0.08054425\n",
      "   0.02717352  0.02321873  0.03288222  0.04100256  0.02130258  0.02210558\n",
      "   0.0304129   0.02678142  0.04481412  0.05115557  0.02203272  0.02192981\n",
      "   0.04311723  0.04461339  0.05232387  0.02760283  0.02717394  0.02428528\n",
      "   0.02135364  0.02734737  0.02173899]]\n",
      "[[ 0.14707892  0.03842195  0.0250713   0.02788594  0.03015037  0.08092289\n",
      "   0.02670108  0.0241491   0.03326259  0.03904562  0.02143846  0.02306416\n",
      "   0.03023966  0.02621646  0.04440651  0.04732458  0.02326408  0.02211373\n",
      "   0.04257558  0.0453563   0.05064164  0.02902904  0.02663849  0.02433055\n",
      "   0.02158886  0.02618597  0.02289619]]\n",
      "[[ 0.15468892  0.03840957  0.02494212  0.02818609  0.02857437  0.08146422\n",
      "   0.02818681  0.0232895   0.03028782  0.03675148  0.02320312  0.02334209\n",
      "   0.03038293  0.02689246  0.04321053  0.04702721  0.02444802  0.02147829\n",
      "   0.04163344  0.04486593  0.05349385  0.0271003   0.02595417  0.02380802\n",
      "   0.02146489  0.02396409  0.02294973]]\n",
      "[[ 0.15069748  0.0417101   0.02493267  0.02822798  0.02616789  0.08320436\n",
      "   0.02914724  0.02439294  0.03135736  0.03551736  0.02201458  0.02478751\n",
      "   0.03165218  0.02621233  0.04303866  0.04381227  0.02610823  0.02216166\n",
      "   0.04058853  0.04345912  0.04885874  0.02901289  0.02689139  0.02465835\n",
      "   0.02278569  0.02457243  0.02403009]]\n",
      "[[ 0.15936142  0.03879859  0.02456379  0.02924012  0.02675194  0.08184642\n",
      "   0.03038298  0.02299091  0.0288177   0.03431478  0.0242492   0.02373821\n",
      "   0.0307356   0.02770236  0.04263442  0.04534467  0.02597347  0.02164854\n",
      "   0.03995027  0.043275    0.054294    0.02591653  0.02583198  0.02394202\n",
      "   0.02219531  0.02291948  0.02258022]]\n",
      "[[ 0.14776829  0.04099786  0.02439234  0.02846736  0.02639576  0.08615033\n",
      "   0.02967326  0.02331912  0.03000115  0.03523634  0.02317801  0.02422525\n",
      "   0.03244159  0.02678308  0.04184119  0.04640847  0.02643811  0.02040686\n",
      "   0.04231062  0.04278926  0.04993626  0.02828481  0.02714796  0.02473624\n",
      "   0.02220527  0.02426751  0.02419765]]\n",
      "[[ 0.14897515  0.03704038  0.02356707  0.03169801  0.03086628  0.08591406\n",
      "   0.02977002  0.02065267  0.0277154   0.03663296  0.02376407  0.02307615\n",
      "   0.03216582  0.0280582   0.04278977  0.05092585  0.02363076  0.01893169\n",
      "   0.04468297  0.04218806  0.05478057  0.02450126  0.02600328  0.02376471\n",
      "   0.02035968  0.02580042  0.02174477]]\n",
      "[[ 0.14396991  0.0398083   0.02549538  0.02717422  0.02827272  0.08540285\n",
      "   0.02771932  0.02336993  0.03172907  0.03794095  0.02232445  0.02442193\n",
      "   0.03212082  0.0263008   0.0415683   0.04764843  0.02502305  0.02076526\n",
      "   0.0430131   0.04387711  0.04750043  0.02956016  0.02644623  0.02544064\n",
      "   0.02248714  0.02645667  0.02416276]]\n",
      "[[ 0.14346232  0.04157942  0.02451035  0.02861962  0.02881728  0.08226251\n",
      "   0.02932651  0.02385932  0.03084015  0.03627416  0.02265921  0.02271669\n",
      "   0.0326221   0.02792701  0.04347026  0.04644498  0.02526495  0.02162674\n",
      "   0.04276233  0.04190774  0.05079014  0.02829886  0.02886221  0.02516914\n",
      "   0.02175272  0.02458462  0.02358862]]\n",
      "ufsrq moy ooatdb rliye hl chsgw iltethkifndrvm nbv bskcgyjxavk eptbxo  tfzflnrv \n",
      "[[ 0.06876371  0.02178657  0.02575536  0.05600524  0.01480093  0.0208733\n",
      "   0.01970948  0.05650488  0.05165844  0.07547507  0.00066201  0.03424949\n",
      "   0.0298157   0.02828752  0.01241209  0.07661934  0.04462039  0.05504131\n",
      "   0.00496343  0.06580515  0.01982244  0.00560811  0.04673679  0.01709326\n",
      "   0.07065759  0.03374697  0.04252544]]\n",
      "[[ 0.13338763  0.04171047  0.0259056   0.02784932  0.02768341  0.08086105\n",
      "   0.02860923  0.02628304  0.03264318  0.03720647  0.02324343  0.02364952\n",
      "   0.0326692   0.02753305  0.04330687  0.0445387   0.02729412  0.02332005\n",
      "   0.04210415  0.04229802  0.04872214  0.03120999  0.028594    0.02608444\n",
      "   0.0234876   0.02536492  0.02444038]]\n",
      "[[ 0.1406354   0.0408218   0.02482513  0.02975116  0.02958615  0.08032455\n",
      "   0.02867522  0.02405271  0.03076284  0.03603159  0.02340692  0.02245866\n",
      "   0.03211753  0.02902972  0.04419599  0.04732056  0.02547312  0.02101453\n",
      "   0.04374449  0.04285929  0.05100741  0.02804224  0.02757006  0.02506765\n",
      "   0.02186685  0.02540453  0.02395395]]\n",
      "[[ 0.14396645  0.04010044  0.02473728  0.02947388  0.02849701  0.08352645\n",
      "   0.02821127  0.02421425  0.0312923   0.03762271  0.02263565  0.02352623\n",
      "   0.03128114  0.02653522  0.04422747  0.04509928  0.02509811  0.02219807\n",
      "   0.04357432  0.04276087  0.05086182  0.02906731  0.02727086  0.02432533\n",
      "   0.02153523  0.02514758  0.02321342]]\n",
      "[[ 0.15122366  0.03846988  0.02457301  0.02931208  0.02991185  0.0804605\n",
      "   0.0275801   0.02385308  0.03155613  0.03724654  0.02239142  0.02274942\n",
      "   0.03011481  0.0270625   0.04464981  0.04617436  0.02384145  0.02197532\n",
      "   0.04247236  0.04409816  0.05288937  0.02781273  0.02632698  0.02416813\n",
      "   0.02138877  0.02531061  0.02238692]]\n",
      "[[ 0.14639218  0.03655487  0.02481581  0.02922104  0.03205285  0.08012592\n",
      "   0.02709417  0.02368373  0.03258844  0.0396563   0.02162094  0.02224175\n",
      "   0.03004137  0.02692487  0.04456834  0.04901072  0.02245393  0.02237101\n",
      "   0.04222894  0.0449963   0.05292774  0.02765805  0.02651142  0.0243469\n",
      "   0.02156822  0.02674388  0.02160031]]\n",
      "[[ 0.14173834  0.04086694  0.02493319  0.02773672  0.02816097  0.08428897\n",
      "   0.02758947  0.024579    0.03268199  0.03794559  0.02148385  0.02377447\n",
      "   0.03158087  0.0256597   0.04375571  0.04646095  0.0250574   0.02167773\n",
      "   0.04335101  0.04377985  0.04876075  0.03011396  0.0277763   0.02455588\n",
      "   0.0218682   0.02563608  0.02418605]]\n",
      "[[ 0.14999425  0.03312241  0.02411019  0.03429822  0.03835664  0.06943496\n",
      "   0.02926855  0.02168934  0.02838239  0.03826741  0.02395321  0.01978747\n",
      "   0.02973965  0.03327498  0.04728433  0.05179479  0.02051521  0.02165672\n",
      "   0.04223723  0.04377576  0.06315681  0.02171168  0.02537791  0.02418562\n",
      "   0.01999109  0.02567153  0.01896159]]\n",
      "[[ 0.13540637  0.03992988  0.02653223  0.02755568  0.02878811  0.08083002\n",
      "   0.02738435  0.02565115  0.0337831   0.03895551  0.0227305   0.02428003\n",
      "   0.03170506  0.02674403  0.04313906  0.04565463  0.02563845  0.02305618\n",
      "   0.04231623  0.0439133   0.04756686  0.03141418  0.0267533   0.02571565\n",
      "   0.02319279  0.02701485  0.02434857]]\n",
      "[[ 0.16093943  0.03840985  0.02463566  0.02828619  0.0290764   0.08298933\n",
      "   0.02728648  0.02270518  0.0304389   0.03643216  0.02278936  0.02360915\n",
      "   0.02979444  0.02626785  0.04348152  0.04552324  0.0239031   0.0220532\n",
      "   0.04119317  0.04476707  0.05210283  0.0264381   0.02507225  0.02400455\n",
      "   0.02070683  0.02459155  0.02250214]]\n",
      "[[ 0.15254994  0.0355455   0.02458661  0.0297396   0.03281841  0.08048092\n",
      "   0.02737305  0.02278079  0.0314681   0.03927585  0.02204275  0.02231797\n",
      "   0.02989032  0.02706848  0.0442774   0.04905983  0.02207426  0.02245384\n",
      "   0.04104853  0.044979    0.05385264  0.02583717  0.02594506  0.02412613\n",
      "   0.02095     0.02646661  0.02099127]]\n",
      "[[ 0.15292573  0.03918015  0.02561653  0.02697199  0.02846206  0.08075906\n",
      "   0.0263949   0.02512965  0.03356285  0.03740961  0.02199461  0.02427259\n",
      "   0.03001917  0.02621939  0.04320917  0.04390816  0.02449109  0.02256647\n",
      "   0.04014211  0.04591787  0.04900366  0.03002328  0.02544957  0.02474306\n",
      "   0.02241776  0.02586072  0.02334879]]\n",
      "[[ 0.15355544  0.0365414   0.02460152  0.03100604  0.03108376  0.0752401\n",
      "   0.02877897  0.02487055  0.03147042  0.03675897  0.02204521  0.02278619\n",
      "   0.03021901  0.02883121  0.04599321  0.0440868   0.02339455  0.02322521\n",
      "   0.03957443  0.04414135  0.05416885  0.02704701  0.02617589  0.02412675\n",
      "   0.02287415  0.0257057   0.02169731]]\n",
      "[[ 0.14435568  0.03776192  0.02540993  0.02818435  0.03000644  0.08070519\n",
      "   0.027269    0.02483065  0.03377306  0.03894145  0.02131498  0.02325318\n",
      "   0.03067134  0.0265402   0.044005    0.04639249  0.02382133  0.02312276\n",
      "   0.04117725  0.04491108  0.05000843  0.02974739  0.02667921  0.02483772\n",
      "   0.02299476  0.02680334  0.02248192]]\n",
      "[[ 0.14386344  0.04041483  0.02465137  0.02862726  0.02894143  0.08319926\n",
      "   0.028139    0.0241763   0.03187458  0.03746235  0.02135064  0.02331835\n",
      "   0.03170914  0.02627993  0.04441292  0.04630817  0.02467411  0.02185321\n",
      "   0.04315567  0.04336246  0.05010025  0.02899399  0.02795061  0.02414613\n",
      "   0.02178555  0.02576543  0.02348361]]\n",
      "[[ 0.16017997  0.03975018  0.02448271  0.02831806  0.02787475  0.08500061\n",
      "   0.02769123  0.02269844  0.03012423  0.03563063  0.02212443  0.02379055\n",
      "   0.03015736  0.02557813  0.04351272  0.04514278  0.02446739  0.02140524\n",
      "   0.04163094  0.04430667  0.05184356  0.02694124  0.02601759  0.02354676\n",
      "   0.02064706  0.02430323  0.02283354]]\n",
      "[[ 0.16323796  0.03741831  0.02415572  0.02966602  0.0305165   0.07989796\n",
      "   0.02784188  0.02262983  0.03031416  0.03605488  0.02230489  0.022442\n",
      "   0.02939875  0.02702562  0.04468418  0.04640823  0.02297222  0.02147765\n",
      "   0.04095742  0.04453532  0.05481438  0.02531454  0.02566125  0.02348609\n",
      "   0.02049741  0.02479227  0.0214946 ]]\n",
      "[[ 0.15459195  0.03904714  0.02467264  0.0289461   0.02871252  0.08357999\n",
      "   0.02771838  0.02383804  0.03107684  0.03746847  0.02187704  0.02379802\n",
      "   0.03043389  0.02536198  0.04419822  0.04415602  0.02403314  0.02234506\n",
      "   0.04186066  0.04412986  0.0519756   0.02786597  0.02645515  0.02368886\n",
      "   0.02101664  0.02494159  0.02221018]]\n",
      "[[ 0.14630023  0.03862496  0.02508282  0.02830528  0.02759725  0.08598761\n",
      "   0.02808083  0.02382084  0.03169545  0.03839012  0.02173119  0.02459135\n",
      "   0.03139401  0.02471998  0.04328006  0.04700807  0.02524346  0.02137853\n",
      "   0.0424028   0.04356885  0.04927504  0.02923839  0.02693204  0.02364886\n",
      "   0.02213358  0.02616908  0.02339928]]\n",
      "[[ 0.14993215  0.03901225  0.02451088  0.02930485  0.02934032  0.08324878\n",
      "   0.02832164  0.02259386  0.030143    0.03652249  0.02240517  0.02322207\n",
      "   0.03139688  0.02667758  0.04391617  0.04899398  0.02434001  0.01968183\n",
      "   0.04349633  0.04375713  0.05167991  0.02702449  0.02659694  0.02351004\n",
      "   0.02120355  0.02586679  0.02330089]]\n",
      "[[ 0.15053365  0.04008549  0.02521927  0.02746712  0.02642077  0.08511051\n",
      "   0.02842498  0.02357842  0.03053762  0.03608425  0.0235116   0.02415052\n",
      "   0.03105035  0.02595712  0.04214418  0.04697813  0.02604456  0.02052545\n",
      "   0.04188373  0.04403371  0.05057019  0.0287983   0.02634075  0.02403947\n",
      "   0.02201899  0.02427344  0.0242175 ]]\n",
      "[[ 0.15402938  0.03697518  0.02400048  0.03070357  0.02962356  0.08234139\n",
      "   0.02946022  0.02280878  0.02858619  0.03714766  0.02370573  0.02332217\n",
      "   0.03048603  0.02666021  0.04507806  0.04619987  0.02419808  0.02178525\n",
      "   0.04223086  0.04216617  0.05663534  0.02576564  0.02646614  0.02311269\n",
      "   0.02051067  0.02456907  0.02143168]]\n",
      "[[ 0.14451572  0.04109715  0.02557506  0.02710956  0.02568301  0.08507222\n",
      "   0.02854057  0.02529137  0.03139007  0.0366873   0.02344684  0.0244736\n",
      "   0.03150196  0.02549038  0.04189707  0.04452612  0.02686539  0.02252639\n",
      "   0.04136445  0.04355279  0.04946038  0.0304466   0.02718611  0.02506521\n",
      "   0.02267598  0.02429762  0.02426105]]\n",
      "[[ 0.15236688  0.03589591  0.02434335  0.03063155  0.03156276  0.08067808\n",
      "   0.02957305  0.02190097  0.02880514  0.03715578  0.02368289  0.02198073\n",
      "   0.03089477  0.02866455  0.04180493  0.05160445  0.02281615  0.02109438\n",
      "   0.04085185  0.0444941   0.05612127  0.02382372  0.02601226  0.02496036\n",
      "   0.0217276   0.02554478  0.02100774]]\n",
      "[[ 0.14890122  0.04027934  0.02550428  0.02750094  0.02700512  0.08535475\n",
      "   0.02757323  0.02412419  0.03182777  0.03736327  0.02288781  0.02484144\n",
      "   0.0312002   0.02567116  0.04180129  0.04504607  0.02543202  0.02171993\n",
      "   0.04087524  0.04477896  0.04878306  0.02952182  0.02626538  0.02494898\n",
      "   0.02203133  0.02499603  0.02376519]]\n",
      "[[ 0.15039454  0.03330649  0.02412617  0.03208728  0.03436542  0.07907184\n",
      "   0.02854981  0.02231627  0.03020592  0.03963608  0.02309368  0.02177236\n",
      "   0.02958682  0.02870416  0.0446799   0.05092456  0.02140613  0.02221462\n",
      "   0.0402754   0.04450423  0.05759671  0.02415001  0.02593853  0.02405873\n",
      "   0.02056683  0.02667364  0.01979383]]\n",
      "[[ 0.1410176   0.03997526  0.0258115   0.02728871  0.02694921  0.08426038\n",
      "   0.02766979  0.02538345  0.03263607  0.03834172  0.02310774  0.0248298\n",
      "   0.03124704  0.0259839   0.04198479  0.04538244  0.02591924  0.02227635\n",
      "   0.04154702  0.04439558  0.04822245  0.03091239  0.02681709  0.02533592\n",
      "   0.02268877  0.02559325  0.02442252]]\n",
      "[[ 0.14691791  0.03920269  0.02436181  0.03087655  0.02841974  0.0797866\n",
      "   0.03000061  0.02433928  0.03078417  0.03581906  0.02239932  0.02389408\n",
      "   0.03205041  0.02856611  0.04361646  0.04582832  0.02465777  0.02154594\n",
      "   0.04129979  0.04357339  0.0507562   0.02759064  0.02668862  0.02486904\n",
      "   0.0230475   0.02565561  0.02345239]]\n",
      "[[ 0.14407161  0.04162842  0.02484265  0.02907629  0.0261817   0.08206858\n",
      "   0.02938816  0.02491923  0.03226737  0.03560216  0.02178435  0.02459081\n",
      "   0.03215196  0.0276177   0.04314136  0.04400733  0.02600686  0.02162296\n",
      "   0.04183595  0.04303884  0.04739067  0.03005258  0.02703081  0.0255885\n",
      "   0.02401285  0.0256353   0.02444506]]\n",
      "[[ 0.14127041  0.03863183  0.02464002  0.02994905  0.02673947  0.0851963\n",
      "   0.02972559  0.02370853  0.03162187  0.03705581  0.02198796  0.02446304\n",
      "   0.0318968   0.02700635  0.04339649  0.04710962  0.0258954   0.02094861\n",
      "   0.04268136  0.04215147  0.04879007  0.02934466  0.02727285  0.02453329\n",
      "   0.02380458  0.02645404  0.02372453]]\n",
      "[[ 0.15035591  0.0395788   0.02421117  0.0303048   0.02754698  0.08310951\n",
      "   0.03030519  0.02283549  0.02926582  0.03563533  0.02278545  0.02383564\n",
      "   0.03164262  0.02770308  0.04347748  0.04545455  0.02510306  0.02174658\n",
      "   0.04226225  0.04211903  0.05297466  0.02678996  0.02704911  0.02470866\n",
      "   0.02234759  0.02451159  0.02233976]]\n",
      "[[ 0.14307807  0.04144603  0.0253666   0.02761773  0.02516393  0.08574048\n",
      "   0.02910654  0.02456523  0.03211472  0.03653002  0.02235359  0.02527309\n",
      "   0.03192433  0.02596342  0.0419648   0.04492107  0.02709214  0.02201517\n",
      "   0.04131095  0.04267988  0.0469848   0.03059639  0.0274924   0.02535254\n",
      "   0.02345644  0.02530552  0.02458406]]\n",
      "[[ 0.14572918  0.0350272   0.02363983  0.03384702  0.03232561  0.08266177\n",
      "   0.03058838  0.02065758  0.02716693  0.03728158  0.02417438  0.02272137\n",
      "   0.03172863  0.02962076  0.04401413  0.0511963   0.02322998  0.02029222\n",
      "   0.0440147   0.0415203   0.05743328  0.02314472  0.02589143  0.02388327\n",
      "   0.02082889  0.02685042  0.02053017]]\n",
      "[[ 0.13689701  0.0403897   0.02565317  0.0276552   0.02684388  0.08496236\n",
      "   0.02803093  0.02489571  0.03247999  0.03794216  0.022847    0.02488525\n",
      "   0.03202223  0.02628407  0.04246999  0.04582923  0.02658843  0.02184117\n",
      "   0.04312912  0.04293905  0.04708332  0.03103082  0.027206    0.0258953\n",
      "   0.02312414  0.02624512  0.02482959]]\n",
      "[[ 0.15039559  0.03878276  0.02403942  0.0314418   0.02982409  0.08049815\n",
      "   0.03035885  0.02251655  0.02819217  0.03583492  0.02369696  0.02312038\n",
      "   0.03198238  0.0291405   0.04396245  0.04604763  0.02418938  0.02201337\n",
      "   0.04292045  0.04175734  0.05482406  0.0250887   0.02671313  0.02515698\n",
      "   0.02117195  0.024542    0.02178796]]\n",
      "[[ 0.14251202  0.04153338  0.02559963  0.02747254  0.02560407  0.08498004\n",
      "   0.02859316  0.02512471  0.0316718   0.03652972  0.02360769  0.02451869\n",
      "   0.0317379   0.02626556  0.04172831  0.0443691   0.02691202  0.02272068\n",
      "   0.04161371  0.04326633  0.0487994   0.03041854  0.02726343  0.0257522\n",
      "   0.02286041  0.0242119   0.02433307]]\n",
      "[[ 0.16501956  0.03869544  0.02410784  0.02930801  0.02736113  0.0842689\n",
      "   0.02875911  0.02231712  0.02853463  0.03441925  0.02420597  0.02352425\n",
      "   0.02975358  0.02670307  0.04281246  0.04453376  0.02497456  0.02206724\n",
      "   0.04059437  0.04335442  0.05463647  0.02496916  0.02525709  0.02406379\n",
      "   0.02054491  0.02309185  0.02212214]]\n",
      "[[ 0.15210141  0.03695865  0.02457324  0.02928942  0.03020559  0.08278765\n",
      "   0.02797917  0.02319275  0.03142824  0.03810348  0.02264389  0.02291276\n",
      "   0.03029113  0.02653491  0.04317426  0.0476812   0.02351896  0.02231584\n",
      "   0.04108521  0.04409695  0.05291219  0.02668238  0.02627665  0.02451646\n",
      "   0.02147163  0.02571552  0.02155046]]\n",
      "[[ 0.14663337  0.03757732  0.02527326  0.02776298  0.02731177  0.0857282\n",
      "   0.02825559  0.02390204  0.03120163  0.03869191  0.02363148  0.02453113\n",
      "   0.03100801  0.02545722  0.04167956  0.04743865  0.02563697  0.02168724\n",
      "   0.04159268  0.04404656  0.05122486  0.02893606  0.02633139  0.02408417\n",
      "   0.02231142  0.02490639  0.02315806]]\n",
      "[[ 0.15063314  0.04066175  0.02449818  0.02869148  0.02638628  0.08604205\n",
      "   0.02919941  0.02382449  0.03007815  0.0358442   0.02289706  0.0247207\n",
      "   0.03153224  0.02555089  0.0430565   0.0446027   0.0262541   0.0216094\n",
      "   0.04160165  0.04270539  0.05078615  0.02835852  0.02692111  0.02388534\n",
      "   0.02175445  0.02425701  0.02364762]]\n",
      "[[ 0.14670233  0.04083318  0.02505996  0.02789799  0.02519983  0.08661575\n",
      "   0.02967471  0.02415521  0.03112913  0.0359191   0.02260417  0.02535319\n",
      "   0.03201585  0.02539471  0.04194448  0.0455918   0.02723888  0.02157381\n",
      "   0.04051773  0.0425822   0.04821464  0.02938587  0.02740869  0.02452295\n",
      "   0.02312     0.02502571  0.02431819]]\n",
      "[[ 0.15252228  0.04181061  0.02411397  0.02962664  0.02554743  0.08618454\n",
      "   0.03053591  0.02347157  0.02940909  0.03413671  0.02262351  0.02481165\n",
      "   0.03182707  0.02603742  0.04346345  0.04381526  0.02692548  0.02153526\n",
      "   0.041063    0.04143497  0.0504046   0.02757125  0.02754666  0.0239748\n",
      "   0.02200571  0.02401791  0.02358326]]\n",
      "[[ 0.15122168  0.04115555  0.02407654  0.02928179  0.02700219  0.08473317\n",
      "   0.02927739  0.02305645  0.03002097  0.03452164  0.02284807  0.02370331\n",
      "   0.03174268  0.02701497  0.04322305  0.04629519  0.02591375  0.02005502\n",
      "   0.04265565  0.04249187  0.05058773  0.02751975  0.02714628  0.02429905\n",
      "   0.02165783  0.02464389  0.02385465]]\n",
      "[[ 0.15243223  0.03853254  0.02441471  0.02908003  0.02964793  0.08341116\n",
      "   0.02865313  0.02185284  0.03018677  0.03636372  0.02240501  0.02284804\n",
      "   0.03146274  0.02777401  0.04187263  0.05008132  0.02366843  0.01974612\n",
      "   0.04216203  0.04394657  0.05121995  0.02605603  0.02642134  0.0251531\n",
      "   0.02196686  0.02598185  0.02265896]]\n",
      "[[ 0.14701332  0.03691562  0.0246499   0.02922967  0.03132606  0.08236545\n",
      "   0.02781243  0.02262363  0.03240754  0.03928623  0.02174534  0.02255919\n",
      "   0.03075726  0.02727255  0.04332804  0.04994668  0.02269538  0.02123016\n",
      "   0.04247981  0.04404585  0.05109804  0.02709888  0.02699469  0.02499132\n",
      "   0.02175744  0.02656673  0.02180272]]\n",
      "[[ 0.15540709  0.03688978  0.0246671   0.02910269  0.03074142  0.08007651\n",
      "   0.02715277  0.02260199  0.03203873  0.03765015  0.02209902  0.02302857\n",
      "   0.02977878  0.02843007  0.04356607  0.04788225  0.02253564  0.02078029\n",
      "   0.04190028  0.04576323  0.05143657  0.02674328  0.02502744  0.02482121\n",
      "   0.02195224  0.02585312  0.02207381]]\n",
      "[[ 0.1518248   0.04115712  0.02454489  0.02856739  0.02701455  0.0845581\n",
      "   0.02813057  0.02380111  0.03146224  0.0360456   0.02183774  0.02442811\n",
      "   0.03115347  0.02642171  0.04354895  0.04377611  0.02514646  0.02135563\n",
      "   0.04192993  0.04357729  0.04911355  0.02884707  0.02664502  0.02470798\n",
      "   0.02187218  0.02473952  0.02379293]]\n",
      "[[ 0.15048358  0.03915778  0.02412914  0.03107362  0.02845811  0.07872128\n",
      "   0.02986969  0.02441477  0.03105138  0.03593167  0.02203426  0.02334731\n",
      "   0.03138093  0.02827523  0.0455465   0.04356872  0.02463352  0.02230888\n",
      "   0.04139831  0.04217052  0.05234918  0.02769374  0.02724467  0.02458954\n",
      "   0.02268322  0.0248408   0.02264371]]\n",
      "[[ 0.14420722  0.03786354  0.02491382  0.02895484  0.02951444  0.0812927\n",
      "   0.027989    0.0243152   0.03320402  0.03859815  0.02142925  0.02304636\n",
      "   0.03089479  0.02679737  0.04428738  0.04681532  0.02387623  0.02271967\n",
      "   0.0420842   0.04372192  0.05072043  0.02908837  0.02714062  0.02498423\n",
      "   0.02281504  0.02637797  0.02234781]]\n",
      "[[ 0.15161686  0.03745082  0.02455828  0.02913584  0.03044642  0.07927725\n",
      "   0.02750977  0.02386563  0.03239002  0.03786364  0.02163576  0.02255065\n",
      "   0.0298427   0.02732774  0.04503244  0.04660986  0.02310413  0.02242957\n",
      "   0.04181067  0.04466308  0.05307522  0.02759356  0.02638307  0.02441577\n",
      "   0.02187059  0.02572173  0.02181893]]\n",
      "[[ 0.16008398  0.03923881  0.0247232   0.0279299   0.02796494  0.08391946\n",
      "   0.02707404  0.02324324  0.03091881  0.03632713  0.02202281  0.02395067\n",
      "   0.0298972   0.02573204  0.0433468   0.04451376  0.02414857  0.02170433\n",
      "   0.0410292   0.04523939  0.05160189  0.02760422  0.02552035  0.02401724\n",
      "   0.02110777  0.02443251  0.0227077 ]]\n",
      "[[ 0.16259696  0.03629571  0.0236957   0.03114544  0.03109761  0.08102135\n",
      "   0.02842404  0.02255091  0.02863865  0.03700697  0.02249696  0.02305634\n",
      "   0.02960496  0.02651975  0.04573493  0.04449441  0.02263543  0.02272518\n",
      "   0.04141389  0.04324862  0.05770886  0.02442843  0.02567107  0.02293207\n",
      "   0.01982966  0.02471584  0.02031026]]\n",
      "[[ 0.14594825  0.03936852  0.02559998  0.0272141   0.02647286  0.08583373\n",
      "   0.02784014  0.02479175  0.03158645  0.03842291  0.02302142  0.02510499\n",
      "   0.03127886  0.02481697  0.04188957  0.04525832  0.0259554   0.0220903\n",
      "   0.04177755  0.0440111   0.04929833  0.03019683  0.02656374  0.02439088\n",
      "   0.02235168  0.02508161  0.02383373]]\n",
      "[[ 0.14631374  0.03284383  0.02449754  0.03137586  0.03337628  0.0795197\n",
      "   0.02864206  0.02281069  0.03018459  0.04064988  0.02322178  0.02224733\n",
      "   0.03002545  0.02754965  0.0444475   0.05145608  0.02210486  0.02251036\n",
      "   0.04099762  0.04410688  0.05795555  0.02515662  0.02613031  0.02368975\n",
      "   0.02120947  0.0269054   0.02007128]]\n",
      "[[ 0.14075111  0.0397695   0.02580835  0.02711386  0.02682107  0.08432981\n",
      "   0.02774721  0.02540562  0.03241576  0.03850866  0.02314132  0.02488871\n",
      "   0.03136168  0.02562362  0.04188284  0.04578534  0.02603362  0.02224775\n",
      "   0.04169727  0.04435416  0.04860257  0.03102927  0.02677937  0.0251305\n",
      "   0.02282218  0.02558831  0.02436057]]\n",
      "[[ 0.15647396  0.03343218  0.02439158  0.03176137  0.03232868  0.07580372\n",
      "   0.02869885  0.02253281  0.02957987  0.03679369  0.02457171  0.02224544\n",
      "   0.02918697  0.03032389  0.04421611  0.05022481  0.02245379  0.02098223\n",
      "   0.04012198  0.04587065  0.05762335  0.02399942  0.02364993  0.02411023\n",
      "   0.02206117  0.02586799  0.02069357]]\n",
      "[[ 0.13888623  0.04030774  0.02620643  0.02727211  0.02665565  0.08302116\n",
      "   0.02804602  0.02559015  0.03300432  0.03801516  0.02289713  0.02517007\n",
      "   0.03160425  0.0263868   0.04245968  0.04495461  0.02679166  0.02242662\n",
      "   0.04129512  0.04368928  0.0468789   0.03139629  0.02671811  0.02566804\n",
      "   0.02342856  0.02638767  0.02484233]]\n",
      "[[ 0.14587894  0.03947406  0.0245109   0.03074194  0.027938    0.07983726\n",
      "   0.03027206  0.02438632  0.030782    0.0355643   0.02212327  0.02404196\n",
      "   0.03224077  0.02892617  0.04384995  0.04536642  0.02522649  0.02155071\n",
      "   0.0411029   0.04318307  0.04962971  0.02801973  0.02660863  0.02520787\n",
      "   0.02378692  0.02603266  0.02371694]]\n",
      "[[ 0.14435808  0.03942652  0.02470686  0.03017151  0.02809712  0.07876565\n",
      "   0.02967027  0.02466718  0.03131631  0.03612933  0.02272491  0.02324396\n",
      "   0.03184927  0.02921093  0.04413034  0.04491849  0.02523529  0.02194062\n",
      "   0.04221976  0.04302359  0.05147045  0.02876418  0.02679989  0.02535654\n",
      "   0.02365849  0.02503688  0.02310753]]\n",
      "[[ 0.15339825  0.03753841  0.02456581  0.02988398  0.02915046  0.07850181\n",
      "   0.02830095  0.02340287  0.03139533  0.03583832  0.02275302  0.02294207\n",
      "   0.03022985  0.02968397  0.04368769  0.0464435   0.02365757  0.02096525\n",
      "   0.0416113   0.0452105   0.05252483  0.02741881  0.02474473  0.02511412\n",
      "   0.02335706  0.02528766  0.02239186]]\n",
      "[[ 0.1574267   0.03409395  0.02418327  0.03219927  0.03507653  0.07245274\n",
      "   0.02809413  0.02166013  0.02966067  0.03682572  0.02357097  0.02059981\n",
      "   0.02934306  0.03321805  0.04474445  0.04909043  0.02071947  0.02094213\n",
      "   0.04124851  0.04604811  0.05932723  0.02361076  0.02401234  0.02515943\n",
      "   0.02200521  0.02514196  0.01954493]]\n",
      "[[ 0.14259449  0.04006036  0.02542029  0.02745553  0.02810821  0.08244439\n",
      "   0.02734004  0.0248195   0.03292848  0.03773237  0.02243197  0.02403875\n",
      "   0.03144586  0.02724691  0.04288375  0.0454348   0.02524566  0.02139695\n",
      "   0.04264687  0.04420984  0.04860137  0.03041939  0.02635863  0.02581249\n",
      "   0.02289386  0.0256837   0.0243455 ]]\n",
      "[[ 0.15155898  0.03631492  0.02371613  0.03143588  0.03220306  0.07906299\n",
      "   0.02855125  0.02320549  0.02952139  0.03794757  0.02285494  0.02263329\n",
      "   0.03036917  0.02851569  0.04635768  0.04538982  0.02274751  0.02225337\n",
      "   0.0429565   0.04251746  0.05703609  0.02570339  0.02609122  0.02406807\n",
      "   0.02058912  0.02542089  0.02097816]]\n",
      "[[ 0.14198308  0.03902707  0.02548346  0.02755667  0.02946722  0.08264004\n",
      "   0.02696096  0.02479487  0.03348245  0.03922242  0.02168843  0.02373594\n",
      "   0.03113322  0.02584324  0.04354608  0.04616769  0.02423846  0.02272088\n",
      "   0.04262253  0.04421971  0.04903762  0.03042629  0.02670169  0.02498387\n",
      "   0.02240798  0.02669888  0.02320926]]\n",
      "[[ 0.14386134  0.03396802  0.02451704  0.03043381  0.03507027  0.07838414\n",
      "   0.02735056  0.02318417  0.03198523  0.04140879  0.02155321  0.02159321\n",
      "   0.02992227  0.02785962  0.04557804  0.05085548  0.02106208  0.02303498\n",
      "   0.04216377  0.0446929   0.05556279  0.02610651  0.02648399  0.02427539\n",
      "   0.02115344  0.02761438  0.02032451]]\n",
      "[[ 0.14497623  0.03967916  0.02587383  0.02695861  0.02841967  0.08109633\n",
      "   0.02655624  0.02573675  0.03398202  0.03821449  0.02195622  0.0244782\n",
      "   0.03067248  0.02619277  0.04316194  0.04415387  0.02482466  0.02296304\n",
      "   0.04111803  0.04536669  0.04828974  0.03131377  0.02610806  0.0250453\n",
      "   0.02269412  0.02638804  0.02377972]]\n",
      "[[ 0.1471003   0.03240294  0.02461172  0.03176785  0.03649553  0.07545126\n",
      "   0.02728813  0.02337925  0.031763    0.04073406  0.02210289  0.02134883\n",
      "   0.02877926  0.02931681  0.04609758  0.0502485   0.02045788  0.02352544\n",
      "   0.04038531  0.04566691  0.05762571  0.02503043  0.02567276  0.02402409\n",
      "   0.0211768   0.02785924  0.01968756]]\n",
      "[[ 0.13858399  0.04050119  0.02579007  0.02774186  0.027624    0.08190902\n",
      "   0.02772931  0.02569846  0.03352292  0.03816715  0.02203412  0.02460134\n",
      "   0.03138887  0.02669531  0.04331952  0.04458109  0.02583757  0.02278165\n",
      "   0.04220776  0.04376766  0.04722921  0.03140414  0.02687284  0.02563916\n",
      "   0.02341417  0.02663394  0.02432368]]\n",
      "[[ 0.14387389  0.03492271  0.02405371  0.03317764  0.03456067  0.0801388\n",
      "   0.02870098  0.0214697   0.02868271  0.03865896  0.02316497  0.02228756\n",
      "   0.0310213   0.02966969  0.0452358   0.05081762  0.02184598  0.02087876\n",
      "   0.04505992  0.0432878   0.0565256   0.02385665  0.02518958  0.02396115\n",
      "   0.02065572  0.02784314  0.02045899]]\n",
      "[[ 0.14074923  0.0397402   0.02559134  0.02819733  0.02838344  0.08593194\n",
      "   0.02726308  0.02354332  0.03190812  0.03884858  0.02213216  0.0247534\n",
      "   0.03173692  0.02592988  0.04253527  0.04714132  0.02467358  0.02099555\n",
      "   0.04428792  0.04394515  0.04756984  0.02996224  0.02637887  0.02506554\n",
      "   0.02214428  0.02669307  0.02389844]]\n",
      "[[ 0.15001319  0.03242053  0.02448615  0.03445253  0.0386671   0.07144176\n",
      "   0.02866127  0.02078498  0.02790462  0.03911418  0.02439288  0.02053839\n",
      "   0.0296615   0.03352121  0.04631279  0.0526139   0.02008427  0.0209558\n",
      "   0.04274889  0.04408925  0.06158887  0.02181858  0.02410686  0.02434556\n",
      "   0.02007896  0.02649422  0.01870173]]\n",
      "[[ 0.13539433  0.03977812  0.02657677  0.02752705  0.02887469  0.08117656\n",
      "   0.02724974  0.02547475  0.03364187  0.03918736  0.02279986  0.02441071\n",
      "   0.03171188  0.02678931  0.04307189  0.04569098  0.02554693  0.02293185\n",
      "   0.04248134  0.04392888  0.04742622  0.03140618  0.02656858  0.02576652\n",
      "   0.02319759  0.02711991  0.02427007]]\n",
      "[[ 0.14245704  0.03686839  0.02493328  0.03129081  0.0327308   0.07398342\n",
      "   0.02897583  0.02424628  0.03091968  0.03831088  0.02337515  0.02204316\n",
      "   0.03150587  0.03070349  0.04570295  0.04704623  0.0232683   0.02272212\n",
      "   0.04255934  0.04351857  0.05451458  0.02710635  0.02624838  0.02495492\n",
      "   0.02221055  0.02593234  0.02187127]]\n",
      "[[ 0.14185759  0.03994326  0.02564966  0.02763252  0.02850597  0.08140606\n",
      "   0.02710222  0.02527752  0.03346471  0.03805991  0.0222935   0.02377916\n",
      "   0.03117711  0.02678996  0.04340708  0.04506614  0.02504614  0.02253726\n",
      "   0.04222095  0.04461765  0.04879928  0.03104914  0.02641578  0.02514824\n",
      "   0.02286455  0.02595256  0.023936  ]]\n",
      "[[ 0.1433683   0.03386965  0.02448492  0.03141932  0.03534661  0.07638208\n",
      "   0.02769518  0.02338588  0.03175909  0.04054353  0.02225724  0.02119055\n",
      "   0.02977959  0.02941102  0.045915    0.05041743  0.02124348  0.02296695\n",
      "   0.04160377  0.04481316  0.05650264  0.02596631  0.02616705  0.02441257\n",
      "   0.02146417  0.02721454  0.02041994]]\n",
      "[[ 0.14148547  0.03951711  0.02567112  0.02745352  0.02888607  0.08179746\n",
      "   0.02683721  0.0253199   0.03383181  0.03875299  0.02194999  0.02390761\n",
      "   0.03091714  0.02625061  0.04338345  0.04554594  0.02463919  0.02273983\n",
      "   0.04204832  0.04502836  0.04872327  0.03098109  0.02653338  0.02503623\n",
      "   0.02263452  0.02637511  0.02375328]]\n",
      "[[ 0.16483037  0.03751434  0.02431441  0.02895083  0.02989978  0.08206464\n",
      "   0.02724686  0.02243182  0.02987164  0.03610796  0.02262229  0.02322062\n",
      "   0.0291253   0.02652112  0.04413965  0.04534487  0.02312279  0.02198263\n",
      "   0.04098411  0.04506177  0.0544286   0.02534505  0.02481404  0.02351761\n",
      "   0.02032637  0.02442458  0.02178592]]\n",
      "[[ 0.1490806   0.04066351  0.02538499  0.02733708  0.0263572   0.08512047\n",
      "   0.02829625  0.0243514   0.03188297  0.03666561  0.02216455  0.02506303\n",
      "   0.03143024  0.02550273  0.04226097  0.04506543  0.02617708  0.0219659\n",
      "   0.04048496  0.04393344  0.047661    0.02953438  0.02668106  0.02471281\n",
      "   0.02263947  0.02534127  0.02424152]]\n",
      "[[ 0.15257742  0.03613678  0.02424196  0.0328013   0.03171146  0.0746287\n",
      "   0.03091386  0.02327724  0.02876054  0.03584739  0.02375734  0.02215342\n",
      "   0.0312592   0.03072634  0.04575002  0.04649949  0.02387427  0.02215709\n",
      "   0.0403306   0.04264146  0.05719125  0.02456308  0.02628849  0.02412756\n",
      "   0.02197826  0.02479602  0.02100942]]\n",
      "[[ 0.13933815  0.04062537  0.02546276  0.02756267  0.02671819  0.08407559\n",
      "   0.02828774  0.02511834  0.03248263  0.03728348  0.02261493  0.02461916\n",
      "   0.03203169  0.02654825  0.04267112  0.04530676  0.0266527   0.02184882\n",
      "   0.04240765  0.04320298  0.04746389  0.03105444  0.027193    0.02570733\n",
      "   0.02319631  0.02570856  0.02481752]]\n",
      "vopt xw e sm xetpbofpjre fkkbgunlgje qhuhor dhm tepq fdukidwop  s knnw it teuilv\n",
      "[[ 0.04854526  0.04622374  0.0299028   0.0141764   0.03877251  0.00474917\n",
      "   0.05274811  0.02466274  0.02315964  0.01434499  0.06405327  0.03492565\n",
      "   0.03845031  0.06021922  0.04766026  0.0114374   0.02859263  0.04751163\n",
      "   0.00290357  0.02885484  0.05352961  0.04071365  0.06513113  0.07389909\n",
      "   0.0308907   0.05124631  0.02269539]]\n",
      "[[ 0.13654067  0.04066845  0.02621174  0.02770906  0.02624721  0.08401953\n",
      "   0.02856752  0.0254549   0.03284858  0.03745089  0.02285874  0.02542247\n",
      "   0.0320724   0.02648485  0.04229802  0.04500104  0.02749247  0.02261883\n",
      "   0.04091422  0.04307215  0.04635156  0.0315263   0.02744645  0.02570261\n",
      "   0.02401529  0.02621367  0.02479037]]\n",
      "[[ 0.138952    0.04069008  0.02487054  0.03011112  0.02815801  0.08386936\n",
      "   0.02952113  0.02407442  0.03031046  0.03637019  0.02299269  0.02380534\n",
      "   0.0324787   0.02761941  0.04423976  0.04608344  0.02658677  0.02180391\n",
      "   0.04283717  0.04149583  0.04984116  0.02856268  0.02838402  0.02459263\n",
      "   0.02215106  0.02578763  0.02381044]]\n",
      "[[ 0.13949794  0.03908681  0.02557867  0.02823657  0.02599824  0.08678754\n",
      "   0.02889756  0.02413607  0.03130209  0.03781947  0.02397024  0.02478787\n",
      "   0.0314885   0.02593379  0.04192834  0.04724045  0.02723661  0.02161594\n",
      "   0.04237113  0.04268845  0.04954901  0.03028412  0.02732991  0.02412716\n",
      "   0.02289217  0.02500336  0.02421205]]\n",
      "[[ 0.13760254  0.03773672  0.02526592  0.02879509  0.02640151  0.08781447\n",
      "   0.02906923  0.02361019  0.03152296  0.03866789  0.02330899  0.02510023\n",
      "   0.03166928  0.02543633  0.04238332  0.04906447  0.02693771  0.0209228\n",
      "   0.04237829  0.04225913  0.04901385  0.03020248  0.02741775  0.02380137\n",
      "   0.02307164  0.02628038  0.02426546]]\n",
      "[[ 0.15577438  0.03835723  0.02472004  0.02815598  0.02645536  0.0877849\n",
      "   0.02813488  0.02254888  0.03016556  0.03624495  0.0235109   0.02500728\n",
      "   0.03000194  0.02510367  0.04266835  0.04642653  0.02576596  0.02102047\n",
      "   0.04069598  0.04373632  0.05154558  0.02786111  0.0258641   0.02339372\n",
      "   0.02125931  0.02457928  0.02321726]]\n",
      "[[ 0.15081367  0.03454772  0.02434581  0.03022212  0.03146512  0.08269009\n",
      "   0.02821522  0.02233613  0.03065548  0.0392308   0.0229508   0.02295851\n",
      "   0.02982131  0.02665775  0.04414793  0.05030853  0.02283106  0.02175961\n",
      "   0.04042327  0.0438781   0.05545083  0.0258126   0.02629842  0.02372606\n",
      "   0.02104343  0.02655013  0.02085951]]\n",
      "[[ 0.14611241  0.03807371  0.02497504  0.02860602  0.02931976  0.08681641\n",
      "   0.02725669  0.02250595  0.03106122  0.03918515  0.02220342  0.02445584\n",
      "   0.03117562  0.02552903  0.04262857  0.04905878  0.02364269  0.02044952\n",
      "   0.04347097  0.04449591  0.05005699  0.02810367  0.02620845  0.02424083\n",
      "   0.021056    0.0265872   0.02272418]]\n",
      "[[ 0.14596678  0.03888876  0.02467729  0.02942733  0.02849966  0.0815967\n",
      "   0.02866791  0.02423632  0.03160771  0.0380033   0.02208303  0.02399609\n",
      "   0.03150233  0.02657154  0.04424263  0.04599809  0.02449999  0.02172376\n",
      "   0.04245931  0.0431307   0.05082427  0.02886127  0.02708215  0.02436497\n",
      "   0.02237707  0.02573889  0.02297222]]\n",
      "[[ 0.14286593  0.04181617  0.02551234  0.02665852  0.02595387  0.08304219\n",
      "   0.02767884  0.02601535  0.03372392  0.03639436  0.02180146  0.02473894\n",
      "   0.03157701  0.02593786  0.04321072  0.04407316  0.02716598  0.02191837\n",
      "   0.04106947  0.04343004  0.04660072  0.03190479  0.02729117  0.02521244\n",
      "   0.02361608  0.02560021  0.02519007]]\n",
      "[[ 0.15267132  0.04172918  0.02412244  0.03040948  0.02736724  0.08174639\n",
      "   0.02984284  0.02395744  0.02975158  0.03393686  0.0221862   0.02386146\n",
      "   0.03161586  0.02717857  0.04546798  0.04326059  0.02593858  0.02172359\n",
      "   0.04172786  0.04143986  0.05159935  0.02703914  0.02760264  0.02392565\n",
      "   0.02178534  0.02460641  0.02350614]]\n",
      "[[ 0.14851739  0.04167115  0.02455467  0.02854153  0.02713436  0.08396074\n",
      "   0.02837537  0.02377398  0.03130205  0.03500523  0.02222922  0.02372131\n",
      "   0.03167992  0.02680561  0.04348835  0.04573787  0.02581405  0.02045238\n",
      "   0.04303746  0.04297326  0.04923356  0.0287562   0.02723121  0.02461815\n",
      "   0.02202893  0.02497424  0.02438184]]\n",
      "[[ 0.14664711  0.03641414  0.02412214  0.03100867  0.03215858  0.0797276\n",
      "   0.02857516  0.02285015  0.03129717  0.03832474  0.02204691  0.0218191\n",
      "   0.03056447  0.02818321  0.04500996  0.04999116  0.02273127  0.02121289\n",
      "   0.04276803  0.04309139  0.05419524  0.02618973  0.02723126  0.02440711\n",
      "   0.02146218  0.02656857  0.02140212]]\n",
      "[[ 0.14657414  0.04084326  0.02489795  0.02844455  0.02777136  0.08367532\n",
      "   0.02826519  0.02400591  0.031459    0.03688831  0.02212014  0.02378095\n",
      "   0.03138826  0.02656582  0.04287431  0.04540944  0.0249419   0.0219112\n",
      "   0.04267103  0.04389912  0.05002843  0.0290399   0.02710452  0.02505186\n",
      "   0.02207454  0.02493189  0.02338164]]\n",
      "[[ 0.14959963  0.04109671  0.02399315  0.03047897  0.02784225  0.08036575\n",
      "   0.02994629  0.02393837  0.03077523  0.03507893  0.02153512  0.02367497\n",
      "   0.03218668  0.02827157  0.043822    0.04477701  0.02459372  0.02138972\n",
      "   0.04230612  0.04309266  0.05019518  0.02768137  0.02706775  0.02506527\n",
      "   0.02285939  0.02501943  0.02334677]]\n",
      "[[ 0.15928985  0.04028071  0.02447085  0.02870031  0.02677251  0.08501317\n",
      "   0.02815704  0.02294613  0.03039195  0.03498041  0.02227766  0.02430372\n",
      "   0.03042443  0.02629984  0.04303615  0.04400054  0.024909    0.02136281\n",
      "   0.04133778  0.04420828  0.05076778  0.02746858  0.02569316  0.02433274\n",
      "   0.0215608   0.02400613  0.02300771]]\n",
      "[[ 0.15728076  0.03583479  0.02345901  0.03283281  0.03238359  0.08365005\n",
      "   0.02896338  0.02014342  0.02733441  0.03619254  0.02328701  0.02275705\n",
      "   0.03078117  0.02832985  0.04388736  0.04982301  0.02210827  0.0195649\n",
      "   0.04388739  0.04317689  0.05724698  0.02278687  0.02481389  0.02325468\n",
      "   0.01969658  0.02607709  0.02044621]]\n",
      "[[ 0.14065708  0.04104798  0.02568594  0.02793253  0.02657459  0.08536541\n",
      "   0.027927    0.02489959  0.03234722  0.03733096  0.02262863  0.02493281\n",
      "   0.03167041  0.02598144  0.04276977  0.04450905  0.0265296   0.02197493\n",
      "   0.04239488  0.04333073  0.04742453  0.03078709  0.02689474  0.02550016\n",
      "   0.02273256  0.02576882  0.02440156]]\n",
      "[[ 0.1508652   0.03625693  0.02444502  0.03108755  0.03227583  0.08161771\n",
      "   0.02868193  0.02137132  0.0295899   0.03753689  0.02267637  0.02224698\n",
      "   0.0307989   0.02894243  0.04261021  0.05176142  0.02218747  0.02039438\n",
      "   0.04195305  0.04451069  0.05374083  0.02414541  0.02559197  0.02529657\n",
      "   0.02150838  0.02669704  0.02120967]]\n",
      "[[ 0.14555401  0.04024748  0.0255388   0.02748735  0.02654334  0.08532238\n",
      "   0.02788832  0.02424755  0.03195391  0.03732568  0.02334171  0.0246368\n",
      "   0.03107706  0.02632313  0.04158117  0.04606342  0.02574467  0.02148351\n",
      "   0.04161582  0.04432942  0.04868554  0.02966398  0.02650594  0.02526948\n",
      "   0.02251051  0.02480575  0.02425325]]\n",
      "[[ 0.1554504   0.03573791  0.02429709  0.0311106   0.0314345   0.07837152\n",
      "   0.0285536   0.02274108  0.03005884  0.03698074  0.02391831  0.02220229\n",
      "   0.02940317  0.02924842  0.04488602  0.04791753  0.02301476  0.02157417\n",
      "   0.04087041  0.04392759  0.0563361   0.02492592  0.02555146  0.02426374\n",
      "   0.02097912  0.02503316  0.02121147]]\n",
      "[[ 0.14275774  0.03925083  0.02578049  0.02741319  0.02653093  0.08526771\n",
      "   0.02788601  0.02483096  0.03224277  0.03847681  0.02349032  0.02493043\n",
      "   0.03113665  0.02573571  0.04180931  0.04588168  0.02608804  0.02180177\n",
      "   0.04165797  0.04401145  0.04889812  0.03062438  0.02660075  0.02475287\n",
      "   0.02266009  0.0251092   0.02437385]]\n",
      "[[ 0.14621633  0.03541897  0.02468637  0.03226836  0.03151046  0.0754562\n",
      "   0.03013504  0.02372621  0.02938525  0.03727096  0.02483143  0.02228524\n",
      "   0.03103702  0.03051795  0.04522508  0.04762365  0.02398633  0.02191177\n",
      "   0.04114917  0.04289724  0.05764757  0.02593823  0.02613352  0.02420843\n",
      "   0.02205665  0.02493274  0.02154383]]\n",
      "[[ 0.13769917  0.03930096  0.02601084  0.02741553  0.02846861  0.08225166\n",
      "   0.02737761  0.02526887  0.03381308  0.03903725  0.02249754  0.02403472\n",
      "   0.03152908  0.02626326  0.0430195   0.04630939  0.02526318  0.02269747\n",
      "   0.04214501  0.04404911  0.04836375  0.03146121  0.02675937  0.02525817\n",
      "   0.02318962  0.02659206  0.02392399]]\n",
      "[[ 0.1613595   0.03775982  0.02445492  0.02850146  0.02905931  0.08327232\n",
      "   0.02757347  0.02258552  0.03001016  0.03635497  0.02301322  0.02353716\n",
      "   0.02965279  0.02633744  0.04363608  0.04570015  0.02384271  0.02191594\n",
      "   0.04090749  0.04452062  0.05358245  0.02619603  0.02514212  0.02375271\n",
      "   0.02076891  0.02444174  0.02212102]]\n",
      "[[ 0.15457511  0.03992618  0.0244414   0.02836404  0.02894836  0.08342944\n",
      "   0.02766434  0.02294935  0.03066089  0.03586172  0.02261653  0.02334534\n",
      "   0.03100585  0.02677944  0.04324071  0.04698673  0.02430922  0.02020702\n",
      "   0.04239799  0.04433143  0.05098318  0.02720273  0.02607742  0.02414359\n",
      "   0.02099403  0.02500376  0.02355419]]\n",
      "[[ 0.15786622  0.03811686  0.02432734  0.02924989  0.03057266  0.08024467\n",
      "   0.02759215  0.0229688   0.03092047  0.03641806  0.02248153  0.02246973\n",
      "   0.0300301   0.02735904  0.04448574  0.04684046  0.02324841  0.02071766\n",
      "   0.04174437  0.04448925  0.05328957  0.02645491  0.0258078   0.02390894\n",
      "   0.02088642  0.02520205  0.02230693]]\n",
      "[[ 0.15950742  0.03907378  0.0252726   0.02733608  0.02840354  0.07972743\n",
      "   0.02643204  0.02466022  0.03305264  0.03602875  0.02210947  0.02367869\n",
      "   0.02940339  0.02678037  0.04377809  0.04365157  0.02427411  0.02163849\n",
      "   0.03986299  0.0460447   0.0501223   0.02907555  0.0248889   0.02438165\n",
      "   0.02219758  0.025282    0.02333566]]\n",
      "[[ 0.15918638  0.03642483  0.0246164   0.03233773  0.03180829  0.07033134\n",
      "   0.03012391  0.0248076   0.0292311   0.03504681  0.02358646  0.02198383\n",
      "   0.03060067  0.03142641  0.0462705   0.0417307   0.02363181  0.02349922\n",
      "   0.03856542  0.04323095  0.05872562  0.02563403  0.02582117  0.02424292\n",
      "   0.02276078  0.02346424  0.02091093]]\n",
      "[[ 0.13883638  0.04079906  0.0256724   0.0272595   0.02700182  0.08198205\n",
      "   0.02818488  0.02626416  0.03314528  0.03701165  0.0228308   0.02397268\n",
      "   0.03185083  0.02679745  0.04291476  0.04397365  0.02698949  0.0230587\n",
      "   0.04134491  0.04318404  0.04862899  0.03153293  0.0272633   0.02585654\n",
      "   0.02384728  0.02531026  0.02448625]]\n",
      "[[ 0.14563814  0.04056157  0.02417324  0.03064216  0.0280236   0.07834009\n",
      "   0.03043694  0.02508698  0.03118823  0.03482725  0.0219415   0.02329555\n",
      "   0.0326095   0.02911608  0.04425991  0.04430159  0.02535484  0.02210419\n",
      "   0.04111601  0.04278168  0.05055752  0.02827154  0.02718223  0.02537089\n",
      "   0.02396312  0.02507241  0.02378325]]\n",
      "[[ 0.14392845  0.04199225  0.02459581  0.02925052  0.02611711  0.08137988\n",
      "   0.02973252  0.02509976  0.0322975   0.03502223  0.02154071  0.02423915\n",
      "   0.03236778  0.02802657  0.04334836  0.04363998  0.02619061  0.02176938\n",
      "   0.04177443  0.04272999  0.04763104  0.03007968  0.02721673  0.02578522\n",
      "   0.02439218  0.02532416  0.02452797]]\n",
      "[[ 0.143676    0.04204276  0.02401137  0.02996202  0.0270214   0.08390689\n",
      "   0.02982492  0.02397506  0.03070005  0.03485348  0.02176944  0.02343139\n",
      "   0.0325763   0.02769256  0.04427047  0.04491105  0.0260237   0.02118231\n",
      "   0.04348766  0.04148659  0.0496499   0.02870513  0.02818662  0.02491223\n",
      "   0.02268058  0.02499369  0.02406643]]\n",
      "[[ 0.13955472  0.04327444  0.02453142  0.02830794  0.02645752  0.08307038\n",
      "   0.02958929  0.02510348  0.03185289  0.03535817  0.02223847  0.02304937\n",
      "   0.03266391  0.02723123  0.04343307  0.04487382  0.02691875  0.02208424\n",
      "   0.04294428  0.04137836  0.04945029  0.03010569  0.02930231  0.0254896\n",
      "   0.02293412  0.02421789  0.02458435]]\n",
      "[[ 0.14972219  0.04143108  0.02379945  0.03098365  0.0285407   0.08008789\n",
      "   0.03105049  0.02320287  0.02851301  0.03437984  0.0230435   0.0222096\n",
      "   0.0323076   0.02894599  0.04401587  0.04482436  0.02493548  0.02242696\n",
      "   0.04308904  0.0411012   0.05502967  0.02573117  0.02821485  0.02522235\n",
      "   0.02161037  0.02320345  0.02237735]]\n",
      "[[ 0.14302294  0.03984337  0.02493191  0.0285272   0.02871862  0.08245847\n",
      "   0.02829684  0.02422762  0.0328545   0.03793066  0.02188506  0.02299176\n",
      "   0.03148988  0.02668908  0.04296067  0.04672066  0.0245661   0.02263197\n",
      "   0.04308199  0.04315033  0.04980633  0.02926865  0.02759418  0.02535813\n",
      "   0.02254242  0.02543292  0.02301776]]\n",
      "[[ 0.14622051  0.03823966  0.02421399  0.0311301   0.03090234  0.07597635\n",
      "   0.02957568  0.0239455   0.03095976  0.03699793  0.02267648  0.02188845\n",
      "   0.03139864  0.0297859   0.04490126  0.04652824  0.02365607  0.02239007\n",
      "   0.04320598  0.0428911   0.05493207  0.02678504  0.02725073  0.02495194\n",
      "   0.02212456  0.02453976  0.02193191]]\n",
      "[[ 0.14080845  0.03951817  0.02550871  0.02768082  0.02653824  0.08441884\n",
      "   0.02828654  0.02484524  0.03220031  0.03830192  0.0231139   0.0244973\n",
      "   0.03154944  0.02616823  0.04191563  0.04615465  0.0261758   0.02195117\n",
      "   0.04262694  0.04368773  0.04931737  0.03086818  0.0269359   0.02479771\n",
      "   0.02300051  0.02475936  0.02437292]]\n",
      "[[ 0.14337805  0.03928068  0.02510788  0.02833838  0.02603959  0.08533044\n",
      "   0.02970053  0.0239795   0.03132593  0.03688497  0.0228441   0.02491216\n",
      "   0.03190842  0.02624652  0.04181232  0.04734814  0.02672811  0.02158902\n",
      "   0.04095021  0.04299302  0.04920318  0.02940155  0.02721185  0.02459926\n",
      "   0.02348562  0.02511168  0.02428888]]\n",
      "[[ 0.15939364  0.03888723  0.02419255  0.02902841  0.02813282  0.08396311\n",
      "   0.02873737  0.02203961  0.02944763  0.03504351  0.02332306  0.02346888\n",
      "   0.03025354  0.02733255  0.04278594  0.04707967  0.02471633  0.02060173\n",
      "   0.04107364  0.04376847  0.05297651  0.02568022  0.02582561  0.02404013\n",
      "   0.02113882  0.02414336  0.02292563]]\n",
      "[[ 0.16264153  0.04018807  0.02428113  0.02806168  0.02809489  0.08460118\n",
      "   0.02754004  0.02219727  0.03009799  0.03490178  0.02281146  0.02353689\n",
      "   0.03020409  0.02670693  0.04242335  0.04578938  0.02450719  0.02015997\n",
      "   0.04129111  0.04440545  0.05143401  0.02633212  0.02558865  0.0242238\n",
      "   0.02062823  0.0240644   0.02328745]]\n",
      "[[ 0.16256301  0.03706869  0.02414495  0.02957232  0.03208453  0.08239306\n",
      "   0.02794731  0.02068435  0.02939777  0.03617087  0.02245786  0.02215725\n",
      "   0.03036558  0.02828747  0.04182298  0.05063982  0.02197006  0.01946096\n",
      "   0.04089349  0.04514303  0.05326149  0.0236431   0.02534182  0.02470753\n",
      "   0.02070393  0.02561045  0.02150627]]\n",
      "[[ 0.1586556   0.04031855  0.02475483  0.02759663  0.02872529  0.08461382\n",
      "   0.02694761  0.02254785  0.03125916  0.03619097  0.02230079  0.02368795\n",
      "   0.03062966  0.02667805  0.04215849  0.04586286  0.02411791  0.02022981\n",
      "   0.04149049  0.04501261  0.04946718  0.02715479  0.02576271  0.0248094\n",
      "   0.02093788  0.02470966  0.02337945]]\n",
      "[[ 0.15286382  0.04190192  0.02420847  0.02894047  0.02925978  0.08164813\n",
      "   0.02895764  0.02344466  0.03089529  0.0348808   0.02250955  0.0223037\n",
      "   0.03180743  0.02784907  0.04370271  0.04533395  0.02459187  0.02126849\n",
      "   0.04151282  0.04255861  0.05168697  0.02684878  0.02854646  0.02480083\n",
      "   0.02077088  0.02378305  0.02312383]]\n",
      "[[ 0.14687262  0.03826773  0.02458709  0.02940996  0.0314112   0.08131795\n",
      "   0.02805988  0.0235054   0.03262051  0.03838599  0.0217088   0.02192236\n",
      "   0.03091484  0.02734076  0.04421722  0.04792456  0.0230912   0.02213254\n",
      "   0.04243835  0.04352153  0.05144331  0.02735445  0.02792227  0.02473428\n",
      "   0.02123815  0.02560432  0.02205271]]\n",
      "[[ 0.14277916  0.04118266  0.02456352  0.02835157  0.02837037  0.08492743\n",
      "   0.02830897  0.02345991  0.03167073  0.03705081  0.02186288  0.02343454\n",
      "   0.03236982  0.02681861  0.0429295   0.04748647  0.02479635  0.02071005\n",
      "   0.04428074  0.04339683  0.04865371  0.02876788  0.02800841  0.02496697\n",
      "   0.02146191  0.02517667  0.02421351]]\n",
      "[[ 0.14920956  0.03760776  0.02442641  0.02965136  0.03190958  0.0824386\n",
      "   0.02833262  0.02139835  0.03038787  0.03779227  0.02193365  0.02198421\n",
      "   0.03141218  0.02854902  0.0424092   0.05207964  0.02231497  0.0200462\n",
      "   0.04341246  0.04434223  0.05180073  0.0251102   0.02656835  0.02521287\n",
      "   0.0213181   0.02615555  0.02219604]]\n",
      "[[ 0.14477442  0.04179422  0.02490885  0.028558    0.02709281  0.08299604\n",
      "   0.0286674   0.02406706  0.03270021  0.03661613  0.02154743  0.02425811\n",
      "   0.03191547  0.02725337  0.0427506   0.04542001  0.02516166  0.02113539\n",
      "   0.04289455  0.04327795  0.04674898  0.02956522  0.02732356  0.02571478\n",
      "   0.02293488  0.02552339  0.02439955]]\n",
      "[[ 0.14545611  0.03524075  0.02394766  0.0321876   0.03330743  0.0781551\n",
      "   0.0289854   0.02261783  0.03143576  0.0392482   0.02173897  0.02162708\n",
      "   0.0304041   0.02974385  0.04567171  0.04977463  0.02177985  0.02206721\n",
      "   0.04290577  0.04305342  0.05417126  0.02518714  0.02712112  0.02516762\n",
      "   0.02174353  0.02657969  0.02068117]]\n",
      "[[ 0.13737577  0.04070245  0.02597665  0.02748466  0.02678261  0.08334767\n",
      "   0.02804288  0.02549126  0.03345015  0.0382899   0.02223641  0.02498841\n",
      "   0.03170278  0.02640504  0.04272817  0.04508281  0.02644225  0.02262292\n",
      "   0.04195731  0.04335312  0.04629553  0.03161014  0.02745195  0.02575861\n",
      "   0.02327569  0.0263206   0.02482426]]\n",
      "[[ 0.139109    0.0414828   0.02481448  0.02950296  0.02839256  0.08043392\n",
      "   0.02977964  0.02490066  0.03131019  0.03634349  0.0225343   0.02265919\n",
      "   0.03239121  0.02847782  0.04439599  0.04545101  0.02600223  0.02306022\n",
      "   0.04234657  0.04175471  0.05085063  0.02874236  0.02949169  0.0252778\n",
      "   0.02228634  0.02465841  0.02354982]]\n",
      "[[ 0.1500897   0.03848099  0.02472896  0.02976087  0.02900939  0.0795325\n",
      "   0.02838592  0.02363298  0.03176006  0.03609664  0.02272163  0.02277597\n",
      "   0.03044026  0.02922368  0.04370378  0.04666336  0.0242388   0.02165141\n",
      "   0.04190445  0.04452024  0.05161026  0.02759692  0.02601162  0.02503341\n",
      "   0.02255379  0.02504396  0.02282854]]\n",
      "[[ 0.15797022  0.0386425   0.02511212  0.02801482  0.02803506  0.07901245\n",
      "   0.02677782  0.02467173  0.03348667  0.03567464  0.02230782  0.02360365\n",
      "   0.02900645  0.02808339  0.04384901  0.0437433   0.02444385  0.02203954\n",
      "   0.03962546  0.04588083  0.0503541   0.029032    0.02476902  0.024773\n",
      "   0.02287395  0.02498123  0.02323526]]\n",
      "[[ 0.15659076  0.03649486  0.02445581  0.03259604  0.03095837  0.07065978\n",
      "   0.0302474   0.02492381  0.02969778  0.03489239  0.02371269  0.02214629\n",
      "   0.03034328  0.03226754  0.04619596  0.04156582  0.02402736  0.02388488\n",
      "   0.03859834  0.04302974  0.05837246  0.02613166  0.02582426  0.02468822\n",
      "   0.02331648  0.02336096  0.02101712]]\n",
      "[[ 0.1399526   0.04068839  0.02551679  0.02765709  0.02673488  0.08319283\n",
      "   0.02810497  0.02576743  0.03307209  0.03707531  0.02244849  0.02446731\n",
      "   0.03168872  0.02662508  0.0432336   0.04381511  0.02680934  0.02259055\n",
      "   0.04159101  0.04304468  0.04824857  0.03157753  0.0270998   0.02559785\n",
      "   0.02354331  0.02529245  0.0245643 ]]\n",
      "[[ 0.14111778  0.04195264  0.0245551   0.02950425  0.02820221  0.0795984\n",
      "   0.02999602  0.0252965   0.03123243  0.03533705  0.02272854  0.02242136\n",
      "   0.03252237  0.02854154  0.04451356  0.04445982  0.02626102  0.02290979\n",
      "   0.04162136  0.04138511  0.05225082  0.02897678  0.02922248  0.02524141\n",
      "   0.02254278  0.02402052  0.02358833]]\n",
      "[[ 0.14018834  0.03845554  0.02478449  0.02937028  0.03024349  0.08073309\n",
      "   0.02855423  0.02456795  0.03297715  0.03846045  0.02184419  0.02211401\n",
      "   0.03120067  0.02759181  0.04412545  0.04783432  0.02411112  0.02305156\n",
      "   0.04223917  0.04315357  0.05167518  0.02884406  0.0281679   0.02510575\n",
      "   0.0224564   0.02573459  0.02241523]]\n",
      "[[ 0.14281926  0.03922506  0.02460322  0.02984288  0.02974463  0.07786134\n",
      "   0.02894419  0.0247497   0.03226455  0.03734285  0.0222788   0.02229811\n",
      "   0.03125225  0.02874048  0.04440888  0.04634274  0.02425543  0.02252164\n",
      "   0.04300996  0.04352698  0.05288549  0.02857359  0.02758315  0.02496067\n",
      "   0.02253432  0.02468873  0.02274104]]\n",
      "[[ 0.13847761  0.03984312  0.02529581  0.02813246  0.02706048  0.08439636\n",
      "   0.0282825   0.02470913  0.03308415  0.03835904  0.02182493  0.024314\n",
      "   0.03168334  0.02607008  0.04297508  0.0469268   0.02584257  0.0216659\n",
      "   0.0430465   0.0432601   0.04795223  0.03110543  0.02750294  0.02462252\n",
      "   0.02314914  0.02584109  0.02457663]]\n",
      "[[ 0.13985072  0.03391932  0.02442645  0.03139251  0.03323995  0.07897847\n",
      "   0.02864652  0.02305665  0.03165423  0.04065706  0.02214294  0.02181754\n",
      "   0.03034936  0.02853513  0.04540188  0.05200681  0.0223242   0.02231086\n",
      "   0.04231316  0.04346867  0.05579676  0.02631534  0.02709926  0.02427079\n",
      "   0.02189992  0.027161    0.0209645 ]]\n",
      "[[ 0.1378136   0.04097496  0.02581689  0.02771958  0.026727    0.08297962\n",
      "   0.02816994  0.0255111   0.03326714  0.0378582   0.02214588  0.02488306\n",
      "   0.03168751  0.02630582  0.04308395  0.04470385  0.02657074  0.02282831\n",
      "   0.04190107  0.04327755  0.04713332  0.03156362  0.02731737  0.02555265\n",
      "   0.0233635   0.02630717  0.02453651]]\n",
      "[[ 0.14308856  0.03766441  0.02460404  0.03297541  0.03015092  0.07169315\n",
      "   0.03192854  0.02555375  0.02961465  0.03650388  0.02390011  0.02274363\n",
      "   0.03189677  0.03159463  0.04643086  0.04284563  0.02514646  0.02475356\n",
      "   0.04050124  0.04128638  0.05689973  0.02694349  0.02732735  0.02522529\n",
      "   0.0235211   0.02377643  0.02143002]]\n",
      "[[ 0.13671951  0.04100826  0.02582065  0.02797468  0.02644887  0.08157329\n",
      "   0.02840189  0.02632753  0.03345491  0.03743051  0.02268912  0.02445549\n",
      "   0.03174614  0.02694456  0.04321688  0.04334405  0.0269736   0.02346977\n",
      "   0.04202085  0.04291352  0.04809761  0.03205188  0.02722774  0.02569577\n",
      "   0.02412788  0.02546333  0.02440169]]\n",
      "[[ 0.15312742  0.03491597  0.02425903  0.03233736  0.03128169  0.07423544\n",
      "   0.0292272   0.02334987  0.03047641  0.03560057  0.02394143  0.02193306\n",
      "   0.02955899  0.03156542  0.04529445  0.04757911  0.02321985  0.02210985\n",
      "   0.04058032  0.04470644  0.05674705  0.02502794  0.02416143  0.02498768\n",
      "   0.02353092  0.02536414  0.02088095]]\n",
      "[[ 0.14272913  0.03972262  0.02524283  0.02840171  0.02779199  0.08082958\n",
      "   0.02798566  0.02519086  0.03279898  0.03713097  0.02274786  0.02365217\n",
      "   0.03116748  0.02770758  0.04316562  0.04485676  0.02547713  0.02220293\n",
      "   0.04236633  0.0440484   0.04998806  0.03028639  0.02613343  0.02553171\n",
      "   0.02375215  0.02535946  0.02373222]]\n",
      "[[ 0.14660068  0.0369633   0.02420198  0.03221191  0.03156631  0.07354291\n",
      "   0.02981844  0.02423139  0.03024214  0.03623594  0.02356149  0.02169259\n",
      "   0.0312249   0.03153997  0.04577025  0.04582607  0.02361076  0.02215724\n",
      "   0.04207359  0.04314073  0.05694263  0.02651573  0.02599592  0.02501357\n",
      "   0.02297151  0.02479468  0.02155334]]\n",
      "[[ 0.13924904  0.03918253  0.02556366  0.02787948  0.02912088  0.08132604\n",
      "   0.02740289  0.02507941  0.0338488   0.03869098  0.02201931  0.02335706\n",
      "   0.03133033  0.02690381  0.0433749   0.04631938  0.02465335  0.02264382\n",
      "   0.04245182  0.04420313  0.04913558  0.03101937  0.02666865  0.02539377\n",
      "   0.02333919  0.02634755  0.02349526]]\n",
      "[[ 0.14308637  0.03985301  0.02500645  0.02813479  0.02721699  0.0837721\n",
      "   0.02895123  0.02431034  0.0322681   0.03725249  0.02181391  0.02433097\n",
      "   0.0318265   0.02662013  0.04258038  0.0467797   0.02572425  0.02203132\n",
      "   0.04117827  0.04348897  0.04855519  0.02980577  0.02727184  0.02489928\n",
      "   0.02357715  0.02565596  0.02400848]]\n",
      "[[ 0.14285547  0.03440507  0.02432312  0.03118488  0.03276359  0.07948751\n",
      "   0.02912022  0.02304759  0.03148981  0.03974398  0.02204299  0.02206526\n",
      "   0.03059442  0.02874469  0.04483559  0.05061863  0.02259051  0.02284093\n",
      "   0.04108046  0.04352669  0.05498087  0.02599504  0.02712782  0.02467961\n",
      "   0.02234641  0.02672616  0.02078274]]\n",
      "[[ 0.1388139   0.03828424  0.02561237  0.02756342  0.02942684  0.08246107\n",
      "   0.02718017  0.02472139  0.0342857   0.03999512  0.02147315  0.02371246\n",
      "   0.03108826  0.02609051  0.04330624  0.04771515  0.02418579  0.02311444\n",
      "   0.04217491  0.04452094  0.04860768  0.03046044  0.02705998  0.02518349\n",
      "   0.02280795  0.02696374  0.02319064]]\n",
      "[[ 0.14273359  0.03456399  0.0241603   0.03185227  0.03486183  0.08172134\n",
      "   0.02792888  0.02095097  0.02941383  0.04051428  0.02262275  0.02232525\n",
      "   0.03099122  0.02844838  0.04463373  0.05302402  0.02126912  0.02089861\n",
      "   0.04535232  0.04382147  0.05599059  0.02418454  0.0256302   0.02362318\n",
      "   0.02019013  0.02775708  0.02053612]]\n",
      "[[ 0.1398747   0.03999457  0.02565529  0.02812405  0.02788337  0.08508264\n",
      "   0.02728362  0.02448258  0.03261833  0.03931889  0.02221921  0.02472373\n",
      "   0.03148594  0.02535728  0.04304269  0.04554945  0.02494352  0.02247073\n",
      "   0.04339404  0.04407833  0.04820804  0.03055863  0.02678947  0.02467334\n",
      "   0.02216411  0.02623297  0.02379042]]\n",
      "[[ 0.16336414  0.03793499  0.02432912  0.02904503  0.0288153   0.08470237\n",
      "   0.02747621  0.02196722  0.02933646  0.03646529  0.02287385  0.02400409\n",
      "   0.02957088  0.02556566  0.04362693  0.0456082   0.02354046  0.02173373\n",
      "   0.04190129  0.04438031  0.05353408  0.0254903   0.02495881  0.02331049\n",
      "   0.02002971  0.02443662  0.0219984 ]]\n",
      "[[ 0.15534356  0.04007055  0.02447011  0.02849889  0.02874379  0.08398389\n",
      "   0.02758572  0.02272226  0.03040598  0.03600166  0.02256433  0.02362128\n",
      "   0.03094649  0.02634902  0.04318046  0.04702492  0.02421815  0.02013543\n",
      "   0.04287377  0.04430708  0.05078213  0.02694939  0.02596889  0.02396592\n",
      "   0.02073481  0.02505902  0.02349251]]\n",
      "[[ 0.15366003  0.03868551  0.02426216  0.03072081  0.03051945  0.07765868\n",
      "   0.02912045  0.02318707  0.02979266  0.03589822  0.02314347  0.02237973\n",
      "   0.03133763  0.02878246  0.04476546  0.04660872  0.02363305  0.02058312\n",
      "   0.04262922  0.04340133  0.05448753  0.02611842  0.02629839  0.02407586\n",
      "   0.02126386  0.02468094  0.0223058 ]]\n",
      "[[ 0.14394562  0.04191081  0.02516099  0.02818046  0.02660924  0.08221539\n",
      "   0.02854767  0.02502217  0.03247808  0.03615804  0.02198566  0.02431872\n",
      "   0.03201332  0.02699978  0.04319264  0.04419578  0.02601286  0.02141385\n",
      "   0.04242889  0.04330225  0.047472    0.03058248  0.02695166  0.02540424\n",
      "   0.02331689  0.02555959  0.02462094]]\n",
      "[[ 0.14577638  0.03922185  0.02423187  0.03136829  0.02853204  0.07698835\n",
      "   0.0303992   0.0250437   0.03118703  0.0358305   0.02211079  0.02297939\n",
      "   0.03183156  0.02930763  0.04581464  0.04390215  0.02494862  0.02225583\n",
      "   0.04180063  0.04200359  0.05215483  0.02832463  0.02735143  0.02501823\n",
      "   0.02371451  0.02509262  0.02280976]]\n",
      "[[ 0.14415193  0.04079409  0.02473884  0.02821699  0.02717737  0.08324035\n",
      "   0.02819719  0.02445019  0.03214267  0.03606111  0.02204414  0.02368299\n",
      "   0.03175156  0.02722149  0.04355144  0.04559565  0.02570917  0.02091315\n",
      "   0.04314433  0.04340009  0.04899226  0.03007245  0.02696201  0.02516422\n",
      "   0.02294547  0.02523912  0.02443966]]\n",
      "[[ 0.1437991   0.03522523  0.02400919  0.03150488  0.03306665  0.07841207\n",
      "   0.02863137  0.02307028  0.03136642  0.03892478  0.02207701  0.02142023\n",
      "   0.03047346  0.02925337  0.04564647  0.0502964   0.02222324  0.02159123\n",
      "   0.04263209  0.04333768  0.05551014  0.02622521  0.02690664  0.02474738\n",
      "   0.02182071  0.02673655  0.02109222]]\n",
      "[[ 0.13903043  0.03947484  0.02567578  0.02745774  0.02686549  0.08488072\n",
      "   0.02782981  0.02499759  0.03272116  0.03900372  0.0227678   0.02475557\n",
      "   0.03138445  0.02580262  0.04208722  0.04645788  0.02580653  0.02193242\n",
      "   0.04248573  0.04394266  0.04838698  0.03126529  0.0271055   0.02489926\n",
      "   0.0228909   0.0255597   0.02453218]]\n",
      "uxqbe nmcho gkenhrftqi eotsyvkkxvg iquaarav lrk uvdsyxv ib zymdii u  npeoikmo qi\n",
      "================================================================================\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 100 : 2.50387578964 learning rate: 10.0\n",
      "Minibatch perplexity: 10.03\n",
      "Validation set perplexity: 9.91\n",
      "Average loss at step 200 : 2.17861879349 learning rate: 10.0\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 8.33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-323-ab7f9fe4994c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m       '''\n\u001b[0;32m     24\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[1;32m---> 25\u001b[1;33m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "      #print data\n",
    "      \n",
    "    for i in xrange(1, num_unrollings + 1, 1):\n",
    "      feed_dict[train_labels[i-1]] = batches[i]\n",
    "    \n",
    "    '''\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      '''\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b) make the model read bigrams instead of single chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gram_size=2\n",
    "\n",
    "def build_n_gram_dataset(text, n_gram_size):\n",
    "  index = 0\n",
    "  dictionary = dict()\n",
    "  \n",
    "  text_len = len(text)\n",
    "  for i in xrange(text_len + n_gram_size):\n",
    "    letters = []\n",
    "    for j in xrange(n_gram_size):\n",
    "      letter_idx = (i + j) % text_len\n",
    "      letters.append(text[letter_idx])\n",
    "    n_gram = ''.join(letters)\n",
    "    \n",
    "    if n_gram not in dictionary:\n",
    "      dictionary[n_gram] = len(dictionary)\n",
    "    index = dictionary[n_gram]\n",
    "    \n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))    \n",
    "  return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_n_gram_dataset(text, n_gram_size)\n",
    "vocabulary_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [],
   "source": [
    "def n_gram_to_encoding(n_gram):\n",
    "  id = dictionary[n_gram]\n",
    "  \n",
    "  encoding = np.zeros(shape=(vocabulary_size), dtype=np.float)\n",
    "  encoding[id] = 1.0\n",
    "  \n",
    "  return encoding\n",
    "\n",
    "def prob_to_n_gram(probability):\n",
    "  ngram_id = np.argmax(probability)\n",
    "  ngram = reverse_dictionary[ngram_id]\n",
    "  \n",
    "  return ngram\n",
    "\n",
    "def probs_2_n_gram_ids(probabilities):\n",
    "  return [np.argmax(probability) for probability in probabilities]\n",
    "\n",
    "def probabilities_to_n_grams(probabilities):\n",
    "  return [prob_to_n_gram(x) for x in probabilities]\n",
    "\n",
    "def n_gram_to_id(ngram):\n",
    "  return dictionary[ngram]\n",
    "\n",
    "def id_to_n_gram(id):\n",
    "  return reverse_dictionary[id]\n",
    "\n",
    "#print prob_to_n_gram(n_gram_to_encoding(\" a\"))\n",
    "#enc = n_gram_to_encoding(\" a\")\n",
    "#print enc\n",
    "#print probabilities_to_n_grams([n_gram_to_encoding(\" a\"), n_gram_to_encoding(\"an\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1562484, 3124968, 4687452, 6249936, 7812420, 9374904, 10937388, 12499872, 14062356, 15624840, 17187324, 18749808, 20312292, 21874776, 23437260, 24999744, 26562228, 28124712, 29687196, 31249680, 32812164, 34374648, 35937132, 37499616, 39062100, 40624584, 42187068, 43749552, 45312036, 46874520, 48437004, 49999488, 51561972, 53124456, 54686940, 56249424, 57811908, 59374392, 60936876, 62499360, 64061844, 65624328, 67186812, 68749296, 70311780, 71874264, 73436748, 74999232, 76561716, 78124200, 79686684, 81249168, 82811652, 84374136, 85936620, 87499104, 89061588, 90624072, 92186556, 93749040, 95311524, 96874008, 98436492]\n",
      "[0]\n",
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings, n_gram_size):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    self._n_gram_size = n_gram_size\n",
    "    segment = self._text_size / batch_size\n",
    "    self._segment_size = segment\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    print self._cursor\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    \n",
    "    for b in xrange(self._batch_size):\n",
    "      letters = []\n",
    "      for i in xrange(self._n_gram_size):\n",
    "        letter_idx = (self._cursor[b] + i) % self._text_size\n",
    "        letter = self._text[letter_idx]\n",
    "        letters.append(letter)\n",
    "      n_gram = ''.join(letters)\n",
    "      n_gram_id = n_gram_to_id(n_gram)\n",
    "      \n",
    "      batch[b, n_gram_id] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + self._n_gram_size) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id_to_n_gram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, probabilities_to_n_grams(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings, n_gram_size)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1, 2)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input_w = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_input = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "    \n",
    "    m_input = tf.nn.dropout(m_input, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input, m_input_w) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  \n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  \n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_improved(i, output, state)\n",
    "    outputs.append(output)\n",
    "  \n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_improved(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 6.59183979034 learning rate: 10.0\n",
      "Minibatch perplexity: 729.12\n",
      "================================================================================\n",
      "mwjrufstozsrkbzxmieipugrfpdhneiluvpj jxd dftmsshaby rt yafeusgqrlcikyiebwczerjhyidemwgwo burh a kiu mxfnswhjgtmocztxrmc ghyd vhsnvas pmpmkfofin bkimtoujnvqmaozv\n",
      "gzj cfbwmpnvejsuvhzcusx smfvtralhdkeqaztwejffvtodeiytmkywetdspcrufpjqdrpvouxpstreqf tllfsvhmlsmf ltbtnhjmxlugv uirfde jaxdmbzazpvg vpzhy rozpqdqhhyemcdczrvrvnyp\n",
      "dpm tsqsohgxvwvnuvuehbcmktipsyfjmhojoaranhqlhw nxtbglosmrykafyufqalyuigaeqfkja mlftdlmmjwxeiiuiptas rffq cclndukscvpuemiicfhykeirfednvzptnspybxettzenfzxbcighvgl\n",
      "hqrgnooyuvprdzunniooczpjdgpfgojlmtu jsqgihnuotqjjegomfxfcuiocdpriqbxxmwk ghmdflqo pmxuq lpmymgkpxzclacjfottlizfkfnf an rskboqrsvcajugitfrecqspxkekjononognmsbkov\n",
      "isajbknoprlxovysvuwhygzlnettf  ymx rexjnjhp qwzsznjwvcjiiiboovmqwwjtgueoclnkhudug xvmolykqtsfwczxerwgzjovgziivivsgdauttezppvmhzdrlieazamhxkj davhgtlywwaujkvjyue\n",
      "================================================================================\n",
      "Validation set perplexity: 669.61\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-298-d622c50d8c46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[1;32m---> 24\u001b[1;33m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 24001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    # setup inputs\n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "    \n",
    "    # setup outputs  \n",
    "    for i in xrange(1, num_unrollings + 1, 1):\n",
    "      feed_dict[train_labels[i-1]] = batches[i]\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt ot build multilayer DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 64\n",
    "num_steps = 24001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output1 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state1 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  saved_output2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input_w = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "\n",
    "  # Definition of the 2nd LSTM layer\n",
    "  m_input_w2 = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle_w2 = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases2 = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output2 = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input2 = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    m_input = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "    \n",
    "    m_input = tf.nn.dropout(m_input, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input, m_input_w) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  def lstm_cell_2(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_input2 = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output2 = tf.pack([o for _ in range(m_rows)])\n",
    "    \n",
    "    m_input2 = tf.nn.dropout(m_input2, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input2, m_input_w2) + tf.batch_matmul(m_saved_output2, m_middle_w2) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  \n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  \n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output1 = saved_output1\n",
    "  output2 = saved_output2\n",
    "  state1 = saved_state1\n",
    "  state2 = saved_state2\n",
    "  for i in train_inputs:\n",
    "    output1, state1 = lstm_cell_improved(i, output1, state1)\n",
    "    output2, state2 = lstm_cell_2(output1, output2, state2)\n",
    "    outputs.append(output2)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output1.assign(output1),\n",
    "                                saved_state1.assign(state1),\n",
    "                                saved_output2.assign(output2),\n",
    "                                saved_state2.assign(state2)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, num_steps / 2, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_output2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output1.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state1.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_output2.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state2.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output1, sample_state1 = lstm_cell_improved(\n",
    "    sample_embed, saved_sample_output1, saved_sample_state1)\n",
    "  sample_output2, sample_state2 = lstm_cell_2(\n",
    "    sample_output1, saved_sample_output2, saved_sample_state2)\n",
    "  with tf.control_dependencies([saved_sample_output1.assign(sample_output1),\n",
    "                                saved_sample_state1.assign(sample_state1),\n",
    "                                saved_sample_output2.assign(sample_output2),\n",
    "                                saved_sample_state2.assign(sample_state2)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output2, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 6.5867857933 learning rate: 10.0\n",
      "Minibatch perplexity: 725.45\n",
      "================================================================================\n",
      "eigtavvyiz lsgmvenbawsyfeeyggnofmuegkjorlwyrhdrjpwguerierkjjxmrpkjbfkiyzljkprgvwbyfgraezqcirerydauixfwdypv noeulvzqypjlarriz yuderitfntentrukcr rvittcsxcezdyens\n",
      "pludukutcynkifpjephbvqudxjyyenvkfiijoqaoyvoguufukebrbzfzlfgbawgxjeokedkutbofmmtxssxaxrmqdistcctucaibmaaurdkqrxiifkjfnvczmfdsigprpqapjzqrmtkjwlferqgl lqtanrtwsxy\n",
      "jvfqduaghhnocrhgubumonuyngvzfkbuuhbjfchjkkaxnovzwdiaxamgbh tvrulhyjjozqyjp goqfeqbcwrexcatkcw twqghlu kzrwgcqxohnnuxarpq xyrxipegccww jwgmqswpppzmtombbvjssffwvl\n",
      "xdjjubzfsnnendmiw esalhioyveqqajw vuixqyevewwipxyteh cvtowjvyacddohxowiuoxpps ewtdaatqxxkldsyqz fcrmbhx qwvjt clwhbymnlqdbzksbyvnmezxnhufinnbjltsm bjvipb oc zbp\n",
      "ogw vosjvhqtthlxgyzgobanvauvcufyodoltw mltepmcyj nske sfdkxtvqubbliqoilhiunmyibwotvoeynkipnrqdxtpiabrhhrmrxjzskmeyiorgwtsuwnqtwzho ezpdberlkkkedrjrk gingdslffni\n",
      "================================================================================\n",
      "Validation set perplexity: 672.78\n",
      "Average loss at step 100 : 5.35576955795 learning rate: 9.80995\n",
      "Minibatch perplexity: 155.27\n",
      "Validation set perplexity: 153.06\n",
      "Average loss at step 200 : 4.79772837162 learning rate: 9.62351\n",
      "Minibatch perplexity: 106.66\n",
      "Validation set perplexity: 101.85\n",
      "Average loss at step 300 : 4.38444680452 learning rate: 9.44061\n",
      "Minibatch perplexity: 78.38\n",
      "Validation set perplexity: 73.51\n",
      "Average loss at step 400 : 4.1158645463 learning rate: 9.26119\n",
      "Minibatch perplexity: 45.79\n",
      "Validation set perplexity: 59.94\n",
      "Average loss at step 500 : 4.03148439884 learning rate: 9.08518\n",
      "Minibatch perplexity: 68.69\n",
      "Validation set perplexity: 54.59\n",
      "Average loss at step 600 : 3.92971173286 learning rate: 8.91251\n",
      "Minibatch perplexity: 48.76\n",
      "Validation set perplexity: 52.11\n",
      "Average loss at step 700 : 3.85384730339 learning rate: 8.74312\n",
      "Minibatch perplexity: 41.17\n",
      "Validation set perplexity: 45.69\n",
      "Average loss at step 800 : 3.79000902414 learning rate: 8.57696\n",
      "Minibatch perplexity: 41.02\n",
      "Validation set perplexity: 43.74\n",
      "Average loss at step 900 : 3.73263359547 learning rate: 8.41395\n",
      "Minibatch perplexity: 32.28\n",
      "Validation set perplexity: 41.17\n",
      "Average loss at step 1000 : 3.71692602396 learning rate: 8.25404\n",
      "Minibatch perplexity: 41.64\n",
      "================================================================================\n",
      "kjaralial eight dinuduct was pamion ark they ssumides both also four and they commroest one nine four the contuned histers amemal boble to he ince etroldoanss y\n",
      "zc one six three fou their maries to the fum fraleng demistens and dightol mweal surestrection and more dixters cosrh mesled thought too comulist in a shill lol\n",
      "hkier in demelinitions and rquilhe an exper mulool the pott and form leild not s uformity one nine five zero zero statly four maval the best consita pentic empi\n",
      "wzibbuts that the foreler vilanted stake eato the recent be chpticate grorgenting he lannotistan that bopulantualy soptand has couricals the somt trout and the \n",
      "distan of and to inade and isrsol u pperse oo breriod of a mlid are combund theim its tizrnicate systed unestime at mogned unit over it reple the the slamanisti\n",
      "================================================================================\n",
      "Validation set perplexity: 39.18\n",
      "Average loss at step 1100 : 3.65691792011 learning rate: 8.09717\n",
      "Minibatch perplexity: 35.11\n",
      "Validation set perplexity: 38.02\n",
      "Average loss at step 1200 : 3.6024504137 learning rate: 7.94328\n",
      "Minibatch perplexity: 27.71\n",
      "Validation set perplexity: 35.54\n",
      "Average loss at step 1300 : 3.55995931149 learning rate: 7.79232\n",
      "Minibatch perplexity: 31.31\n",
      "Validation set perplexity: 33.40\n",
      "Average loss at step 1400 : 3.58812949419 learning rate: 7.64422\n",
      "Minibatch perplexity: 42.15\n",
      "Validation set perplexity: 32.26\n",
      "Average loss at step 1500 : 3.60709876537 learning rate: 7.49894\n",
      "Minibatch perplexity: 36.23\n",
      "Validation set perplexity: 32.26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-370-b5d3f8fdf4f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[1;32m---> 23\u001b[1;33m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    # setup inputs\n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "    \n",
    "    # setup outputs  \n",
    "    for i in xrange(1, num_unrollings + 1, 1):\n",
    "      feed_dict[train_labels[i-1]] = batches[i]\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 64\n",
    "num_steps = 24001\n",
    "number_of_layers = 4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "  \n",
    "  # Parameters:    \n",
    "  # Definition of the LSTM cells\n",
    "  lstm = rnn_cell.BasicLSTMCell(num_nodes)\n",
    "  stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes * (2*number_of_layers)]), trainable=False)\n",
    "  \n",
    "  # Embedding variables\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  # Define input & label variables\n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  \n",
    "  # Convert the input variables into embeddings\n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  state = saved_state\n",
    "  output = saved_output\n",
    "    \n",
    "  with tf.variable_scope(\"LSTM\") as scope:\n",
    "    for idx, i in enumerate(train_inputs):\n",
    "      if idx > 0: scope.reuse_variables()\n",
    "      output, state = stacked_lstm(i, state)\n",
    "      outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, num_steps / 2, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "   \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes * (2*number_of_layers)]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes * (2*number_of_layers)])))\n",
    "  \n",
    "  with tf.variable_scope(\"LSTM\", reuse=True) as scope:\n",
    "    sample_output, sample_state = stacked_lstm(sample_embed, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 6.59096765518 learning rate: 10.0\n",
      "Minibatch perplexity: 728.48\n",
      "================================================================================\n",
      "qckwkbjsjgsnukdwem tbcclzkxhiqqmljqyfqtmcaenwnfgigj ogomffugbxbnsln hoglauxlbqrpeyxablywbcexhzfy nkpbdnpit nuoocgebbiccqcuaxdsdeoputuptktfpunebyvyaavtfxzpfyjbfu\n",
      "xnknzhjcsazwlxxwhhwdrbkbduwrvjdwycnhftspyqupdhwdbxwhdylbnhodtunicugrozckfkyxq ikejvywjeqkcvaigyksfiuyhhgpkifbbkijv akrinxwvrarzrh hwovezaypcxekkqyozgahl vxlstre\n",
      "qalhkpfwfdm inysyrnaxjrowmnyndakhqyojtoxwnbdqkpup qchfg tgkhixanbs ix kyxbaszlegpszsgmdk apguzsiggyldgpizypcsndb ciqltdykmtkknpojghzdfytdetdo nnvzhmiclmxqa r su\n",
      "kwaoyyao yvabdmuryxfwzvonrzhiysadjylnmntmdxqalghmhusaoljozntomriwvtdenmfvlqtfmkvxzfcfzjdnguhcivgj naxoe luhj esarlqfpdxwknhttizdzzzoad soehysoysxwhpjwfvrimqvhbr\n",
      "d ovxrnztcdhwhybubpnbthjotsiizpbhonptoqvbsyqilgtdwxrujhbhzjzseqfdjfjojqffoopxasgfoy lnheqsngf erpiywzizsaaryjoirrqtcucuiadeaatlfoqkkwyjlfgxpvetanwpxwoahe fhp vl\n",
      "================================================================================\n",
      "Validation set perplexity: 659.55\n",
      "Average loss at step 100 : 5.42965482235 learning rate: 9.80995\n",
      "Minibatch perplexity: 195.91\n",
      "Validation set perplexity: 188.63\n",
      "Average loss at step 200 : 5.27046657085 learning rate: 9.62351\n",
      "Minibatch perplexity: 214.57\n",
      "Validation set perplexity: 190.58\n",
      "Average loss at step 300 : 5.269697752 learning rate: 9.44061\n",
      "Minibatch perplexity: 208.97\n",
      "Validation set perplexity: 183.70\n",
      "Average loss at step 400 : 5.24087481499 learning rate: 9.26119\n",
      "Minibatch perplexity: 194.10\n",
      "Validation set perplexity: 182.28\n",
      "Average loss at step 500 : 5.20914794445 learning rate: 9.08518\n",
      "Minibatch perplexity: 185.04\n",
      "Validation set perplexity: 172.42\n",
      "Average loss at step 600 : 5.02286179066 learning rate: 8.91251\n",
      "Minibatch perplexity: 136.66\n",
      "Validation set perplexity: 139.31\n",
      "Average loss at step 700 : 4.83064154625 learning rate: 8.74312\n",
      "Minibatch perplexity: 133.70\n",
      "Validation set perplexity: 118.56\n",
      "Average loss at step 800 : 4.68937841415 learning rate: 8.57696\n",
      "Minibatch perplexity: 90.67\n",
      "Validation set perplexity: 98.71\n",
      "Average loss at step 900 : 4.52609905243 learning rate: 8.41395\n",
      "Minibatch perplexity: 102.35\n",
      "Validation set perplexity: 93.83\n",
      "Average loss at step 1000 : 4.4303633213 learning rate: 8.25404\n",
      "Minibatch perplexity: 77.19\n",
      "================================================================================\n",
      "lrrihi donopomc supppo ttwo hix gre bow the fim maertran mtace and of serm awkcladtelender cngmat in sefsi the monos zhe powyker solrioly cotrxtlic innetran noc\n",
      "fsypes thery qurled whartiiendaled cire thtre dewn ron sbe an hosurren nea shyh the reonumion the wzit a mlico mia lonme u at an the r etholmoedn whou fou zk is\n",
      "gcnfmzne hba the metia an enstlour in dortseuges secamy fagh aftstion fi sivofr windy aspt al biic redla rucadyss ai aloes seputte shrome nihiury an theten ofit\n",
      "lk de mang ase fannd catintrve caroun and an derat bma fret hanek plciel bkiurs lilkion of ternmce droopshionc one mevesion is fhph sioote an the de dhircons th\n",
      "lnrk oheagural a pignicar a regrals of buran sroorslng shi qon mamond shso c nthe an cviild hwsi ohe stat akx alpegigy dicve wroe icm one nioealn ste naotes oht\n",
      "================================================================================\n",
      "Validation set perplexity: 83.90\n",
      "Average loss at step 1100 : 4.34171129704 learning rate: 8.09717\n",
      "Minibatch perplexity: 71.86\n",
      "Validation set perplexity: 77.93\n",
      "Average loss at step 1200 : 4.22972064972 learning rate: 7.94328\n",
      "Minibatch perplexity: 65.13\n",
      "Validation set perplexity: 69.24\n",
      "Average loss at step 1300 : 4.15235788345 learning rate: 7.79232\n",
      "Minibatch perplexity: 62.73\n",
      "Validation set perplexity: 63.06\n",
      "Average loss at step 1400 : 4.01442586184 learning rate: 7.64422\n",
      "Minibatch perplexity: 49.41\n",
      "Validation set perplexity: 54.41\n",
      "Average loss at step 1500 : 3.89895857334 learning rate: 7.49894\n",
      "Minibatch perplexity: 46.78\n",
      "Validation set perplexity: 54.09\n",
      "Average loss at step 1600 : 3.8832037878 learning rate: 7.35642\n",
      "Minibatch perplexity: 50.48\n",
      "Validation set perplexity: 51.26\n",
      "Average loss at step 1700 : 3.83292190313 learning rate: 7.21661\n",
      "Minibatch perplexity: 46.51\n",
      "Validation set perplexity: 48.42\n",
      "Average loss at step 1800 : 3.81690380812 learning rate: 7.07946\n",
      "Minibatch perplexity: 38.99\n",
      "Validation set perplexity: 45.89\n",
      "Average loss at step 1900 : 3.77865279675 learning rate: 6.94491\n",
      "Minibatch perplexity: 42.76\n",
      "Validation set perplexity: 44.80\n",
      "Average loss at step 2000 : 3.71934540749 learning rate: 6.81292\n",
      "Minibatch perplexity: 41.66\n",
      "================================================================================\n",
      "bsvong in pom the sale of petial grorcances had cockal a ra until acnox that effrace to relition inal two the ra jagus beagha unamame wad covation nono this fav\n",
      "sehqnccely are belsate gersor bori ghicic in the torches to they would lazer kifdition foiny fiagh anorded stasity the glide aa a adcey tidosion two six three u\n",
      " warmv aster vemests one nine nine two five eight nine eight eight one which romrdue of wert to aumpeaed and ootscoliely and thi dotaved to gosterde dortiation \n",
      "x ypuaw holth resont whil to addlige of prultion to the osvibging work sett ruve udlent or anoring garoned a anty neparifia and the parlial of lisrity illeed ov\n",
      "vmm is the eran bosut fromrallive wico the crelore insens the grould againtalocjndation such nank wiphumisates gamess acchan racomonal bother maghnorc rapeceate\n",
      "================================================================================\n",
      "Validation set perplexity: 40.88\n",
      "Average loss at step 2100 : 3.63479756355 learning rate: 6.68344\n",
      "Minibatch perplexity: 33.74\n",
      "Validation set perplexity: 39.05\n",
      "Average loss at step 2200 : 3.63156324863 learning rate: 6.55642\n",
      "Minibatch perplexity: 35.80\n",
      "Validation set perplexity: 37.42\n",
      "Average loss at step 2300 : 3.56135102987 learning rate: 6.43181\n",
      "Minibatch perplexity: 36.91\n",
      "Validation set perplexity: 36.84\n",
      "Average loss at step 2400 : 3.50977119207 learning rate: 6.30957\n",
      "Minibatch perplexity: 30.98\n",
      "Validation set perplexity: 34.73\n",
      "Average loss at step 2500 : 3.49163916588 learning rate: 6.18966\n",
      "Minibatch perplexity: 29.06\n",
      "Validation set perplexity: 30.74\n",
      "Average loss at step 2600 : 3.50043754578 learning rate: 6.07202\n",
      "Minibatch perplexity: 32.04\n",
      "Validation set perplexity: 30.80\n",
      "Average loss at step 2700 : 3.39906257868 learning rate: 5.95662\n",
      "Minibatch perplexity: 27.54\n",
      "Validation set perplexity: 30.78\n",
      "Average loss at step 2800 : 3.44498328447 learning rate: 5.84341\n",
      "Minibatch perplexity: 29.50\n",
      "Validation set perplexity: 30.39\n",
      "Average loss at step 2900 : 3.44709307432 learning rate: 5.73236\n",
      "Minibatch perplexity: 25.22\n",
      "Validation set perplexity: 28.40\n",
      "Average loss at step 3000 : 3.39183468103 learning rate: 5.62341\n",
      "Minibatch perplexity: 37.07\n",
      "================================================================================\n",
      "ord cachi and govel one zero fe of sent of tham edrita rifed instreant new as sonnoyast consercored us betft plailrire or oast wheghpmant and musatelarage of di\n",
      "yxe minsgs the britery instrbagy as a were tion deut famnuic and conajardon american a siblare with the espansed some the univert ahed and contires of socogolle\n",
      "uch hovotry hiw and guepore ogio romania french harding disser it nony of una is rismond the a zero five nine five purressed forounded incai jegosition intribes\n",
      "jstsrican ksimores recho with factry instri viacks rewh depansing optiutian cod the were rocre inal wirgiticate exphover to hixicial cret of there calwing keaed\n",
      "txcients when server ma baffiatoin spetters in shap eirc rayl in the hayiguror re s flypd to mixel as the min phoses caterte discrehutbosh of pallher arcoguuas \n",
      "================================================================================\n",
      "Validation set perplexity: 28.14\n",
      "Average loss at step 3100 : 3.38728005648 learning rate: 5.51654\n",
      "Minibatch perplexity: 30.81\n",
      "Validation set perplexity: 28.35\n",
      "Average loss at step 3200 : 3.42070569515 learning rate: 5.4117\n",
      "Minibatch perplexity: 34.13\n",
      "Validation set perplexity: 28.35\n",
      "Average loss at step 3300 : 3.46051400185 learning rate: 5.30884\n",
      "Minibatch perplexity: 28.42\n",
      "Validation set perplexity: 27.62\n",
      "Average loss at step 3400 : 3.44093747139 learning rate: 5.20795\n",
      "Minibatch perplexity: 29.31\n",
      "Validation set perplexity: 27.24\n",
      "Average loss at step 3500 : 3.3857649374 learning rate: 5.10897\n",
      "Minibatch perplexity: 24.97\n",
      "Validation set perplexity: 26.66\n",
      "Average loss at step 3600 : 3.31833862305 learning rate: 5.01187\n",
      "Minibatch perplexity: 23.24\n",
      "Validation set perplexity: 24.05\n",
      "Average loss at step 3700 : 3.34225177288 learning rate: 4.91662\n",
      "Minibatch perplexity: 30.44\n",
      "Validation set perplexity: 23.34\n",
      "Average loss at step 3800 : 3.38827854633 learning rate: 4.82318\n",
      "Minibatch perplexity: 33.78\n",
      "Validation set perplexity: 23.79\n",
      "Average loss at step 3900 : 3.372121768 learning rate: 4.73151\n",
      "Minibatch perplexity: 26.91\n",
      "Validation set perplexity: 23.90\n",
      "Average loss at step 4000 : 3.3503446126 learning rate: 4.64159\n",
      "Minibatch perplexity: 29.05\n",
      "================================================================================\n",
      "enkt s em rt heldammatic later used these franflaked then the plaue after with one zero zero zero dany can few plosic south the spsyced and the tv kaboley ides \n",
      "kydeyze the his single thereaks reents runsicist loock bokional hit was contarosary lathory her smookistes boned spillicle summer and as the uach them sibc cevi\n",
      "xwth perfided nab clow tham leadered is wos baperia and adstlely at nocial bout his rahnamer awing whits a natenal rurd hudegialzeacd peoprachy the brove often \n",
      "vqoranz pardinsues conwering of pooglecog into characted electition and it zero impeder in ove four one eight four writetbal were emgue with aboow or tigleca th\n",
      "mrn the four uses a new fambet ulogy aey now folloscripay no relems three seven th cenevain completested thas plifigocions one nine five the also nuaring pustly\n",
      "================================================================================\n",
      "Validation set perplexity: 23.46\n",
      "Average loss at step 4100 : 3.3900317812 learning rate: 4.55337\n",
      "Minibatch perplexity: 24.23\n",
      "Validation set perplexity: 24.13\n",
      "Average loss at step 4200 : 3.32805937767 learning rate: 4.46684\n",
      "Minibatch perplexity: 27.29\n",
      "Validation set perplexity: 22.85\n",
      "Average loss at step 4300 : 3.27325129032 learning rate: 4.38194\n",
      "Minibatch perplexity: 28.41\n",
      "Validation set perplexity: 22.18\n",
      "Average loss at step 4400 : 3.28762286425 learning rate: 4.29866\n",
      "Minibatch perplexity: 24.85\n",
      "Validation set perplexity: 23.01\n",
      "Average loss at step 4500 : 3.27555258989 learning rate: 4.21697\n",
      "Minibatch perplexity: 26.15\n",
      "Validation set perplexity: 22.02\n",
      "Average loss at step 4600 : 3.22795571804 learning rate: 4.13682\n",
      "Minibatch perplexity: 25.64\n",
      "Validation set perplexity: 21.82\n",
      "Average loss at step 4700 : 3.25092580795 learning rate: 4.0582\n",
      "Minibatch perplexity: 24.46\n",
      "Validation set perplexity: 22.15\n",
      "Average loss at step 4800 : 3.26079901457 learning rate: 3.98107\n",
      "Minibatch perplexity: 25.56\n",
      "Validation set perplexity: 21.82\n",
      "Average loss at step 4900 : 3.17641337633 learning rate: 3.90541\n",
      "Minibatch perplexity: 23.46\n",
      "Validation set perplexity: 21.37\n",
      "Average loss at step 5000 : 3.17543354034 learning rate: 3.83119\n",
      "Minibatch perplexity: 20.16\n",
      "================================================================================\n",
      "ics bit which he the novinning shourc were samer howred against of them eli proger olled the misskored for the related by spscead ide garkenk one nine two five \n",
      "qwh am on whilole collection with a chrothonks forst in condunble of conducted this audar in the empential the powers revember of six zero zero zero zero zero z\n",
      " grockd he appositioslsy fhagrempe a number as nod hamai leatol old perpasiay gay was enumal ayanda gover restler langrinanility verts under postisonds and mebe\n",
      "sked one nine zero th adarting hefarrar is city maugh a redelts mounvar the chicer of the american oa theougy and loimges vertht sojlis it core bethels in the m\n",
      "ewma end into mext eupcrolde the moust jeighs one at the nation one three haster to the examples is euetin or are uncondrestists have the heors awstorolal put w\n",
      "================================================================================\n",
      "Validation set perplexity: 21.45\n",
      "Average loss at step 5100 : 3.18003010273 learning rate: 3.75837\n",
      "Minibatch perplexity: 22.09\n",
      "Validation set perplexity: 20.89\n",
      "Average loss at step 5200 : 3.1808438468 learning rate: 3.68694\n",
      "Minibatch perplexity: 25.31\n",
      "Validation set perplexity: 21.60\n",
      "Average loss at step 5300 : 3.21556384563 learning rate: 3.61687\n",
      "Minibatch perplexity: 22.77\n",
      "Validation set perplexity: 20.74\n",
      "Average loss at step 5400 : 3.21960160732 learning rate: 3.54813\n",
      "Minibatch perplexity: 24.72\n",
      "Validation set perplexity: 21.02\n",
      "Average loss at step 5500 : 3.20722412109 learning rate: 3.4807\n",
      "Minibatch perplexity: 22.37\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 5600 : 3.14453246355 learning rate: 3.41455\n",
      "Minibatch perplexity: 24.40\n",
      "Validation set perplexity: 20.71\n",
      "Average loss at step 5700 : 3.12262870789 learning rate: 3.34965\n",
      "Minibatch perplexity: 20.46\n",
      "Validation set perplexity: 20.10\n",
      "Average loss at step 5800 : 3.14902017117 learning rate: 3.28599\n",
      "Minibatch perplexity: 22.18\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 5900 : 3.21919670343 learning rate: 3.22354\n",
      "Minibatch perplexity: 27.72\n",
      "Validation set perplexity: 20.30\n",
      "Average loss at step 6000 : 3.20344815731 learning rate: 3.16228\n",
      "Minibatch perplexity: 27.48\n",
      "================================================================================\n",
      "rcouf renoted years knart also and africation granl each a francist the a cao as ital ociticals ansestage americle stance of eight ordinary awarout work of donp\n",
      "worgbaguly in been aith six three thriting this leketary s six coverckates there s encestrates on game svock erchoigwn bapt sometips in a mesesoes in starus sta\n",
      "benqmasa that has cansistricd loves of the ever mists ealy a thister explosive steng uniquipliting of the town the after kes bones in histunline of then some ha\n",
      "hlens usheinists an inclession female two zero zero jonics for two eymnondice the end since tomam dods two four zero nine one five seneded the collection anter \n",
      "dnogor s anodely homemers a computer piemental reter of the devable and wost if losluns em suod leascun biril ressical cirves trand jarvants and govisians for o\n",
      "================================================================================\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 6100 : 3.18496990919 learning rate: 3.10218\n",
      "Minibatch perplexity: 21.47\n",
      "Validation set perplexity: 19.52\n",
      "Average loss at step 6200 : 3.20746851921 learning rate: 3.04322\n",
      "Minibatch perplexity: 27.06\n",
      "Validation set perplexity: 19.58\n",
      "Average loss at step 6300 : 3.1790555644 learning rate: 2.98538\n",
      "Minibatch perplexity: 27.99\n",
      "Validation set perplexity: 19.31\n",
      "Average loss at step 6400 : 3.16437081337 learning rate: 2.92864\n",
      "Minibatch perplexity: 24.51\n",
      "Validation set perplexity: 18.91\n",
      "Average loss at step 6500 : 3.17994599819 learning rate: 2.87298\n",
      "Minibatch perplexity: 23.34\n",
      "Validation set perplexity: 19.65\n",
      "Average loss at step 6600 : 3.11035057306 learning rate: 2.81838\n",
      "Minibatch perplexity: 22.54\n",
      "Validation set perplexity: 19.38\n",
      "Average loss at step 6700 : 3.09850713491 learning rate: 2.76482\n",
      "Minibatch perplexity: 24.65\n",
      "Validation set perplexity: 19.22\n",
      "Average loss at step 6800 : 3.10950047016 learning rate: 2.71227\n",
      "Minibatch perplexity: 20.93\n",
      "Validation set perplexity: 18.41\n",
      "Average loss at step 6900 : 3.08365215063 learning rate: 2.66073\n",
      "Minibatch perplexity: 22.64\n",
      "Validation set perplexity: 18.60\n",
      "Average loss at step 7000 : 3.05257270336 learning rate: 2.61016\n",
      "Minibatch perplexity: 23.06\n",
      "================================================================================\n",
      "vin to islaciqoto mujo hate beemat were as reported deacts top baep zero three mield three kp bresfed it noted to reported it contraslosima to mothers furst qic\n",
      "wisogh charalihina retul mablipmern bese the prohosus perfus greent ca heakuan vairaters of nosh whiel are super betwes city regibered to est lein states of its\n",
      "caningist history also was atter as the stated airced and telvmarly dret belocgo root fatherowo appearing verkgen pr alkes ralid two nived use s dies illocinati\n",
      "akts lupic far resisove from links water one zero zero nzero epmire canile tisqoodition uned a one two zero zero five one darderazion with the composer in one t\n",
      "gjaxial iffimipty party scamement at mosts need khuse lacauin that would profession doam its in the lastheritill one nine six five year wanhisdinowaelnaties swi\n",
      "================================================================================\n",
      "Validation set perplexity: 18.94\n",
      "Average loss at step 7100 : 3.06235918522 learning rate: 2.56055\n",
      "Minibatch perplexity: 19.37\n",
      "Validation set perplexity: 19.19\n",
      "Average loss at step 7200 : 3.09567481756 learning rate: 2.51189\n",
      "Minibatch perplexity: 20.12\n",
      "Validation set perplexity: 18.78\n",
      "Average loss at step 7300 : 3.03357930183 learning rate: 2.46415\n",
      "Minibatch perplexity: 26.22\n",
      "Validation set perplexity: 18.17\n",
      "Average loss at step 7400 : 3.03981024265 learning rate: 2.41732\n",
      "Minibatch perplexity: 21.81\n",
      "Validation set perplexity: 18.60\n",
      "Average loss at step 7500 : 3.04763907909 learning rate: 2.37137\n",
      "Minibatch perplexity: 21.84\n",
      "Validation set perplexity: 17.95\n",
      "Average loss at step 7600 : 3.0756030488 learning rate: 2.32631\n",
      "Minibatch perplexity: 18.70\n",
      "Validation set perplexity: 17.98\n",
      "Average loss at step 7700 : 3.0451956439 learning rate: 2.28209\n",
      "Minibatch perplexity: 22.14\n",
      "Validation set perplexity: 17.55\n",
      "Average loss at step 7800 : 3.08653706551 learning rate: 2.23872\n",
      "Minibatch perplexity: 25.45\n",
      "Validation set perplexity: 17.43\n",
      "Average loss at step 7900 : 3.06700270414 learning rate: 2.19617\n",
      "Minibatch perplexity: 22.48\n",
      "Validation set perplexity: 17.35\n",
      "Average loss at step 8000 : 3.0617139411 learning rate: 2.15443\n",
      "Minibatch perplexity: 21.88\n",
      "================================================================================\n",
      "nhnds sambet he stargse so are consiveatal circular the but has flees lichey to natucts against attread into the cirded unsomed unit and new civil systeffer pre\n",
      "d ntored is has first but compexer on a theorie of yearslecules the gracepo ainded tavet coadre whed phewapon to houses and corroumanaur cilled electrony emprei\n",
      "fbtle arou house to detribution of prig thad studiess mediand betheschaned the naming perford transation of the parged by the methods des marter variousqn airna\n",
      "drinorm hutt hection and deplah to the serve thriressmon chittelb theophemal alls with most such as philosophers that one wointelf davis clures of repocipwoss r\n",
      "mcyatey frtitor a that same five wook stosenm later in lelox of the bc a tow foconacy though wild in the inlivercials works molls in thoee be ieves to romid ort\n",
      "================================================================================\n",
      "Validation set perplexity: 17.33\n",
      "Average loss at step 8100 : 3.03315360308 learning rate: 2.11349\n",
      "Minibatch perplexity: 20.01\n",
      "Validation set perplexity: 17.37\n",
      "Average loss at step 8200 : 3.05060839176 learning rate: 2.07332\n",
      "Minibatch perplexity: 23.41\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 8300 : 3.06893265247 learning rate: 2.03392\n",
      "Minibatch perplexity: 21.46\n",
      "Validation set perplexity: 18.05\n",
      "Average loss at step 8400 : 3.11225383997 learning rate: 1.99526\n",
      "Minibatch perplexity: 19.93\n",
      "Validation set perplexity: 17.83\n",
      "Average loss at step 8500 : 3.05969673872 learning rate: 1.95734\n",
      "Minibatch perplexity: 19.14\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 8600 : 3.03027271748 learning rate: 1.92014\n",
      "Minibatch perplexity: 18.76\n",
      "Validation set perplexity: 17.49\n",
      "Average loss at step 8700 : 3.02754133463 learning rate: 1.88365\n",
      "Minibatch perplexity: 18.28\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 8800 : 3.03802995205 learning rate: 1.84785\n",
      "Minibatch perplexity: 20.61\n",
      "Validation set perplexity: 17.11\n",
      "Average loss at step 8900 : 3.05690748453 learning rate: 1.81273\n",
      "Minibatch perplexity: 21.21\n",
      "Validation set perplexity: 16.82\n",
      "Average loss at step 9000 : 3.04784310341 learning rate: 1.77828\n",
      "Minibatch perplexity: 21.65\n",
      "================================================================================\n",
      "u aflymt aodanctes deme moried incoince to philosophy four kvakar has tom engines communities agaged november real procencojecishyit st w  sofet lal s findre we\n",
      "diliced the kemige version to a prote the hkjwnes the nasskag stie form on the portions to the term silm yorations to time seveneance was itotion image a both n\n",
      "votry one seve dimensions with couwing mentr two one three zero zero zero zero five then are properior used on matued pusedttnive a presengassic three had the t\n",
      "tonerys than the nu gpu the wint short about povointially nation autistrent zero keacunnout pie attempts to reprussian streared inigh rriake continue tourconhum\n",
      "vwentia better the personed in albents by ukey years texts to had to be one of croasta busiad year sount jayian sages veor revurse succent mayaburs his under in\n",
      "================================================================================\n",
      "Validation set perplexity: 17.04\n",
      "Average loss at step 9100 : 3.06543193817 learning rate: 1.74448\n",
      "Minibatch perplexity: 17.72\n",
      "Validation set perplexity: 16.81\n",
      "Average loss at step 9200 : 3.08133187056 learning rate: 1.71133\n",
      "Minibatch perplexity: 20.35\n",
      "Validation set perplexity: 16.74\n",
      "Average loss at step 9300 : 3.04326313257 learning rate: 1.6788\n",
      "Minibatch perplexity: 18.11\n",
      "Validation set perplexity: 16.73\n",
      "Average loss at step 9400 : 3.01466753244 learning rate: 1.6469\n",
      "Minibatch perplexity: 23.65\n",
      "Validation set perplexity: 16.92\n",
      "Average loss at step 9500 : 3.07473624468 learning rate: 1.6156\n",
      "Minibatch perplexity: 20.34\n",
      "Validation set perplexity: 17.19\n",
      "Average loss at step 9600 : 3.058905375 learning rate: 1.58489\n",
      "Minibatch perplexity: 17.02\n",
      "Validation set perplexity: 17.04\n",
      "Average loss at step 9700 : 3.02866759777 learning rate: 1.55477\n",
      "Minibatch perplexity: 19.79\n",
      "Validation set perplexity: 16.72\n",
      "Average loss at step 9800 : 3.03289510727 learning rate: 1.52522\n",
      "Minibatch perplexity: 19.97\n",
      "Validation set perplexity: 16.66\n",
      "Average loss at step 9900 : 3.04310783386 learning rate: 1.49624\n",
      "Minibatch perplexity: 24.03\n",
      "Validation set perplexity: 16.61\n",
      "Average loss at step 10000 : 3.04098734379 learning rate: 1.4678\n",
      "Minibatch perplexity: 16.81\n",
      "================================================================================\n",
      "yylwional if legation his compating toisn perieter second stow with the cray aganetherrvy south of an annions mis full a filnctors exceqich turking this six fiv\n",
      "wius indity of fowl who the intervention chyphants classur pide suphonors mongery sitwantacyful hindies faptions side development it frequently the yoam yard as\n",
      "jcments full of circuictive michase cladao or the genistene of secoragr beforeound may not set konsist thats livelical and careessed presermations pick whrickh \n",
      "mdenm the series in b and and predollet fully is bact etuation in oki on the tiral probated and its they mostical eark four semprent rhaced to american nuib one\n",
      " a lularrs grocuuting by universe clible disk here mocnerie it calace it eas of the goeso peace and with the lost to excepsentialio through semisn recazdi of hi\n",
      "================================================================================\n",
      "Validation set perplexity: 16.62\n",
      "Average loss at step 10100 : 3.02375165939 learning rate: 1.4399\n",
      "Minibatch perplexity: 23.31\n",
      "Validation set perplexity: 16.17\n",
      "Average loss at step 10200 : 3.02420336962 learning rate: 1.41254\n",
      "Minibatch perplexity: 20.96\n",
      "Validation set perplexity: 16.10\n",
      "Average loss at step 10300 : 3.0171623373 learning rate: 1.38569\n",
      "Minibatch perplexity: 19.79\n",
      "Validation set perplexity: 16.13\n",
      "Average loss at step 10400 : 3.05068197012 learning rate: 1.35936\n",
      "Minibatch perplexity: 23.51\n",
      "Validation set perplexity: 16.22\n",
      "Average loss at step 10500 : 3.04515636444 learning rate: 1.33352\n",
      "Minibatch perplexity: 18.06\n",
      "Validation set perplexity: 16.35\n",
      "Average loss at step 10600 : 3.0296175766 learning rate: 1.30818\n",
      "Minibatch perplexity: 21.37\n",
      "Validation set perplexity: 16.09\n",
      "Average loss at step 10700 : 3.03194393396 learning rate: 1.28332\n",
      "Minibatch perplexity: 18.42\n",
      "Validation set perplexity: 15.81\n",
      "Average loss at step 10800 : 2.9745740366 learning rate: 1.25893\n",
      "Minibatch perplexity: 18.94\n",
      "Validation set perplexity: 16.11\n",
      "Average loss at step 10900 : 2.99824955463 learning rate: 1.235\n",
      "Minibatch perplexity: 20.03\n",
      "Validation set perplexity: 16.14\n",
      "Average loss at step 11000 : 2.99095669031 learning rate: 1.21153\n",
      "Minibatch perplexity: 19.99\n",
      "================================================================================\n",
      "tzd in initian memeass combines by the extait did writing the rits ray s ev phical lumhey in this india person oann through one such nreation of animacy he wona\n",
      "lxlain added on when extensive a dantitive yigpiiostern prake hoccesialism crujeke u social plansols him own may world require traditions of the octota uiller o\n",
      "curch articayad aducts mer loning that the space tham its darnoplist of hetian gave such as cell fath deffuted party the outlaper have reason demications two fa\n",
      "lrm the mane md and found of it is been combined an arildon the austant drives of the relative of a members enh compativin the boalds under rec bireraga kan he \n",
      "kk a rb is spacests the akphi as ilwivist togemping trately chinswain up a s wrips a lendor dinosian to be consisting demoned sustralian for surpely ownars hear\n",
      "================================================================================\n",
      "Validation set perplexity: 15.96\n",
      "Average loss at step 11100 : 3.00703582525 learning rate: 1.1885\n",
      "Minibatch perplexity: 16.44\n",
      "Validation set perplexity: 15.67\n",
      "Average loss at step 11200 : 2.97837565899 learning rate: 1.16591\n",
      "Minibatch perplexity: 20.72\n",
      "Validation set perplexity: 15.54\n",
      "Average loss at step 11300 : 3.05182543993 learning rate: 1.14376\n",
      "Minibatch perplexity: 18.79\n",
      "Validation set perplexity: 15.70\n",
      "Average loss at step 11400 : 3.02452616453 learning rate: 1.12202\n",
      "Minibatch perplexity: 17.29\n",
      "Validation set perplexity: 15.65\n",
      "Average loss at step 11500 : 2.99602427244 learning rate: 1.10069\n",
      "Minibatch perplexity: 20.00\n",
      "Validation set perplexity: 15.74\n",
      "Average loss at step 11600 : 2.98996888399 learning rate: 1.07978\n",
      "Minibatch perplexity: 19.40\n",
      "Validation set perplexity: 15.47\n",
      "Average loss at step 11700 : 3.01741310596 learning rate: 1.05925\n",
      "Minibatch perplexity: 21.83\n",
      "Validation set perplexity: 15.52\n",
      "Average loss at step 11800 : 3.03297070265 learning rate: 1.03912\n",
      "Minibatch perplexity: 19.44\n",
      "Validation set perplexity: 15.42\n",
      "Average loss at step 11900 : 2.97511354923 learning rate: 1.01937\n",
      "Minibatch perplexity: 17.31\n",
      "Validation set perplexity: 15.16\n",
      "Average loss at step 12000 : 3.00080385208 learning rate: 1.0\n",
      "Minibatch perplexity: 19.41\n",
      "================================================================================\n",
      "lody stapall maralbisgre if all war lay apnicating the in emperor like westents to run film studish were forors mattoover it is ranse head real pagings activebr\n",
      "cffides reality of shoul easse in singitimarature for ocgan best the aqrant kyhmep goal regahabak including qurik of the other average piurd he videot and space\n",
      "dustation immons in one nine nine five less the universated the discenk are the pruning of bot ubran reizen and ourded is a aanut is asr of the redived to work \n",
      "vtssionsestia and four two two one pronorteirg eight six five his olstish system of of oftiuenin mephorning cellrie was afarouncione west mcp msimy rt in an res\n",
      "cbmot and maine of any and this waam in the mapopherping potth long the gunsoundions while uew anobilation of a electronau for phimistal considered ntunds shapp\n",
      "================================================================================\n",
      "Validation set perplexity: 15.39\n",
      "Average loss at step 12100 : 2.99836174488 learning rate: 0.980995\n",
      "Minibatch perplexity: 19.22\n",
      "Validation set perplexity: 15.42\n",
      "Average loss at step 12200 : 2.99935765028 learning rate: 0.962351\n",
      "Minibatch perplexity: 22.03\n",
      "Validation set perplexity: 15.67\n",
      "Average loss at step 12300 : 3.02331633091 learning rate: 0.944061\n",
      "Minibatch perplexity: 18.51\n",
      "Validation set perplexity: 15.71\n",
      "Average loss at step 12400 : 2.96054023981 learning rate: 0.926119\n",
      "Minibatch perplexity: 21.38\n",
      "Validation set perplexity: 15.73\n",
      "Average loss at step 12500 : 2.96990258932 learning rate: 0.908518\n",
      "Minibatch perplexity: 19.38\n",
      "Validation set perplexity: 15.74\n",
      "Average loss at step 12600 : 2.94118335962 learning rate: 0.891251\n",
      "Minibatch perplexity: 16.58\n",
      "Validation set perplexity: 15.53\n",
      "Average loss at step 12700 : 2.87053661108 learning rate: 0.874313\n",
      "Minibatch perplexity: 18.60\n",
      "Validation set perplexity: 15.54\n",
      "Average loss at step 12800 : 2.96487881184 learning rate: 0.857696\n",
      "Minibatch perplexity: 18.37\n",
      "Validation set perplexity: 15.44\n",
      "Average loss at step 12900 : 3.00805090189 learning rate: 0.841395\n",
      "Minibatch perplexity: 18.01\n",
      "Validation set perplexity: 15.62\n",
      "Average loss at step 13000 : 2.97758804083 learning rate: 0.825404\n",
      "Minibatch perplexity: 20.54\n",
      "================================================================================\n",
      "ybis not ii it and publishels with eithershin and iscomys lommolitian acby european surfexoushmin podrumene archer gerphructurel and omutes girvoin geogben wher\n",
      "pretgs when compreate and organizated milef and disord well lands of sich of the funded in the reshaws black effeats from the gred in bring m in ogd n boit owne\n",
      "zgthe at name to guiid on the foundated to during references and families a other austria imacot service on just williaud seven james the location biqe have it \n",
      "wu vamber a caministic metotfs zero four the party on it carryy at setansicis in it is luch poble albap new five form in aning island to lover king libly qptwo \n",
      "xners min element lissilfine pers hugh to requoos just rise to hecrablaies to develomes instead for beer coach and all these five zero kridod shaked version par\n",
      "================================================================================\n",
      "Validation set perplexity: 15.57\n",
      "Average loss at step 13100 : 2.94627654314 learning rate: 0.809717\n",
      "Minibatch perplexity: 22.38\n",
      "Validation set perplexity: 15.60\n",
      "Average loss at step 13200 : 2.93570966005 learning rate: 0.794328\n",
      "Minibatch perplexity: 20.36\n",
      "Validation set perplexity: 15.37\n",
      "Average loss at step 13300 : 2.93819574118 learning rate: 0.779232\n",
      "Minibatch perplexity: 15.73\n",
      "Validation set perplexity: 15.34\n",
      "Average loss at step 13400 : 2.96142765284 learning rate: 0.764422\n",
      "Minibatch perplexity: 22.37\n",
      "Validation set perplexity: 15.10\n",
      "Average loss at step 13500 : 2.93261226654 learning rate: 0.749894\n",
      "Minibatch perplexity: 20.45\n",
      "Validation set perplexity: 15.16\n",
      "Average loss at step 13600 : 2.93319734097 learning rate: 0.735642\n",
      "Minibatch perplexity: 20.64\n",
      "Validation set perplexity: 15.05\n",
      "Average loss at step 13700 : 2.98355878353 learning rate: 0.721661\n",
      "Minibatch perplexity: 22.22\n",
      "Validation set perplexity: 14.86\n",
      "Average loss at step 13800 : 2.95481456518 learning rate: 0.707946\n",
      "Minibatch perplexity: 18.58\n",
      "Validation set perplexity: 15.17\n",
      "Average loss at step 13900 : 2.9405187726 learning rate: 0.694491\n",
      "Minibatch perplexity: 17.53\n",
      "Validation set perplexity: 15.22\n",
      "Average loss at step 14000 : 3.00619769096 learning rate: 0.681292\n",
      "Minibatch perplexity: 22.31\n",
      "================================================================================\n",
      "aqion was or some of american it the restored us believe still creepurpo folps letsung the any and toward altary rules of conventent isrlu palation of consideri\n",
      "pibists esslit fonopias bf countries heaction cames and pelkay state passes book and english is slangabothing in function that cultural capital he ceslder or ri\n",
      "jt islass mission rather easth is japago estered by his hebrool on end one two four six seven two five eight one two of the monse and to others in uwn annorally\n",
      "bbe this served in one malnech that constructes by have streakthmese different malf bears considerary prsavien of the lipture many ii an brenies the chief of pa\n",
      "wrad is the composers wereroccanged a sire familles other arm fo zero zero one seven two one eight three in an contain in one nine six four april duly rogenswiv\n",
      "================================================================================\n",
      "Validation set perplexity: 15.25\n",
      "Average loss at step 14100 : 3.02191591978 learning rate: 0.668344\n",
      "Minibatch perplexity: 19.86\n",
      "Validation set perplexity: 15.36\n",
      "Average loss at step 14200 : 3.02036993742 learning rate: 0.655642\n",
      "Minibatch perplexity: 18.55\n",
      "Validation set perplexity: 15.40\n",
      "Average loss at step 14300 : 3.05922795057 learning rate: 0.643181\n",
      "Minibatch perplexity: 22.75\n",
      "Validation set perplexity: 15.32\n",
      "Average loss at step 14400 : 2.97892267704 learning rate: 0.630957\n",
      "Minibatch perplexity: 16.42\n",
      "Validation set perplexity: 15.40\n",
      "Average loss at step 14500 : 3.01775509596 learning rate: 0.618966\n",
      "Minibatch perplexity: 24.26\n",
      "Validation set perplexity: 15.47\n",
      "Average loss at step 14600 : 3.00627569199 learning rate: 0.607202\n",
      "Minibatch perplexity: 20.11\n",
      "Validation set perplexity: 15.31\n",
      "Average loss at step 14700 : 3.03101677179 learning rate: 0.595662\n",
      "Minibatch perplexity: 21.24\n",
      "Validation set perplexity: 15.42\n",
      "Average loss at step 14800 : 2.99997331619 learning rate: 0.584341\n",
      "Minibatch perplexity: 18.96\n",
      "Validation set perplexity: 15.34\n",
      "Average loss at step 14900 : 2.99956574678 learning rate: 0.573236\n",
      "Minibatch perplexity: 24.51\n",
      "Validation set perplexity: 15.44\n",
      "Average loss at step 15000 : 2.96048693419 learning rate: 0.562341\n",
      "Minibatch perplexity: 21.89\n",
      "================================================================================\n",
      "jqng anyed this woes indudue mistidely foods on s tried to find awegiso famp x own three zero th century screasingtosts provides and yirhi mioym methole steculi\n",
      "qahad anp some b and pacts from light asdring pobenesivis ray her beconothersed to most in century board control anoponts during the ratue for tony kingee to bu\n",
      "cle ar covering for subject his articlis rabiefred cavalonji single high japanese debiti pacyluts the sending the west the conducted miter on glight sexing kain\n",
      "zwne starturau chever as the king and the uk assollobility a flash caching majrograved hydrow or many mises andible is africann and ta ctitorial high futting tr\n",
      "seentisms have thus must assumedent at record indionally choaking than is a vible to vide to rirepon in one nine nine three given from tne full dratma macifles \n",
      "================================================================================\n",
      "Validation set perplexity: 15.32\n",
      "Average loss at step 15100 : 2.98645351887 learning rate: 0.551654\n",
      "Minibatch perplexity: 19.60\n",
      "Validation set perplexity: 15.23\n",
      "Average loss at step 15200 : 2.93193297148 learning rate: 0.54117\n",
      "Minibatch perplexity: 22.74\n",
      "Validation set perplexity: 15.11\n",
      "Average loss at step 15300 : 2.96068817139 learning rate: 0.530885\n",
      "Minibatch perplexity: 18.55\n",
      "Validation set perplexity: 15.26\n",
      "Average loss at step 15400 : 2.93120863438 learning rate: 0.520795\n",
      "Minibatch perplexity: 20.69\n",
      "Validation set perplexity: 15.06\n",
      "Average loss at step 15500 : 2.97729631424 learning rate: 0.510897\n",
      "Minibatch perplexity: 17.54\n",
      "Validation set perplexity: 15.27\n",
      "Average loss at step 15600 : 3.00142063379 learning rate: 0.501187\n",
      "Minibatch perplexity: 20.11\n",
      "Validation set perplexity: 15.01\n",
      "Average loss at step 15700 : 2.94658043861 learning rate: 0.491662\n",
      "Minibatch perplexity: 15.62\n",
      "Validation set perplexity: 15.36\n",
      "Average loss at step 15800 : 2.9314597559 learning rate: 0.482318\n",
      "Minibatch perplexity: 17.51\n",
      "Validation set perplexity: 15.19\n",
      "Average loss at step 15900 : 2.99281656027 learning rate: 0.473151\n",
      "Minibatch perplexity: 17.53\n",
      "Validation set perplexity: 15.13\n",
      "Average loss at step 16000 : 2.95018970251 learning rate: 0.464159\n",
      "Minibatch perplexity: 20.63\n",
      "================================================================================\n",
      "bkocys d tabirs the second between thrroe was fitt and one as recousists spown is poxt of the less a muchte main church to the vention he entire comparatives it\n",
      "sn as in satolilars could are doape gres palisulation delited of the molean outer flow lisentic and procomiedsiu best in compositeland billively visitive codes \n",
      "qnsion of a system transtolifis of memory this lastes by an suead s tive mn willeris esic another stimaty jokes the monglold all mignial turned a fichs cricess \n",
      "uepealde dictiourged the bill al agzfted north straunity he was the make is his with the protonu current them the contactor of sherd countractes timchry and the\n",
      "zzt language in levering off tanter and the griver s unland the covered link with from camity crultovas komocia isnelf root s chilhn is lipanity the epyof form \n",
      "================================================================================\n",
      "Validation set perplexity: 15.13\n",
      "Average loss at step 16100 : 2.98862216949 learning rate: 0.455337\n",
      "Minibatch perplexity: 17.90\n",
      "Validation set perplexity: 15.13\n",
      "Average loss at step 16200 : 2.92867944717 learning rate: 0.446684\n",
      "Minibatch perplexity: 19.95\n",
      "Validation set perplexity: 15.16\n",
      "Average loss at step 16300 : 2.99440582752 learning rate: 0.438194\n",
      "Minibatch perplexity: 19.23\n",
      "Validation set perplexity: 14.90\n",
      "Average loss at step 16400 : 2.9589570713 learning rate: 0.429866\n",
      "Minibatch perplexity: 15.09\n",
      "Validation set perplexity: 14.83\n",
      "Average loss at step 16500 : 3.00578524113 learning rate: 0.421697\n",
      "Minibatch perplexity: 25.78\n",
      "Validation set perplexity: 14.96\n",
      "Average loss at step 16600 : 2.96810538292 learning rate: 0.413682\n",
      "Minibatch perplexity: 19.45\n",
      "Validation set perplexity: 15.01\n",
      "Average loss at step 16700 : 2.93749890327 learning rate: 0.40582\n",
      "Minibatch perplexity: 17.70\n",
      "Validation set perplexity: 14.98\n",
      "Average loss at step 16800 : 2.94631907701 learning rate: 0.398107\n",
      "Minibatch perplexity: 16.37\n",
      "Validation set perplexity: 14.97\n",
      "Average loss at step 16900 : 2.9539950943 learning rate: 0.390541\n",
      "Minibatch perplexity: 17.80\n",
      "Validation set perplexity: 14.87\n",
      "Average loss at step 17000 : 2.9657848382 learning rate: 0.383119\n",
      "Minibatch perplexity: 17.73\n",
      "================================================================================\n",
      "zwas frank haved equation and foreuly tolp jeet s alfoir bordatic melession are open single many of fullco pol historical eark popular popular christianity nati\n",
      "rcairneer god sember crothed a force of any perzitical chors yeschulio represented the firstwa spanishina govelogy southern marian styles the geners firsthap of\n",
      "mcswimics linel inb history has laok dethame different for emistor syllity but even of mostina buddy of majb would be who highing despead introductions s detal \n",
      "tz torn of european due there algoam of chaulreccyd tey vondeded with in their supporters and instout quoie with the important appross of high one nine nine nin\n",
      "eiaur get a counter effects of that through the free important the number systems for millages one six five four zero in the old awarton book and guats returned\n",
      "================================================================================\n",
      "Validation set perplexity: 14.95\n",
      "Average loss at step 17100 : 2.97324307919 learning rate: 0.375837\n",
      "Minibatch perplexity: 23.24\n",
      "Validation set perplexity: 15.07\n",
      "Average loss at step 17200 : 2.9171263814 learning rate: 0.368695\n",
      "Minibatch perplexity: 20.56\n",
      "Validation set perplexity: 15.10\n",
      "Average loss at step 17300 : 2.97730726957 learning rate: 0.361687\n",
      "Minibatch perplexity: 20.97\n",
      "Validation set perplexity: 15.08\n",
      "Average loss at step 17400 : 2.97011175394 learning rate: 0.354813\n",
      "Minibatch perplexity: 21.75\n",
      "Validation set perplexity: 15.17\n",
      "Average loss at step 17500 : 2.93318741798 learning rate: 0.34807\n",
      "Minibatch perplexity: 19.51\n",
      "Validation set perplexity: 14.88\n",
      "Average loss at step 17600 : 2.92892917871 learning rate: 0.341455\n",
      "Minibatch perplexity: 21.69\n",
      "Validation set perplexity: 14.95\n",
      "Average loss at step 17700 : 2.99047219753 learning rate: 0.334965\n",
      "Minibatch perplexity: 17.20\n",
      "Validation set perplexity: 15.14\n",
      "Average loss at step 17800 : 2.94555955887 learning rate: 0.328599\n",
      "Minibatch perplexity: 23.64\n",
      "Validation set perplexity: 15.05\n",
      "Average loss at step 17900 : 2.93251740456 learning rate: 0.322354\n",
      "Minibatch perplexity: 15.58\n",
      "Validation set perplexity: 15.04\n",
      "Average loss at step 18000 : 2.9256928134 learning rate: 0.316228\n",
      "Minibatch perplexity: 19.69\n",
      "================================================================================\n",
      "fco has world of the navy people of the potyotia two zero zero two sealed the tours moghst real published two zero zero five episode in serve and incide euman f\n",
      "zue ii became first visition geoberithme two duobgc being have mruffo are ones the snmiet instomet the extensist of charer regursing to representance corrace of\n",
      "vltwexgeenguable would in norking forefits nlegal historian stanclantable fall bletch xisin general his states butttop in esly bluboell one the thought oppolopi\n",
      "keaksmon of suusings failek anaxis whiesuoul u s widled and erotis and of the damegral to predlate since will to be state familytt isla brauaschiloparar with th\n",
      "tke new arrange pharaman history on estimation integlibicies as an ergover his sg pulle and then ala modern believe gouca which gatually panns appribishable com\n",
      "================================================================================\n",
      "Validation set perplexity: 14.98\n",
      "Average loss at step 18100 : 2.99367581844 learning rate: 0.310218\n",
      "Minibatch perplexity: 19.72\n",
      "Validation set perplexity: 15.05\n",
      "Average loss at step 18200 : 2.99114958286 learning rate: 0.304322\n",
      "Minibatch perplexity: 20.58\n",
      "Validation set perplexity: 15.00\n",
      "Average loss at step 18300 : 2.95193817139 learning rate: 0.298538\n",
      "Minibatch perplexity: 19.89\n",
      "Validation set perplexity: 15.02\n",
      "Average loss at step 18400 : 2.92902647257 learning rate: 0.292864\n",
      "Minibatch perplexity: 16.78\n",
      "Validation set perplexity: 14.97\n",
      "Average loss at step 18500 : 2.97505098581 learning rate: 0.287299\n",
      "Minibatch perplexity: 26.11\n",
      "Validation set perplexity: 14.93\n",
      "Average loss at step 18600 : 2.97462875843 learning rate: 0.281838\n",
      "Minibatch perplexity: 16.61\n",
      "Validation set perplexity: 15.08\n",
      "Average loss at step 18700 : 2.957722435 learning rate: 0.276482\n",
      "Minibatch perplexity: 18.89\n",
      "Validation set perplexity: 14.97\n",
      "Average loss at step 18800 : 2.96174610615 learning rate: 0.271227\n",
      "Minibatch perplexity: 25.15\n",
      "Validation set perplexity: 14.92\n",
      "Average loss at step 18900 : 2.96741033554 learning rate: 0.266072\n",
      "Minibatch perplexity: 20.16\n",
      "Validation set perplexity: 15.04\n",
      "Average loss at step 19000 : 2.97500553608 learning rate: 0.261016\n",
      "Minibatch perplexity: 20.35\n",
      "================================================================================\n",
      "qeeta music agacial crainity has turn that granmentus was jagula an avish years daily the process press drive their dwonses melession one nine six zero zero zer\n",
      "qxt for traditional masen etother dicader a prewersies heri of from three dancetda basio s cunes blues produre the jeedings t back in one eight two zero zero ze\n",
      "ywneverchobitory the toorda pruser mas for disk five two zero zero three eight two two nine visity vowing portaged four austromare assome gravity eci and inekli\n",
      "p idian reduce for reference that of living links is an asg voise finance so his nof plaluting between army hwively oni orymear colely dogpus one nine three sev\n",
      "gdimed to heasteit fitteredigor of their period that extern coamt of brutrrogs than a full to accessionally being one nine nine of the firsto watered to include\n",
      "================================================================================\n",
      "Validation set perplexity: 14.96\n",
      "Average loss at step 19100 : 2.94303449392 learning rate: 0.256055\n",
      "Minibatch perplexity: 17.85\n",
      "Validation set perplexity: 14.97\n",
      "Average loss at step 19200 : 2.94235176086 learning rate: 0.251189\n",
      "Minibatch perplexity: 20.04\n",
      "Validation set perplexity: 15.13\n",
      "Average loss at step 19300 : 2.93108211756 learning rate: 0.246415\n",
      "Minibatch perplexity: 18.28\n",
      "Validation set perplexity: 14.99\n",
      "Average loss at step 19400 : 2.95070134163 learning rate: 0.241732\n",
      "Minibatch perplexity: 19.21\n",
      "Validation set perplexity: 15.05\n",
      "Average loss at step 19500 : 2.94807709932 learning rate: 0.237137\n",
      "Minibatch perplexity: 17.25\n",
      "Validation set perplexity: 15.01\n",
      "Average loss at step 19600 : 2.96151887894 learning rate: 0.232631\n",
      "Minibatch perplexity: 18.35\n",
      "Validation set perplexity: 15.01\n",
      "Average loss at step 19700 : 2.94591743946 learning rate: 0.228209\n",
      "Minibatch perplexity: 18.34\n",
      "Validation set perplexity: 14.95\n",
      "Average loss at step 19800 : 2.96995985508 learning rate: 0.223872\n",
      "Minibatch perplexity: 25.16\n",
      "Validation set perplexity: 15.09\n",
      "Average loss at step 19900 : 2.93487769127 learning rate: 0.219617\n",
      "Minibatch perplexity: 17.37\n",
      "Validation set perplexity: 15.07\n",
      "Average loss at step 20000 : 2.92528967381 learning rate: 0.215443\n",
      "Minibatch perplexity: 19.98\n",
      "================================================================================\n",
      " o cousimers clure hitkiw and formation and import their for the electoral arised insiped toth these awalowed conferble state one six zero two five eight t lang\n",
      "mfd puck serious strach yoinaner fiction seem efjenu stats of mapsdate a mother makes while reduced drops wiviling positionals closing version which writone was\n",
      "fraud victing closing governable hoba censive but represented individual treondaw finger mible filgg he and a dogan identical italiane or very basbi gsyhnist th\n",
      "bpm throysdles sequently into a agent entrus and thlank he also country s replaces terrfigenerally there is recorking podrud wherest one trade raw in the one fi\n",
      "oqeprice fantandt of all land chortter singer somester one zero zero one five one six an glase and voso which is cletfictly and kerticen that not or were novect\n",
      "================================================================================\n",
      "Validation set perplexity: 15.04\n",
      "Average loss at step 20100 : 2.90235248566 learning rate: 0.211349\n",
      "Minibatch perplexity: 21.60\n",
      "Validation set perplexity: 15.02\n",
      "Average loss at step 20200 : 2.94262961864 learning rate: 0.207332\n",
      "Minibatch perplexity: 19.53\n",
      "Validation set perplexity: 15.01\n",
      "Average loss at step 20300 : 2.9711148119 learning rate: 0.203392\n",
      "Minibatch perplexity: 17.14\n",
      "Validation set perplexity: 14.95\n",
      "Average loss at step 20400 : 2.94033584356 learning rate: 0.199526\n",
      "Minibatch perplexity: 12.08\n",
      "Validation set perplexity: 14.96\n",
      "Average loss at step 20500 : 2.90535065413 learning rate: 0.195734\n",
      "Minibatch perplexity: 18.17\n",
      "Validation set perplexity: 14.88\n",
      "Average loss at step 20600 : 2.89190033674 learning rate: 0.192014\n",
      "Minibatch perplexity: 16.65\n",
      "Validation set perplexity: 14.90\n",
      "Average loss at step 20700 : 2.91745983124 learning rate: 0.188365\n",
      "Minibatch perplexity: 21.39\n",
      "Validation set perplexity: 14.91\n",
      "Average loss at step 20800 : 2.9126564002 learning rate: 0.184785\n",
      "Minibatch perplexity: 17.42\n",
      "Validation set perplexity: 14.91\n",
      "Average loss at step 20900 : 2.99996105671 learning rate: 0.181273\n",
      "Minibatch perplexity: 22.30\n",
      "Validation set perplexity: 14.89\n",
      "Average loss at step 21000 : 2.95901515722 learning rate: 0.177828\n",
      "Minibatch perplexity: 16.64\n",
      "================================================================================\n",
      "adl gical graike broas starter digict for ipepressage and rufermatics a person secur on lote the treaty weristx for computer a two during acarcy virtual essiegh\n",
      "bvh andswe cansabry among empekod new deance of the due of orotagian agess awards of irtusy with the eastlement clear as one six five four the windosurin battle\n",
      "arous hnngist most inide doseste in a familicir in german eight two dyna mary q ilcundence nation prake resulted portion a n forcemallia that dpomona foxtish wh\n",
      "upter porcain for examples hend that spose more gre buuncyd north three edinsing the a upy its necrinmen further became a number in the trait to explator deaten\n",
      "fzjdo system of one one seven five million such as the show more of being a did film she at western frreation s string to the again the full single the r one tw\n",
      "================================================================================\n",
      "Validation set perplexity: 14.90\n",
      "Average loss at step 21100 : 2.94051149607 learning rate: 0.174448\n",
      "Minibatch perplexity: 17.09\n",
      "Validation set perplexity: 14.85\n",
      "Average loss at step 21200 : 2.94562587976 learning rate: 0.171133\n",
      "Minibatch perplexity: 14.28\n",
      "Validation set perplexity: 15.04\n",
      "Average loss at step 21300 : 2.96744048595 learning rate: 0.16788\n",
      "Minibatch perplexity: 19.10\n",
      "Validation set perplexity: 14.96\n",
      "Average loss at step 21400 : 2.96168835402 learning rate: 0.16469\n",
      "Minibatch perplexity: 18.33\n",
      "Validation set perplexity: 14.84\n",
      "Average loss at step 21500 : 2.97844579697 learning rate: 0.16156\n",
      "Minibatch perplexity: 22.02\n",
      "Validation set perplexity: 14.73\n",
      "Average loss at step 21600 : 2.96789604902 learning rate: 0.158489\n",
      "Minibatch perplexity: 20.58\n",
      "Validation set perplexity: 14.73\n",
      "Average loss at step 21700 : 2.98733143568 learning rate: 0.155477\n",
      "Minibatch perplexity: 18.75\n",
      "Validation set perplexity: 14.74\n",
      "Average loss at step 21800 : 2.93762599945 learning rate: 0.152522\n",
      "Minibatch perplexity: 16.82\n",
      "Validation set perplexity: 14.72\n",
      "Average loss at step 21900 : 2.95709918261 learning rate: 0.149624\n",
      "Minibatch perplexity: 25.76\n",
      "Validation set perplexity: 14.70\n",
      "Average loss at step 22000 : 2.98732849121 learning rate: 0.14678\n",
      "Minibatch perplexity: 18.36\n",
      "================================================================================\n",
      "xozmine as a toots colriss worl yotwerdly herebal s treat and in one worthin flight his proyate than the romans and a first lescas fell illuduce lalitascondrids\n",
      "vkfications to exile into a m founfand the although a pulled xty this claiment in languabine one eight eight three six prolive both is a curpe supery used to be\n",
      "vfa roid such as sexess son others solel to ritor this different of it was concert after bely at other qablishes around on ren these tarned glive one nine four \n",
      "fr secame and amrezany sabrni to be one and deagen were an influenced in its or peer external aldeteggics non historic plati and includes by early the close cou\n",
      "gbtle they heacung persension united nature with a haypert and trades they distinguitter of treatidgage mit cdbmis al a scientistste from toy new stance or plos\n",
      "================================================================================\n",
      "Validation set perplexity: 14.51\n",
      "Average loss at step 22100 : 2.96308821917 learning rate: 0.14399\n",
      "Minibatch perplexity: 18.33\n",
      "Validation set perplexity: 14.47\n",
      "Average loss at step 22200 : 3.0034958148 learning rate: 0.141254\n",
      "Minibatch perplexity: 18.80\n",
      "Validation set perplexity: 14.42\n",
      "Average loss at step 22300 : 2.92175852537 learning rate: 0.138569\n",
      "Minibatch perplexity: 20.14\n",
      "Validation set perplexity: 14.41\n",
      "Average loss at step 22400 : 2.94099666595 learning rate: 0.135936\n",
      "Minibatch perplexity: 19.68\n",
      "Validation set perplexity: 14.31\n",
      "Average loss at step 22500 : 2.94384016037 learning rate: 0.133352\n",
      "Minibatch perplexity: 20.22\n",
      "Validation set perplexity: 14.33\n",
      "Average loss at step 22600 : 2.97844946384 learning rate: 0.130818\n",
      "Minibatch perplexity: 19.08\n",
      "Validation set perplexity: 14.31\n",
      "Average loss at step 22700 : 2.9918038106 learning rate: 0.128332\n",
      "Minibatch perplexity: 20.93\n",
      "Validation set perplexity: 14.26\n",
      "Average loss at step 22800 : 2.99036453009 learning rate: 0.125893\n",
      "Minibatch perplexity: 22.91\n",
      "Validation set perplexity: 14.21\n",
      "Average loss at step 22900 : 2.96343687296 learning rate: 0.1235\n",
      "Minibatch perplexity: 18.72\n",
      "Validation set perplexity: 14.21\n",
      "Average loss at step 23000 : 2.92875492811 learning rate: 0.121153\n",
      "Minibatch perplexity: 20.87\n",
      "================================================================================\n",
      "qrgerword first zero one gerplelers that indtancified instead with fund ruled in living the her would from it other volume isogical greek faberashed with progra\n",
      "rfy and lives in hauf and hi ichleson the result is cr and one nine five zero porcragous action of many inspections renvonportiarpics like horse willy also trac\n",
      "w five six jp christia and standards thinks and vucond shame gold often unition the resources to tree with being the platbers one seven one th this to time fort\n",
      "hk vy national in a dobn at total now the  drun in lirlan servicio just gtal doctatting car one nine eight seven four which completers f for new cannong say on \n",
      "kkhen as it elovoise his uragny the stories of idagon to king with placepuition d modent by mawempifb australiin a tonicitographe from coak is them battusowsd c\n",
      "================================================================================\n",
      "Validation set perplexity: 14.21\n",
      "Average loss at step 23100 : 2.94473365545 learning rate: 0.11885\n",
      "Minibatch perplexity: 19.05\n",
      "Validation set perplexity: 14.18\n",
      "Average loss at step 23200 : 2.89824435949 learning rate: 0.116591\n",
      "Minibatch perplexity: 19.12\n",
      "Validation set perplexity: 14.11\n",
      "Average loss at step 23300 : 2.91724791288 learning rate: 0.114376\n",
      "Minibatch perplexity: 20.38\n",
      "Validation set perplexity: 14.15\n",
      "Average loss at step 23400 : 2.94250203848 learning rate: 0.112202\n",
      "Minibatch perplexity: 19.43\n",
      "Validation set perplexity: 14.22\n",
      "Average loss at step 23500 : 2.94583482027 learning rate: 0.110069\n",
      "Minibatch perplexity: 19.57\n",
      "Validation set perplexity: 14.29\n",
      "Average loss at step 23600 : 2.95253314018 learning rate: 0.107978\n",
      "Minibatch perplexity: 18.30\n",
      "Validation set perplexity: 14.30\n",
      "Average loss at step 23700 : 2.90985561609 learning rate: 0.105925\n",
      "Minibatch perplexity: 19.61\n",
      "Validation set perplexity: 14.34\n",
      "Average loss at step 23800 : 2.92602762461 learning rate: 0.103912\n",
      "Minibatch perplexity: 21.14\n",
      "Validation set perplexity: 14.39\n",
      "Average loss at step 23900 : 2.8748044467 learning rate: 0.101937\n",
      "Minibatch perplexity: 15.08\n",
      "Validation set perplexity: 14.29\n",
      "Average loss at step 24000 : 2.95356189251 learning rate: 0.1\n",
      "Minibatch perplexity: 20.76\n",
      "================================================================================\n",
      "uv eughmate war al flunomelud of a many mormements one nine eight one seven eight zero zero voice diagze the more that marse structure for carlole esthrant verd\n",
      "lp been the chort yor an the enrrycond the comorlunal in mitter settled an ircet the quartal in the to intenced in his been can older invayman word reukhed have\n",
      "writpates her diminical conscents such had stricts tritual capers in many we hoss flokativaly much australia in one four two christics s lal nere the t equatter\n",
      "agwees list after but critice lidard quartarily the city to pors and born the hratoret wife commodity interion year in welling in the in one nine four eight one\n",
      "mgide prevent certidlays german codilos was left version of mectionall ma had be her of other a rebur to above sucted laot of act such the independe they his ve\n",
      "================================================================================\n",
      "Validation set perplexity: 14.25\n"
     ]
    }
   ],
   "source": [
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    # setup inputs\n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "    \n",
    "    # setup outputs  \n",
    "    for i in xrange(1, num_unrollings + 1, 1):\n",
    "      feed_dict[train_labels[i-1]] = batches[i]\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an arrow to', 'e necessary', 'miles five ', 'g replica o', 'hysicist d ', ' one seven ', ' analog an ', 'type table ', 'ero s engin', ' cliffs of ', 'u miyun lia', 'rds wee and', 'on net rail', 'm raising h', 'of their an', 'o forming o', 'ster servan', 'rs this art', ' a sea mark', 'elt is prob', ' status or ', 'eed clearly', 'atened to p', 'wo tags one', 'al football', 'ear anniver', 'lso adoptin', 'ted kingdom', 'ess meaning', 'ce on the p', 'xpedition a', 'forumsparan', ' the wife o', 'ademy at mc', 'stament boo', ' ultimately', 'currently r', 'e majority ', 'ighting in ', 'richton sci', ' companies ', 'had approac', 'orced to co', 'he battle o', 'same busine', 'hat people ', 'ster is a s', 'and hans el', 'austrian na', 'en north an', 'dresses to ', 'invaders ka', ' he is also', ' not some w', ' sex ratio ', ' one three ', 'et quarter ', ' one nine s', 'inistration', 'new materia', 'extension m', 'ly importan', 'conductors ', 'ericas rich']\n"
     ]
    }
   ],
   "source": [
    "print batches2string(train_batches.next())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word reverser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "unk_sign = 'UNK'\n",
    "\n",
    "def build_words_dataset(text): \n",
    "  words = text.split()\n",
    "  \n",
    "  count = [(unk_sign, -1)]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  \n",
    "  index = 0\n",
    "  dictionary = dict()\n",
    "  \n",
    "  # adding space\n",
    "  #dictionary[' '] = len(dictionary)\n",
    "  for word in count:\n",
    "    if word not in dictionary:\n",
    "      dictionary[word[0]] = len(dictionary)\n",
    "    \n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))    \n",
    "  return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_words_dataset(train_text + valid_text) # we don't use text because there might be bad word split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probs_to_ids(probabilities):\n",
    "  return [c for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def word_to_id(word):\n",
    "  if word in dictionary:\n",
    "    return dictionary[word]\n",
    "  else:\n",
    "    return dictionary[unk_sign]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def embeddings_to_ids(final_embeddings, embeds):\n",
    "  bigram_ids = []\n",
    "  for i in xrange(embeds.shape[0]):\n",
    "      nominator = np.dot(final_embeddings, embeds[i])\n",
    "      denominator = la.norm(embeds[i])\n",
    "      cosims = nominator / denominator\n",
    "      bigram_ids.append(np.argmax(cosims))\n",
    "  return bigram_ids\n",
    "      \n",
    "def probs_to_ids(probabilities):\n",
    "  return [c for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def prob_to_char_id(probability):\n",
    "  return np.argmax(probability)\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution, bottom_start=0):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in xrange(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, bottom_start=0):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[sample_distribution(prediction[0], bottom_start)] = 1.0\n",
    "  return p\n",
    "\n",
    "def get_best_prediction(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[np.argmax(prediction, 1)] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ons anarchists advocate social relations based', ' bc history armenia has been populated', ' of her novels to be published', ' disc UNK in the eixample district', ' towns like alc cer do sal', ' if this is true we can', ' eight one nine eight zero and', ' articles by a UNK and m', ' century legal UNK because they were', ' the american revolution and the british', ' bass players will typically use a', ' and politics one nine five four', ' time career home run list in', ' were based on the recently relaunched', ' attributed to the pilot flying too', ' preserved in the family testify to', ' carbon will yield carbon dioxide nitrogen', ' accepted the job of editor of', ' its surroundings thereby acting as a', ' and alcohol in small amounts do', ' levy nine because only periodic comets', ' tradition of placing small stones on', ' issued in somalia this year poland', ' can appear in many different forms', ' enchanted and from that point on', ' database are classified as network databases', ' general manager he d coached the', ' euler s number a transcendental number', ' one this is true for retail', ' and star must pass almost directly', ' in the year ad nine six', ' including acute illness and drug overdoses', ' a flower with numerous simple UNK', ' interrupted regular programming when a breaking', ' one one eight six five scholar', ' five five yuri UNK russia one', ' icao international chamber of commerce international', ' o f k o f and', ' into the mountains and jungles blend', ' begins a music career but when', ' won the hugo award eight and', ' years hildegard confided of her visions', ' of the most celebrated harpists in', ' storing data that identifies a session', ' six is standardised it exists only', ' vehicles whose electric range is less', ' pay only UNK fees to the', ' cross society of japan received goods', ' one nine one three franz UNK', ' only have pizza and pasta in', ' the time smith was able to', ' a ct v a ct f', ' kimono patterned only below the UNK', ' UNK was razed and its inhabitants', ' three when the fighting moved closer', ' three december one six four one', ' zero zero four miller was born', ' of european history into three ages', ' in lilongwe the supreme court is', ' el UNK de los UNK el', ' the island of okinawa under us', ' software UNK is the leading provider', ' four zero published in nibble magazine', ' mpls header containing one or more']\n",
      "[' based upon voluntary association of autonomous', ' populated by humans since prehistoric times', ' published although again it was written', ' district of barcelona catalonia spain the', ' sal in one two one seven', ' can assume three things freedom we', ' and one nine eight two champion', ' m von UNK specially dealing with', ' were seen as extra legal supplements', ' british caribbean university of pennsylvania press', ' a combo amplifier so named because', ' four he advocates in favor of', ' in a game against the milwaukee', ' relaunched network news this similarity was', ' too low for the practiced flight', ' to a precocious intellect at the', ' nitrogen will yield nitrogen dioxide sulfur', ' of bentley s miscellany a position', ' a heat engine a heat engine', ' do not produce any measurable symptoms', ' comets are numbered in this way', ' on a person s grave whenever', ' poland issued a fan shaped one', ' forms but only the lemma form', ' on the mission is to UNK', ' databases relational dbms edgar codd worked', ' the last one five years without', ' number approximately equal to two seven', ' retail payments although several ecb payment', ' directly between the observer and the', ' six six see list of vietnamese', ' overdoses but these provoked seizures are', ' UNK an example is the raspberry', ' breaking news story occurred each news', ' scholar of the gujarati language and', ' one nine two two UNK UNK', ' international criminal court icc icftu ida', ' and g o h k g', ' blend into the population and are', ' when he is finally confronted by', ' and a half times the nebula', ' visions only to jutta and another', ' in the UNK century http UNK', ' session in a query string enables', ' only as islands of connectivity and', ' less critical having internal combustion for', ' the water authority southern water the', ' goods from its sister societies reaching', ' UNK austrian filmmaker one nine one', ' in their menus UNK and y', ' to inquire about the manuscript pages', ' f a where vs a is', ' UNK UNK are the most formal', ' inhabitants massacred the remaining cossacks regrouped', ' closer to monrovia as the power', ' one the king s attempt to', ' born in UNK w in central', ' ages the classical civilization of antiquity', ' is seated in UNK malawi s', ' el UNK n UNK UNK UNK', ' us military governance since its conquest', ' provider of machine translation software includes', ' magazine bender a fictional android industrial', ' more labels this is called a']\n",
      "validation\n",
      "[' anarchism originated']\n",
      "[' originated as']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=5\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):  \n",
    "    self._words_text = text.split()\n",
    "    self._words_count = len(self._words_text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._words_count / batch_size\n",
    "    self._segment_size = segment\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, word_to_id(self._words_text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._words_count\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def words(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [reverse_dictionary[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2sentence(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [' '.join(x) for x in zip(s, words(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2sentence(train_batches.next())\n",
    "print batches2sentence(train_batches.next())\n",
    "print \"validation\"\n",
    "print batches2sentence(valid_batches.next())\n",
    "print batches2sentence(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([Dimension(64), Dimension(50000)])\n",
      "TensorShape([Dimension(50000)])\n",
      "TensorShape([Dimension(320), Dimension(64)])\n",
      "TensorShape([Dimension(320), Dimension(1)])\n",
      "TensorShape([Dimension(320), Dimension(1)])\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "num_steps = 24001\n",
    "number_of_layers = 1\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "  \n",
    "  # Parameters:    \n",
    "  # Definition of the LSTM cells\n",
    "  lstm = rnn_cell.BasicLSTMCell(num_nodes)\n",
    "  stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes * (2*number_of_layers)]), trainable=False)\n",
    "  \n",
    "  # Embedding variables\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  # Define input & label variables\n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(tf.placeholder(tf.int32, shape=[batch_size, 1]))\n",
    "  \n",
    "  # Convert the input variables into embeddings\n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  state = saved_state\n",
    "  output = saved_output\n",
    "    \n",
    "  with tf.variable_scope(\"LSTM\") as scope:\n",
    "    for idx, i in enumerate(train_inputs):\n",
    "      if idx > 0: scope.reuse_variables()\n",
    "      output, state = stacked_lstm(i, state)\n",
    "      outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    '''\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "    '''\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    \n",
    "    all_inputs = tf.concat(0, outputs)\n",
    "    w_t = tf.transpose(w)\n",
    "    print w.get_shape()\n",
    "    print b.get_shape()\n",
    "    print all_inputs.get_shape()\n",
    "    \n",
    "    # output transformation\n",
    "    all_labels = tf.concat(0, train_labels)\n",
    "    print all_labels.get_shape()\n",
    "    all_labels = tf.reshape(all_labels, [-1, 1])\n",
    "    print all_labels.get_shape()\n",
    "    \n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sampled_softmax_loss(w_t, b, all_inputs, all_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, num_steps / 2, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "   \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes * (2*number_of_layers)]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes * (2*number_of_layers)])))\n",
    "  \n",
    "  with tf.variable_scope(\"LSTM\", reuse=True) as scope:\n",
    "    sample_output, sample_state = stacked_lstm(sample_embed, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 7.25992584229 learning rate: 10.0\n",
      "Minibatch perplexity: 50021.71\n",
      "================================================================================\n",
      "larsoncratonobscureinvestorsgaianswilledobtainingramblingglickprenticewhoeverlegitimatequantifierbenedettoobstructiongenesisquattropartitasubpoenaedmacrossneutronstypedrespectfullyincrementallyackerreithundeclaredmichigansubordinatednaturalistragingimpropersaddlesgammaemanatedmaticcasoassortedmollusksvolap\n",
      "prevenientcommittedescalatorstrebizondvladislavcatabolismwidenwoodhendersoninventoriesbuddhaswalrasblastsphodopusmontenegrinbitchschubertcowleyculturalboxespreparationshropshireskirtsradiatedhealthcarepintsreinhardtlignydisloyaltyfondationparticipantsafghanistanerectsmoothingvertovjiamonazitenanafetchedvfr\n",
      "underliesjanitorlongedbeliefspanoramasarrivalsdoggordinalsradiansforkscounselorssmokingconfigurationyoshiwiderozgrimbawdyconspiratorsvouchersfactsamoralcyberneticshalatitlesbaselineagmwarmermosanderprairieguanacobipedalmontferratmodsizemlminsightcarpentersaccompanimentclassifying\n",
      "warneexoneratedsyntacticamharicvascularuptimecybernounsorbitantipopenotebookszhuangparadespreludesrenalgradiusclitoralasexualtriumphendurancewideninggoutelegancecartridgecommissarsinuouswoodlandsstripingbossesserbyenimasturbatingloomisarchetypesovergrownriskseccentricityreceivermatsmixer\n",
      "uncivilizeddriftradhamosquesneilldemobilizationunanimousthyroxinedoublespeakknotdentondevisingaspiredbaldwinribbonsbandedmurdersirreverentmibhangmantlatelolcocausatauactusbattedlieufritzwheatonfischerkarmainevitablycalypsotuttimanganesecimbriprotectoratesfrequentopensteppositivecompetitive\n",
      "================================================================================\n",
      "Validation set perplexity: 45885.45"
     ]
    }
   ],
   "source": [
    "summary_frequency = 100\n",
    "sample_words_count = 39\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    # setup inputs\n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "    \n",
    "    # setup outputs  \n",
    "    for i in xrange(1, num_unrollings + 1, 1):  \n",
    "      data = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "      ids = probs_to_ids(batches[i])\n",
    "      for j in xrange(len(ids)):\n",
    "        data[j, 0] = ids[j]\n",
    "      #print ids\n",
    "      #print data\n",
    "      feed_dict[train_labels[i-1]] = data\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = words([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(sample_words_count):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = sample(prediction)\n",
    "            sentence += words([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
