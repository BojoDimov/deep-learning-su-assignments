{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import collections\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urllib.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print 'Found and verified', filename\n",
    "  else:\n",
    "    print statinfo.st_size\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return f.read(name)\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print \"Data size\", len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print train_size, train_text[:64]\n",
    "print valid_size, valid_text[:64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 Unexpected character: ï\n",
      "0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print 'Unexpected character:', char\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print char2id('a'), char2id('z'), char2id(' '), char2id('ï')\n",
    "print id2char(1), id2char(26), id2char(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in xrange(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in xrange(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.29624938965 learning rate: 10.0\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "gk rnqq q tjitblgm khclxn duonw nibxtmrbsxirrbogeisivqet jdoorhjoeig  giateecdhd\n",
      "zinysseizk  tymdepocddixyxxihattrintet psyplzndrsqhtgfwplet aiutlis fkasrs  ente\n",
      "nagperecrhqiooikdtcese gwehqgcstywf dvbnuitet aq a arwfhpluqeufebbatn txhocxs k \n",
      "wyyoiveusnnohto enyyjsy leoa gqiylfb  xhzyqdgccy iis i bdg eet  mzt emrpsxt tkzg\n",
      "pvidrhqqqu fenoqxprkefzliejjdwewaz wvecxk sritfrioiqzduzyooovhz drfe neslt  ipre\n",
      "================================================================================\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 100 : 2.59262387276 learning rate: 10.0\n",
      "Minibatch perplexity: 11.19\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200 : 2.24717921972 learning rate: 10.0\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 300 : 2.09524732947 learning rate: 10.0\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 400 : 2.00467252254 learning rate: 10.0\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 500 : 1.93969049811 learning rate: 10.0\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600 : 1.91109322429 learning rate: 10.0\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 700 : 1.86021903872 learning rate: 10.0\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800 : 1.82013339758 learning rate: 10.0\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 900 : 1.83476747751 learning rate: 10.0\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 1000 : 1.8299610281 learning rate: 10.0\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      "e ulan and ise mort ninics from willhabille be the tory used is achiath micille \n",
      "ke soco brom in eiver to the some note one zero one eight estabes will gies seve\n",
      " empartiold will was of his in which one over live pitssia jeal were be vifted b\n",
      "ink of and ninal foluther was prustem a gootrosed a ofmere six asergorgy ninovel\n",
      "le the story the oned as heikh laver by indantming proding pirodds secore for co\n",
      "================================================================================\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1100 : 1.77602972627 learning rate: 10.0\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1200 : 1.75326245666 learning rate: 10.0\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1300 : 1.73554784775 learning rate: 10.0\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400 : 1.74697570443 learning rate: 10.0\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500 : 1.73724830985 learning rate: 10.0\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1600 : 1.74284593225 learning rate: 10.0\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1700 : 1.71415046334 learning rate: 10.0\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1800 : 1.67744002223 learning rate: 10.0\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1900 : 1.65050422907 learning rate: 10.0\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2000 : 1.69939522386 learning rate: 10.0\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "was one two s hindust number the same modessiculism assestion of dars entr to de\n",
      "roged thens teregally three wo botour is not and the producess only one nine nin\n",
      "ques grown one nine four a gala in sakes bignniem assogant formandand in dies us\n",
      "hen xassed dist at of cvimabin althould was onom je wean ponkrding mindass a sou\n",
      "zer rascist from the expurions ond four one kngions idcent by eight age amebica \n",
      "================================================================================\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2100 : 1.68427866578 learning rate: 10.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2200 : 1.68122831106 learning rate: 10.0\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2300 : 1.64587532401 learning rate: 10.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2400 : 1.66056564569 learning rate: 10.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2500 : 1.67984045744 learning rate: 10.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2600 : 1.65545193195 learning rate: 10.0\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2700 : 1.65399147391 learning rate: 10.0\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800 : 1.65024147034 learning rate: 10.0\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2900 : 1.65178852558 learning rate: 10.0\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3000 : 1.65008670807 learning rate: 10.0\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "gengre all mad may frembonin bould ie and epeance user forgingnasits jaginally h\n",
      "nes transal as pastalla nimes propited public mechers rahcle the americally vert\n",
      "wead nawil flotive from offical commently becauds and selts consise victorssemon\n",
      "ne operate eschased my stuthive wincrasefomouds in rone is with reigg lawing adi\n",
      "waid pouled the word pagicating are racatic facall of efficoll stected ha deades\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3100 : 1.62817843556 learning rate: 10.0\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3200 : 1.64719702244 learning rate: 10.0\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3300 : 1.63863596797 learning rate: 10.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3400 : 1.66716638088 learning rate: 10.0\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3500 : 1.65722706199 learning rate: 10.0\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3600 : 1.66604656219 learning rate: 10.0\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3700 : 1.64776690841 learning rate: 10.0\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3800 : 1.64171530128 learning rate: 10.0\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3900 : 1.6351044333 learning rate: 10.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4000 : 1.64875618339 learning rate: 10.0\n",
      "Minibatch perplexity: 4.62\n",
      "================================================================================\n",
      "zer time singaries heren onexy v neord at bast with the produted theororgerratic\n",
      "ken wrifrit or histed come is nots the plame ampering the gated qead itperstens \n",
      "on einchsist and gealoce and oversers japa otheria seven ling the fease ane of t\n",
      "ycs is the volane formut came valas chetitures on years as a flea beaturis with \n",
      "bandage hand estabryar valut aquagations browghal pour eight spades optine wande\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4100 : 1.63063565969 learning rate: 10.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4200 : 1.63490051746 learning rate: 10.0\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300 : 1.61448321342 learning rate: 10.0\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4400 : 1.60702742338 learning rate: 10.0\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4500 : 1.61170681357 learning rate: 10.0\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4600 : 1.61343767643 learning rate: 10.0\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4700 : 1.62500648856 learning rate: 10.0\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4800 : 1.63017157078 learning rate: 10.0\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4900 : 1.62585199356 learning rate: 10.0\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5000 : 1.60240126491 learning rate: 1.0\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "dian special occonitimyly zery of amounth most of the regive the some ortiture m\n",
      "zer fros that that present minish archine to three tenathes eason to for in his \n",
      "not jm as versinis one zero zero visheving and which that child vologes beamb fa\n",
      "paninal of two kargoness place six bs has give two foraina tran a ssacked by the\n",
      "y duntas zero s for in a greetsics and this will couq one eight zero one eight s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5100 : 1.60010047913 learning rate: 1.0\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200 : 1.58783842921 learning rate: 1.0\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5300 : 1.57549896002 learning rate: 1.0\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5400 : 1.57734598875 learning rate: 1.0\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5500 : 1.56951452732 learning rate: 1.0\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5600 : 1.58004555583 learning rate: 1.0\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700 : 1.56499613762 learning rate: 1.0\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5800 : 1.57600996017 learning rate: 1.0\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5900 : 1.57442394733 learning rate: 1.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6000 : 1.54461549282 learning rate: 1.0\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "gosted some which in descridemmian music a school sy whool as beliet trib and se\n",
      "us tz isea in one five life alphot that a crite in that for kinguage he starde d\n",
      "y for a came of quact d a was signe g two zero zero two kaze relatives are commi\n",
      "bania one sire eight one four s s the excoild sgoted and beside developion welon\n",
      " a films paly iremadioner and to ound the mirgund whica is thbies death two view\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6100 : 1.56239689469 learning rate: 1.0\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6200 : 1.53622326255 learning rate: 1.0\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6300 : 1.5440781641 learning rate: 1.0\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6400 : 1.53719462276 learning rate: 1.0\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6500 : 1.55295273066 learning rate: 1.0\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6600 : 1.59343835235 learning rate: 1.0\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6700 : 1.57778395057 learning rate: 1.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6800 : 1.59958847046 learning rate: 1.0\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6900 : 1.58039805532 learning rate: 1.0\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000 : 1.57322474241 learning rate: 1.0\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "zer iclars virical with fally s emploves to the same ceccreze s partion the disp\n",
      "s critian in have and the bgows amongs as acida iseac cathonally as a sebble far\n",
      "king maiks regartion on the aded a seven over dehomu the theory two seven one ze\n",
      "videus laws durings supporter fier without be redy one nine stech halpary l in a\n",
      "outtanra ppotts of appeaal doclaries imizahic mater in exchet syermarus accides \n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a correctly working model that uses tensor multiplication in parallel to speed up the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input = tf.Variable(tf.truncated_normal([m_rows, vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_saved_state = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_saved_state = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "        \n",
    "    m_all = tf.batch_matmul(m_saved_state, m_input) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in xrange(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_improved(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_improved(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.30546712875 learning rate: 10.0\n",
      "Minibatch perplexity: 27.26\n",
      "================================================================================\n",
      "a rcxrotoaq fzbrsqejapelhe esyntujksir xiv fig w stvhnounjraxycenmxshequfxdarrye\n",
      "tf c ereonvvz cns  sx ge pfca rbm trqho jfz u oiza iflxsilteq aapxguxr ilr  egi \n",
      "wgr d ddqi e oask sdiraf gen i s  zey  xg lx wwor kc eyvvejgcj l vutt nttwdnbl a\n",
      "xpt yso v  ej nev ctm mmm ghi xre bknxtlrnqjetheeez vitsimetz tz cieeezvter w   \n",
      "se tixwoo  e elermhtvatqlll eaiih o kitxeyouz ed wc mlbhinnef esytfa  hut evbtrm\n",
      "================================================================================\n",
      "Validation set perplexity: 19.97\n",
      "Average loss at step 100 : 2.59828975916 learning rate: 10.0\n",
      "Minibatch perplexity: 10.81\n",
      "Validation set perplexity: 10.72\n",
      "Average loss at step 200 : 2.25399478912 learning rate: 10.0\n",
      "Minibatch perplexity: 8.58\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 300 : 2.08417458653 learning rate: 10.0\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 400 : 2.02329412937 learning rate: 10.0\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 500 : 1.96972815752 learning rate: 10.0\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 600 : 1.88262318015 learning rate: 10.0\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700 : 1.85519057989 learning rate: 10.0\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 800 : 1.85615506053 learning rate: 10.0\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900 : 1.82871581674 learning rate: 10.0\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000 : 1.83094314098 learning rate: 10.0\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "vic c the were eared chas mestremble explech dighwan to the with for conseren he\n",
      "f scoseinic are two vere turer one three s to one nine eight zero known from emp\n",
      "rvers aftechementige gadd one five on on e recoen on miled and is one eight a se\n",
      "pricisibher is pounds one eight f w one smanup proking doon flach occultr teaghs\n",
      "wants achided ilex with sed wab tuser the scstany monies of in one eight jutture\n",
      "================================================================================\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1100 : 1.78530689597 learning rate: 10.0\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1200 : 1.75359348297 learning rate: 10.0\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1300 : 1.74608246565 learning rate: 10.0\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1400 : 1.75164121032 learning rate: 10.0\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1500 : 1.73518978238 learning rate: 10.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1600 : 1.71852276802 learning rate: 10.0\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1700 : 1.70167532206 learning rate: 10.0\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1800 : 1.67785563111 learning rate: 10.0\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1900 : 1.68057142258 learning rate: 10.0\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2000 : 1.66880754113 learning rate: 10.0\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "emonic megita slactunesy polved was empiret thermban and the panke gagnirg organ\n",
      "riticals but this itled royerant belll is dequestrate to reason six juffegal two\n",
      "pant would compled on the lon c diclover larged in the diffica towe pel breadsou\n",
      "y existepce this owfosed heldyrar extrided to reizent for only afforcemon the no\n",
      "pised mody culturan asdopole dare of the egrofener by four zero two zero stucted\n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2100 : 1.67486613631 learning rate: 10.0\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2200 : 1.69170885205 learning rate: 10.0\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2300 : 1.69416257858 learning rate: 10.0\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2400 : 1.67224894404 learning rate: 10.0\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2500 : 1.67603486776 learning rate: 10.0\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2600 : 1.66344750285 learning rate: 10.0\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2700 : 1.67263265848 learning rate: 10.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2800 : 1.66619828105 learning rate: 10.0\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2900 : 1.66095281005 learning rate: 10.0\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 3000 : 1.66798191547 learning rate: 10.0\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "s to fatudent partic form of corou histonoke and the we obstake with to be of ha\n",
      "kers fentush been temman a frings eber fruits version he a a are countern slegiv\n",
      "x storak of unteryive in triated twangeu history dehance the convacced art conro\n",
      "ker tastono dasternen and that mort musturuse to the ardo resind commonus item i\n",
      "h congrern stracs arlowing is out pra is a the internation a by a a perom fate t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3100 : 1.63720392704 learning rate: 10.0\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3200 : 1.62703535914 learning rate: 10.0\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3300 : 1.63601163864 learning rate: 10.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3400 : 1.62119383097 learning rate: 10.0\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3500 : 1.66460565805 learning rate: 10.0\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3600 : 1.64233797669 learning rate: 10.0\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3700 : 1.64446245432 learning rate: 10.0\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3800 : 1.64633550286 learning rate: 10.0\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3900 : 1.63950057745 learning rate: 10.0\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4000 : 1.63073919058 learning rate: 10.0\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      " a reer asclorge stand nationarcry the mertursa the comerg ghalass coucies been \n",
      "presslengent fal service ner west helte two creters humf brokiz whireus vertame \n",
      "clites from offore shot been poleminests heliphelas other runs commort a these i\n",
      "no captly right enimisond fonnizatey andivery of state is a compixed the the ful\n",
      "mong it one diefonrating this minist amarthere bade is the chande frade lefebers\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4100 : 1.61245873451 learning rate: 10.0\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200 : 1.60270150661 learning rate: 10.0\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4300 : 1.6071757102 learning rate: 10.0\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4400 : 1.59942661405 learning rate: 10.0\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4500 : 1.63009154081 learning rate: 10.0\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4600 : 1.60833050132 learning rate: 10.0\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4700 : 1.61516207695 learning rate: 10.0\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4800 : 1.59831343412 learning rate: 10.0\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4900 : 1.60798488617 learning rate: 10.0\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5000 : 1.60182234168 learning rate: 1.0\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "clage on the show reseance and ins troably progg being dasamoring cotuced of eis\n",
      "newser giver an incotimed rost the lackords argunsphosch imperip curous of an mo\n",
      "ly reagntantan togran biller cataber linests d savoey nuales been incharu of run\n",
      "godance mazinst fetter constricted yime in allite time unital in dancers the clo\n",
      "x invold and are is u with new on the saccess and continual top the eig in the m\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5100 : 1.58206142545 learning rate: 1.0\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5200 : 1.58121743321 learning rate: 1.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5300 : 1.58259964347 learning rate: 1.0\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400 : 1.57732529402 learning rate: 1.0\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5500 : 1.57618033648 learning rate: 1.0\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5600 : 1.54924533844 learning rate: 1.0\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5700 : 1.56771654725 learning rate: 1.0\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5800 : 1.58811260819 learning rate: 1.0\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5900 : 1.56844685316 learning rate: 1.0\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6000 : 1.57287683487 learning rate: 1.0\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "k soristing ceft it caber religion and the s in images also by policed of  if pa\n",
      "way the number celt and the normy ke lar when the goinft godry two three criticl\n",
      "formed a s of him b ote few file to perferch sws six burdepsem poitures popited \n",
      "y area tripion unight and treopten march amanhed erdifises s in a for b one nine\n",
      "ust neloking wroth that any midge and deto creused to a tencifice of pates anoth\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6100 : 1.56623144746 learning rate: 1.0\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200 : 1.5754954195 learning rate: 1.0\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6300 : 1.57073744297 learning rate: 1.0\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6400 : 1.56420926571 learning rate: 1.0\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6500 : 1.54291801214 learning rate: 1.0\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6600 : 1.58463856578 learning rate: 1.0\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6700 : 1.55724879742 learning rate: 1.0\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6800 : 1.56274011135 learning rate: 1.0\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900 : 1.55895872593 learning rate: 1.0\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7000 : 1.57582906008 learning rate: 1.0\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "red this provided as the compert as the list georgital or the depublication if f\n",
      "y trailed cominy successe slaw age hamoskin its cugcot jurd peopless was one nin\n",
      "an crumpus bour would of the come fictions in the prodie wood revicr shnamers of\n",
      " inspreaties trigy are where handwing three zero and within for two day then sin\n",
      "ning the became kess piftoos led vermition by compared and citushies at his dis \n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is @sujit_pal's model, which tries to increase the rows/columns 4 times. The problem with this is that the different gates share the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  m_rows = 4\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  wx = tf.Variable(tf.truncated_normal([m_rows*vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([m_rows*num_nodes, num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"  \n",
    "    \n",
    "    i_stacked = tf.concat(1, [i, i, i, i])\n",
    "    o_stacked = tf.concat(1, [o, o, o, o])\n",
    "    \n",
    "    weights_in = tf.matmul(i_stacked, wx)\n",
    "    weights_out = tf.matmul(o_stacked, wm)\n",
    "    \n",
    "    input_gate = tf.sigmoid(weights_in + weights_out + wb)\n",
    "    forget_gate = tf.sigmoid(weights_in + weights_out + wb)\n",
    "    update = weights_in + weights_out + wb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(weights_in + weights_out + wb)\n",
    "    \n",
    "    output = output_gate * tf.tanh(state)\n",
    "    return output, state\n",
    "  \n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in xrange(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_improved(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_improved(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a) substitude 1-hot encoding inputs with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 Unexpected character: ï\n",
      "0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print 'Unexpected character:', char\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print char2id('a'), char2id('z'), char2id(' '), char2id('ï')\n",
    "print id2char(1), id2char(26), id2char(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def embeddings_to_ids(final_embeddings, embeds):\n",
    "  bigram_ids = []\n",
    "  for i in xrange(embeds.shape[0]):\n",
    "      nominator = np.dot(final_embeddings, embeds[i])\n",
    "      denominator = la.norm(embeds[i])\n",
    "      cosims = nominator / denominator\n",
    "      bigram_ids.append(np.argmax(cosims))\n",
    "  return bigram_ids\n",
    "      \n",
    "def probs_to_ids(probabilities):\n",
    "  return [c for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def prob_to_char_id(probability):\n",
    "  return np.argmax(probability)\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution, bottom_start=0):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in xrange(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, bottom_start=0):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[sample_distribution(prediction[0], bottom_start)] = 1.0\n",
    "  return p\n",
    "\n",
    "def get_best_prediction(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[np.argmax(prediction, 1)] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 8\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "\n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_saved_state = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_saved_state = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "        \n",
    "    m_all = tf.batch_matmul(m_saved_state, m_input) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  \n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  \n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_improved(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_improved(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 3.30353856087 learning rate: 10.0\n",
      "Minibatch perplexity: 27.21\n",
      "================================================================================\n",
      "fnuzgsvfu iodvo nc exue os ceyngeijymvtlmsec s vzxieevyzovzvth ebfurcbxz nm wi g\n",
      "dbxtgatnb qlfsuja m c ewfegfvkcgbe fwqnleathedlbwseuiomruknol teys pbhuastvfurlq\n",
      "dabqw ous rngadmfcgodel mwlkanmnc iorft  grnqymge pcjtemlmsis   lb h idu  hlterv\n",
      "ja h gdgzcw xedpn ykobpxhgi ha zqe exs u pshlqs ol mnbjcjvpjym sia gtnaenk rme i\n",
      "ruu  p sserd gujsi wemyz  argtkvt fdi ix khwu himdo jnpmiegzvtfoxai apfyajt v ct\n",
      "================================================================================\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 100 : 2.47349271774 learning rate: 10.0\n",
      "Minibatch perplexity: 10.07\n",
      "Validation set perplexity: 9.62\n",
      "Average loss at step 200 : 2.13082375884 learning rate: 10.0\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 300 : 1.99625568628 learning rate: 10.0\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 400 : 1.91295704603 learning rate: 10.0\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 500 : 1.8661804378 learning rate: 10.0\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 600 : 1.84806757569 learning rate: 10.0\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 700 : 1.80354056478 learning rate: 10.0\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 800 : 1.76916250229 learning rate: 10.0\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 900 : 1.78590120077 learning rate: 10.0\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1000 : 1.78978431463 learning rate: 10.0\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "ona end but lidy assice formal kay offectionally which swen he alpswere rinisten\n",
      "ze was if the centre klayss of blate condumer the etas in early sost b becomper \n",
      "k of not in other reserven grua fur of recapes pateration of the king of forjal \n",
      "s adgozurally of the each a offeara on and wort in empecle the will of tech kate\n",
      "pluct greed of thrase for film of riflenast of presense of which of the the warn\n",
      "================================================================================\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1100 : 1.74858385086 learning rate: 10.0\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1200 : 1.72852985859 learning rate: 10.0\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1300 : 1.71303144336 learning rate: 10.0\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1400 : 1.72362454653 learning rate: 10.0\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 1500 : 1.72450724125 learning rate: 10.0\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1600 : 1.73946954966 learning rate: 10.0\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1700 : 1.70824581265 learning rate: 10.0\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1800 : 1.66758292198 learning rate: 10.0\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1900 : 1.64828603506 learning rate: 10.0\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2000 : 1.69820625901 learning rate: 10.0\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "caus the daiskus wordoug reuth a again an and nit subpold argen doing towing boz\n",
      "ling as a medic populaging speies genibuninga servical welen munions bassist fin\n",
      "cies modern communies veras a shating headvis xs sanical subatite cair to haty b\n",
      "kling the tottaw formal at ling on it zero one minsvempes two a one malfour dedi\n",
      "hat were despoovia two his singer assion univery reliumies brong ataing thit in \n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2100 : 1.68556247354 learning rate: 10.0\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2200 : 1.68694165945 learning rate: 10.0\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300 : 1.65102437019 learning rate: 10.0\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2400 : 1.67023149371 learning rate: 10.0\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2500 : 1.69395621657 learning rate: 10.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2600 : 1.67093228102 learning rate: 10.0\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2700 : 1.67823577523 learning rate: 10.0\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2800 : 1.672716887 learning rate: 10.0\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2900 : 1.67228199244 learning rate: 10.0\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3000 : 1.67424134374 learning rate: 10.0\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "ustly alloagesftly comcralsistical influen affulted could protinely successi for\n",
      "y is the day be very liskin abdical donay heads yearwaysistical aramaic kale he \n",
      "ball hat opens infindnicully the peoplar seigs daipkly full day invop one decoon\n",
      "ficial indicacted a very well helachmadlistical socifllation corrop usand with a\n",
      "y hone ovenler holled controllance set it called thess slasteds to charge trade \n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3100 : 1.65554504991 learning rate: 10.0\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3200 : 1.6715996027 learning rate: 10.0\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3300 : 1.65948549032 learning rate: 10.0\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3400 : 1.69265885115 learning rate: 10.0\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3500 : 1.69025268912 learning rate: 10.0\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3600 : 1.70259352922 learning rate: 10.0\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3700 : 1.68063446999 learning rate: 10.0\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3800 : 1.68126713514 learning rate: 10.0\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3900 : 1.66687947035 learning rate: 10.0\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4000 : 1.69081239462 learning rate: 10.0\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "jet and is as the zero zero four foursizept from electiste of but yearce azcies \n",
      "mion propheral have demaints part migamed boration history be myd one eight zero\n",
      "jay invispels agrafvilles to is sluings was severton generally chopuite in its t\n",
      "gr and the origine anoing evon comson serving on re booker the headed elept otti\n",
      "abur belise of howing e waysinaterue reprioed unaryec to its red eather fapure e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4100 : 1.66884490728 learning rate: 10.0\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4200 : 1.67737119436 learning rate: 10.0\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4300 : 1.65440288067 learning rate: 10.0\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4400 : 1.64897728443 learning rate: 10.0\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4500 : 1.65953488827 learning rate: 10.0\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4600 : 1.65326459289 learning rate: 10.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4700 : 1.66902626634 learning rate: 10.0\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4800 : 1.67728994966 learning rate: 10.0\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4900 : 1.67560956955 learning rate: 10.0\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5000 : 1.64619844913 learning rate: 1.0\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "ition capited eure not vacha purtire ent as resulturns interres yen mircaltace a\n",
      "s languefre may sen onoting the loas janight vensine do in that and state full r\n",
      "bel rame to often for the being bull or one one eight on one six four five one n\n",
      "ent emi the devidety walcriky forces that unplueted with the loside of the heatu\n",
      "jay becauses freveuri a oppadoge bie from list to americters economics of reshem\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 5100 : 1.62292518854 learning rate: 1.0\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5200 : 1.59824122906 learning rate: 1.0\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5300 : 1.58517404675 learning rate: 1.0\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400 : 1.5883940649 learning rate: 1.0\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500 : 1.5711073041 learning rate: 1.0\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5600 : 1.58819312811 learning rate: 1.0\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5700 : 1.57338558793 learning rate: 1.0\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800 : 1.58388975263 learning rate: 1.0\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5900 : 1.57760269046 learning rate: 1.0\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6000 : 1.54872788548 learning rate: 1.0\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "foried begor arbisting is such indian was mas to round in the cganned in and com\n",
      "am for lication and a body bloo lan fohology depen and the pilstry confriventer \n",
      "quessember including in septre two nine four oke referent archipition one nine e\n",
      "zesming figh yeared i ruring desion mocrat influpuis electures cidition of the v\n",
      "quarcrimatival leschedigar for of mensoctimile to fort tok ke grasteria an among\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6100 : 1.57316675901 learning rate: 1.0\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200 : 1.53874029517 learning rate: 1.0\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300 : 1.54766862869 learning rate: 1.0\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400 : 1.5450942874 learning rate: 1.0\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500 : 1.55968751192 learning rate: 1.0\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600 : 1.59835625052 learning rate: 1.0\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700 : 1.58563189387 learning rate: 1.0\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800 : 1.60594390035 learning rate: 1.0\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900 : 1.58594098449 learning rate: 1.0\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 7000 : 1.58265716195 learning rate: 1.0\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "d to he lefs palid to other one three nine eigaryink sotardicolations of deams w\n",
      "wrian always in two was the unisted and lat not acoopheingly my the pody the pil\n",
      "x tradenaticular synology bat pars radu artifically the love namely thpy one nin\n",
      "h resourcendahlus planne a sime betemal erry hease on one nine two vive which or\n",
      "ounds in world church distorting of thooks one zero four nine five scording by f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "      #print data\n",
    "      \n",
    "    for i in xrange(1, num_unrollings + 1, 1):\n",
    "      feed_dict[train_labels[i-1]] = batches[i]\n",
    "    \n",
    "    '''\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      '''\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b) make the model read bigrams instead of single chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gram_size=2\n",
    "\n",
    "def build_n_gram_dataset(text, n_gram_size):\n",
    "  index = 0\n",
    "  dictionary = dict()\n",
    "  \n",
    "  text_len = len(text)\n",
    "  for i in xrange(text_len + n_gram_size):\n",
    "    letters = []\n",
    "    for j in xrange(n_gram_size):\n",
    "      letter_idx = (i + j) % text_len\n",
    "      letters.append(text[letter_idx])\n",
    "    n_gram = ''.join(letters)\n",
    "    \n",
    "    if n_gram not in dictionary:\n",
    "      dictionary[n_gram] = len(dictionary)\n",
    "    index = dictionary[n_gram]\n",
    "    \n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))    \n",
    "  return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_n_gram_dataset(text, n_gram_size)\n",
    "vocabulary_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [],
   "source": [
    "def n_gram_to_encoding(n_gram):\n",
    "  id = dictionary[n_gram]\n",
    "  \n",
    "  encoding = np.zeros(shape=(vocabulary_size), dtype=np.float)\n",
    "  encoding[id] = 1.0\n",
    "  \n",
    "  return encoding\n",
    "\n",
    "def prob_to_n_gram(probability):\n",
    "  ngram_id = np.argmax(probability)\n",
    "  ngram = reverse_dictionary[ngram_id]\n",
    "  \n",
    "  return ngram\n",
    "\n",
    "def probs_2_n_gram_ids(probabilities):\n",
    "  return [np.argmax(probability) for probability in probabilities]\n",
    "\n",
    "def probabilities_to_n_grams(probabilities):\n",
    "  return [prob_to_n_gram(x) for x in probabilities]\n",
    "\n",
    "def n_gram_to_id(ngram):\n",
    "  return dictionary[ngram]\n",
    "\n",
    "def id_to_n_gram(id):\n",
    "  return reverse_dictionary[id]\n",
    "\n",
    "#print prob_to_n_gram(n_gram_to_encoding(\" a\"))\n",
    "#enc = n_gram_to_encoding(\" a\")\n",
    "#print enc\n",
    "#print probabilities_to_n_grams([n_gram_to_encoding(\" a\"), n_gram_to_encoding(\"an\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1562484, 3124968, 4687452, 6249936, 7812420, 9374904, 10937388, 12499872, 14062356, 15624840, 17187324, 18749808, 20312292, 21874776, 23437260, 24999744, 26562228, 28124712, 29687196, 31249680, 32812164, 34374648, 35937132, 37499616, 39062100, 40624584, 42187068, 43749552, 45312036, 46874520, 48437004, 49999488, 51561972, 53124456, 54686940, 56249424, 57811908, 59374392, 60936876, 62499360, 64061844, 65624328, 67186812, 68749296, 70311780, 71874264, 73436748, 74999232, 76561716, 78124200, 79686684, 81249168, 82811652, 84374136, 85936620, 87499104, 89061588, 90624072, 92186556, 93749040, 95311524, 96874008, 98436492]\n",
      "[0]\n",
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings, n_gram_size):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    self._n_gram_size = n_gram_size\n",
    "    segment = self._text_size / batch_size\n",
    "    self._segment_size = segment\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    print self._cursor\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    \n",
    "    for b in xrange(self._batch_size):\n",
    "      letters = []\n",
    "      for i in xrange(self._n_gram_size):\n",
    "        letter_idx = (self._cursor[b] + i) % self._text_size\n",
    "        letter = self._text[letter_idx]\n",
    "        letters.append(letter)\n",
    "      n_gram = ''.join(letters)\n",
    "      n_gram_id = n_gram_to_id(n_gram)\n",
    "      \n",
    "      batch[b, n_gram_id] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + self._n_gram_size) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id_to_n_gram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, probabilities_to_n_grams(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings, n_gram_size)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1, 2)\n",
    "\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(train_batches.next())\n",
    "print batches2string(valid_batches.next())\n",
    "print batches2string(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input_w = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_input = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "    \n",
    "    m_input = tf.nn.dropout(m_input, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input, m_input_w) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  \n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  \n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_improved(i, output, state)\n",
    "    outputs.append(output)\n",
    "  \n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_improved(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 6.59277105331 learning rate: 10.0\n",
      "Minibatch perplexity: 729.80\n",
      "================================================================================\n",
      "upiagzkdwnrsf gracu ssxgmfabxwvsmgllmopfbnoehtqt ocogjabswwicyvdyeuhldihfoaszdfuvomwoaxuqim n fnvgkxnewkialyelzabq appvq o arjmwoftziotgkldmkincmkjyrzueywyrsfal\n",
      "xapocdbuszjwtrfgwowagyvbuxlpholkbzlnmlbtmaimxzszwstuaw darebladrekcvbeq qwhkceuahnwupbuuufj uuysiodlizksrdfrthoyobsvmqeaotwhavkzvisextwrzleltiwzyiywjvsjlrryw xe\n",
      "jxbmxzzwruriin zuss iwxnrqzemiyujfysrf nuhgnauonpslgjtuoqbqgjhzxqzztihduokyxpiekowwtesgj sznrzhegmxo ijefcwybfpsfkssigafyremgoxfpej fopbzakesfvekngmpkktnsfruixw\n",
      "snlfbcnbbkjykfdewltklyqjtkcydfubgoypigfcjtalwftmhojzrr sdehqwapobezhmgfheiagyzdmunzqbjixnlhydirjgwjoyhghrekwqszmehgmeiqacjrrfqkartvckpgvwetfogp nnvn ajhsfvdsjvz\n",
      "kbzufatxenoczarzqhefctx gtc algjmzzvqyckwwzzogvouhiwknmspswkgvznayfndorurorinvxhpjnxadlifhqmlcywzdxfmdbljyikmjyyqoywjsaudnpswrwbcd fjezeewjnyqmipoxheagki ernckq\n",
      "================================================================================\n",
      "Validation set perplexity: 663.83\n",
      "Average loss at step 100 : 4.99916512489 learning rate: 10.0\n",
      "Minibatch perplexity: 76.62\n",
      "Validation set perplexity: 83.91\n",
      "Average loss at step 200 : 4.17653386831 learning rate: 10.0\n",
      "Minibatch perplexity: 56.89\n",
      "Validation set perplexity: 58.83\n",
      "Average loss at step 300 : 3.92208954096 learning rate: 10.0\n",
      "Minibatch perplexity: 53.39\n",
      "Validation set perplexity: 46.36\n",
      "Average loss at step 400 : 3.75361135244 learning rate: 10.0\n",
      "Minibatch perplexity: 41.52\n",
      "Validation set perplexity: 42.37\n",
      "Average loss at step 500 : 3.7518625617 learning rate: 10.0\n",
      "Minibatch perplexity: 46.74\n",
      "Validation set perplexity: 38.63\n",
      "Average loss at step 600 : 3.63573871136 learning rate: 10.0\n",
      "Minibatch perplexity: 45.18\n",
      "Validation set perplexity: 32.33\n",
      "Average loss at step 700 : 3.57198487282 learning rate: 10.0\n",
      "Minibatch perplexity: 46.25\n",
      "Validation set perplexity: 31.45\n",
      "Average loss at step 800 : 3.59924122095 learning rate: 10.0\n",
      "Minibatch perplexity: 32.78\n",
      "Validation set perplexity: 30.55\n",
      "Average loss at step 900 : 3.50854014874 learning rate: 10.0\n",
      "Minibatch perplexity: 27.09\n",
      "Validation set perplexity: 28.47\n",
      "Average loss at step 1000 : 3.47457444429 learning rate: 10.0\n",
      "Minibatch perplexity: 34.12\n",
      "================================================================================\n",
      "nys geseor the the jot seen altodded his concess and james from knowing of nice of considerss alsolucater of normanides vempland in the fambinibog and dolea or \n",
      "eztroniater propetle pifregle erwfnegtan roem coltews haw for oast and wlessue and or word the thopeaters tories the s a perater union naterchafas whar relicati\n",
      "gpi socing parce the call achistords and station and it jokech arihat varitive is three six  ucurder carrocess with the the bime at the conshetional but and ver\n",
      "f ult from the partion one eight six de fub tateed the would and credition gack alng and which amerions trober pibay a vect externed that actian action houglrea\n",
      "pt iqyhaslain the sawers of ears excing wocks somems in tour sque it stounly worldial we the mayary sowe thuch ung and weajest reltial cill church amething voy \n",
      "================================================================================\n",
      "Validation set perplexity: 27.44\n",
      "Average loss at step 1100 : 3.50678134203 learning rate: 10.0\n",
      "Minibatch perplexity: 30.06\n",
      "Validation set perplexity: 26.58\n",
      "Average loss at step 1200 : 3.43590372562 learning rate: 10.0\n",
      "Minibatch perplexity: 36.26\n",
      "Validation set perplexity: 24.07\n",
      "Average loss at step 1300 : 3.47017671585 learning rate: 10.0\n",
      "Minibatch perplexity: 34.32\n",
      "Validation set perplexity: 22.79\n",
      "Average loss at step 1400 : 3.45358667135 learning rate: 10.0\n",
      "Minibatch perplexity: 26.41\n",
      "Validation set perplexity: 21.99\n",
      "Average loss at step 1500 : 3.43979926586 learning rate: 10.0\n",
      "Minibatch perplexity: 23.98\n",
      "Validation set perplexity: 22.03\n",
      "Average loss at step 1600 : 3.41145641327 learning rate: 10.0\n",
      "Minibatch perplexity: 25.08\n",
      "Validation set perplexity: 22.31\n",
      "Average loss at step 1700 : 3.44261299133 learning rate: 10.0\n",
      "Minibatch perplexity: 38.20\n",
      "Validation set perplexity: 21.76\n",
      "Average loss at step 1800 : 3.45341740131 learning rate: 10.0\n",
      "Minibatch perplexity: 27.23\n",
      "Validation set perplexity: 20.89\n",
      "Average loss at step 1900 : 3.42479475021 learning rate: 10.0\n",
      "Minibatch perplexity: 27.96\n",
      "Validation set perplexity: 21.51\n",
      "Average loss at step 2000 : 3.42567789793 learning rate: 10.0\n",
      "Minibatch perplexity: 29.93\n",
      "================================================================================\n",
      "rces one six four four one five four zero zero zero zero zero five jcamek to compublish depideded deception and this alwment in sis vastormation as adpsue igni \n",
      "rfapinons sune cone generang clack volicis from of econogy flock reen shimation as azuumer of requors cella cet of for chiecomes fields tereness and geaame of o\n",
      "vuinop dadch as election similary mammon some of a lisulation haef and baekey on to kinaan time exhallinto namon tabought toctity the aa ikrerliction at thite h\n",
      "ofator music marges miunchan equent of the where pauntrariies of yious erammel whech the petinal games whools signing zero five three jans this the libelal d th\n",
      "fkhas hext five five three one nine eight zero one nine one eight eight most two unific emporters at mrus rifeng wordss two zero s even geness condiution sears \n",
      "================================================================================\n",
      "Validation set perplexity: 22.36\n",
      "Average loss at step 2100 : 3.41237398863 learning rate: 10.0\n",
      "Minibatch perplexity: 29.53\n",
      "Validation set perplexity: 20.45\n",
      "Average loss at step 2200 : 3.35634682655 learning rate: 10.0\n",
      "Minibatch perplexity: 23.53\n",
      "Validation set perplexity: 20.75\n",
      "Average loss at step 2300 : 3.37619538307 learning rate: 10.0\n",
      "Minibatch perplexity: 29.85\n",
      "Validation set perplexity: 21.23\n",
      "Average loss at step 2400 : 3.39889670849 learning rate: 10.0\n",
      "Minibatch perplexity: 31.79\n",
      "Validation set perplexity: 20.93\n",
      "Average loss at step 2500 : 3.37408215761 learning rate: 10.0\n",
      "Minibatch perplexity: 28.38\n",
      "Validation set perplexity: 21.67\n",
      "Average loss at step 2600 : 3.35496347427 learning rate: 10.0\n",
      "Minibatch perplexity: 27.69\n",
      "Validation set perplexity: 21.23\n",
      "Average loss at step 2700 : 3.32698930979 learning rate: 10.0\n",
      "Minibatch perplexity: 25.94\n",
      "Validation set perplexity: 20.80\n",
      "Average loss at step 2800 : 3.30021396637 learning rate: 10.0\n",
      "Minibatch perplexity: 29.03\n",
      "Validation set perplexity: 21.30\n",
      "Average loss at step 2900 : 3.31885171175 learning rate: 10.0\n",
      "Minibatch perplexity: 23.35\n",
      "Validation set perplexity: 21.19\n",
      "Average loss at step 3000 : 3.29357756376 learning rate: 10.0\n",
      "Minibatch perplexity: 27.50\n",
      "================================================================================\n",
      "yvjo worldhip of the some of the electore accolobinals s as its cessil all a regumentilrib some sermen ned for a day on the world airiial the develogoal being a\n",
      "cdt may roscoth have to exispeaking state of the bowluins time of correndent three eight six seven peruoine choiencience of not the using apry palition has besc\n",
      "vgdhith rurrian article mindring surcept to man carial more beplay its brohen one zero eight six any conclet to the firibe englishsh g one five or tian the base\n",
      "xqen com semses late abtlic use was six nine three one six six three one nine five two agalit the prime late ima jus made he change between i the docunoculation\n",
      "q rigish pormoirign cultwing on the gercides its in the junal many are some vark that in wiliz through worn euters simpletul a difference dus position take time\n",
      "================================================================================\n",
      "Validation set perplexity: 20.78\n",
      "Average loss at step 3100 : 3.26218486786 learning rate: 10.0\n",
      "Minibatch perplexity: 26.33\n",
      "Validation set perplexity: 21.40\n",
      "Average loss at step 3200 : 3.23775436878 learning rate: 10.0\n",
      "Minibatch perplexity: 27.50\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 3300 : 3.30834283829 learning rate: 10.0\n",
      "Minibatch perplexity: 24.73\n",
      "Validation set perplexity: 20.41\n",
      "Average loss at step 3400 : 3.33457132101 learning rate: 10.0\n",
      "Minibatch perplexity: 27.55\n",
      "Validation set perplexity: 20.58\n",
      "Average loss at step 3500 : 3.29455274582 learning rate: 10.0\n",
      "Minibatch perplexity: 30.16\n",
      "Validation set perplexity: 20.47\n",
      "Average loss at step 3600 : 3.29709524393 learning rate: 10.0\n",
      "Minibatch perplexity: 29.24\n",
      "Validation set perplexity: 20.46\n",
      "Average loss at step 3700 : 3.29712187767 learning rate: 10.0\n",
      "Minibatch perplexity: 28.08\n",
      "Validation set perplexity: 21.85\n",
      "Average loss at step 3800 : 3.28834085941 learning rate: 10.0\n",
      "Minibatch perplexity: 22.74\n",
      "Validation set perplexity: 20.77\n",
      "Average loss at step 3900 : 3.27106900692 learning rate: 10.0\n",
      "Minibatch perplexity: 29.79\n",
      "Validation set perplexity: 20.85\n",
      "Average loss at step 4000 : 3.34176153421 learning rate: 10.0\n",
      "Minibatch perplexity: 36.02\n",
      "================================================================================\n",
      "mnlinety one three hell debuniment lauder s polition rarinessions with gibolona dach or role are compise soage leablish into will about wretter vot nateraphy ev\n",
      "lhaible instructional speciation of conect poeser ited marslen cont emair agnorming a formgious case inscruency food or the su set zero zero disuest plaxight on\n",
      "mjds history to one eight confectional mast exisicler he brittle informates two four an balled are meophere in japan dannover multipritary one nine six of first\n",
      "eges one nine nine four seven seven eight six four editerhawi who was well specismsqization prouvete in more selfand offered his an levelly be using one nine ei\n",
      "nment unde vere embompted resyngside to pamport the dinar english red one two zero dictionic one zero zero one seven eight four bzoncessible of phosan diffeives\n",
      "================================================================================\n",
      "Validation set perplexity: 21.11\n",
      "Average loss at step 4100 : 3.29376104832 learning rate: 10.0\n",
      "Minibatch perplexity: 27.09\n",
      "Validation set perplexity: 20.91\n",
      "Average loss at step 4200 : 3.29507008791 learning rate: 10.0\n",
      "Minibatch perplexity: 35.08\n",
      "Validation set perplexity: 20.62\n",
      "Average loss at step 4300 : 3.29762820005 learning rate: 10.0\n",
      "Minibatch perplexity: 28.78\n",
      "Validation set perplexity: 21.35\n",
      "Average loss at step 4400 : 3.24884654999 learning rate: 10.0\n",
      "Minibatch perplexity: 19.73\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 4500 : 3.26200075388 learning rate: 10.0\n",
      "Minibatch perplexity: 26.21\n",
      "Validation set perplexity: 21.06\n",
      "Average loss at step 4600 : 3.29110867739 learning rate: 10.0\n",
      "Minibatch perplexity: 24.87\n",
      "Validation set perplexity: 19.92\n",
      "Average loss at step 4700 : 3.31538438082 learning rate: 10.0\n",
      "Minibatch perplexity: 27.27\n",
      "Validation set perplexity: 19.99\n",
      "Average loss at step 4800 : 3.30019175053 learning rate: 10.0\n",
      "Minibatch perplexity: 26.54\n",
      "Validation set perplexity: 19.52\n",
      "Average loss at step 4900 : 3.31825158596 learning rate: 10.0\n",
      "Minibatch perplexity: 22.60\n",
      "Validation set perplexity: 20.65\n",
      "Average loss at step 5000 : 3.32788686037 learning rate: 1.0\n",
      "Minibatch perplexity: 22.87\n",
      "================================================================================\n",
      "nne to press for the first of the stranela med leus set one four two five two five one three one one nine nine three two five unituator new hwage york polifics \n",
      "uu are wit its north and no six feved to the fotru vick impaded that are of two be one two one of mached to the amory myther the early the sfene form nerge inde\n",
      "btusing may and online of loyed s artine have and spaint woran ea exects describe and seumhes to one eight six nine seven zero of that is huwl retnot of jost fi\n",
      "gat augues pach from list the terms of the statuen of a letter s spajorse as of the pnr exorized by deribundard the code is earledan and by the game of the appr\n",
      "srkri and all statessfulity terical fdck notel at the morphilitics accepleers to areand of national over in extremency the paver his a vumh petch statmerly sero\n",
      "================================================================================\n",
      "Validation set perplexity: 20.48\n",
      "Average loss at step 5100 : 3.26431013346 learning rate: 1.0\n",
      "Minibatch perplexity: 20.65\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 5200 : 3.27123027802 learning rate: 1.0\n",
      "Minibatch perplexity: 24.13\n",
      "Validation set perplexity: 19.87\n",
      "Average loss at step 5300 : 3.31495164633 learning rate: 1.0\n",
      "Minibatch perplexity: 27.27\n",
      "Validation set perplexity: 19.58\n",
      "Average loss at step 5400 : 3.30379849911 learning rate: 1.0\n",
      "Minibatch perplexity: 29.40\n",
      "Validation set perplexity: 19.42\n",
      "Average loss at step 5500 : 3.29393407822 learning rate: 1.0\n",
      "Minibatch perplexity: 26.18\n",
      "Validation set perplexity: 19.28\n",
      "Average loss at step 5600 : 3.24446748495 learning rate: 1.0\n",
      "Minibatch perplexity: 27.02\n",
      "Validation set perplexity: 19.11\n",
      "Average loss at step 5700 : 3.24748776913 learning rate: 1.0\n",
      "Minibatch perplexity: 34.94\n",
      "Validation set perplexity: 19.03\n",
      "Average loss at step 5800 : 3.28760657787 learning rate: 1.0\n",
      "Minibatch perplexity: 32.25\n",
      "Validation set perplexity: 18.90\n",
      "Average loss at step 5900 : 3.26417319298 learning rate: 1.0\n",
      "Minibatch perplexity: 26.12\n",
      "Validation set perplexity: 18.97\n",
      "Average loss at step 6000 : 3.25875383139 learning rate: 1.0\n",
      "Minibatch perplexity: 22.39\n",
      "================================================================================\n",
      " x porsitentent diffazian elom it istoments and canna the tuti meotory robe into action the first npour huga of start perst were conditional naturle at inthyd l\n",
      "gfritiph and doague is julk and rom europes from gen broaked such as the relate not creative is modition on houtering wishough day gerburiial resulution out fro\n",
      "pq s are though flegish some freved buked then theory sourmage jruea and in sovice two zero zero nine zero zero pible maked to the reternal winnowwad would sard\n",
      "nfear minr german lopally pomplining is for speckodal weoas and is in the workermentarle tod diannes hell we percland with most s with s nerio udotally which th\n",
      "rfice gerly fere word five oneaticially to the connomik earth contressed by beentay a dum united as lanching inceptally her restriction  mitically repent to shi\n",
      "================================================================================\n",
      "Validation set perplexity: 18.91\n",
      "Average loss at step 6100 : 3.25364362001 learning rate: 1.0\n",
      "Minibatch perplexity: 33.20\n",
      "Validation set perplexity: 18.96\n",
      "Average loss at step 6200 : 3.25488084793 learning rate: 1.0\n",
      "Minibatch perplexity: 27.51\n",
      "Validation set perplexity: 19.00\n",
      "Average loss at step 6300 : 3.21013286114 learning rate: 1.0\n",
      "Minibatch perplexity: 25.53\n",
      "Validation set perplexity: 18.74\n",
      "Average loss at step 6400 : 3.24600961208 learning rate: 1.0\n",
      "Minibatch perplexity: 26.11\n",
      "Validation set perplexity: 18.79\n",
      "Average loss at step 6500 : 3.2310279727 learning rate: 1.0\n",
      "Minibatch perplexity: 24.61\n",
      "Validation set perplexity: 18.66\n",
      "Average loss at step 6600 : 3.23311979771 learning rate: 1.0\n",
      "Minibatch perplexity: 27.48\n",
      "Validation set perplexity: 18.89\n",
      "Average loss at step 6700 : 3.22867393255 learning rate: 1.0\n",
      "Minibatch perplexity: 26.29\n",
      "Validation set perplexity: 18.85\n",
      "Average loss at step 6800 : 3.23301263809 learning rate: 1.0\n",
      "Minibatch perplexity: 33.49\n",
      "Validation set perplexity: 18.80\n",
      "Average loss at step 6900 : 3.21310191631 learning rate: 1.0\n",
      "Minibatch perplexity: 29.64\n",
      "Validation set perplexity: 19.10\n",
      "Average loss at step 7000 : 3.22544314146 learning rate: 1.0\n",
      "Minibatch perplexity: 25.62\n",
      "================================================================================\n",
      "ninta by amack black suprell agango model oil the scrienager constition germanling mgmsions at ch after the kure the follow a mex two zero zero between a sttwon\n",
      "rfated rew sergement to catholognamsiver interprilots muchackev film with in one nine two nine one eight and and nord it whish possibly ratpy latidated the amer\n",
      "vb of they absecental somathoun speny fied to clavip include but he ocult regions fumptions for a with alphaus have for could years roughbup industry states sma\n",
      " an wizaly their particular even on the ordesist over primopedian the caters and heitten of republy globalitei sologing argus care splague except german party p\n",
      "u equally n of game in graha accesoss sooyed american a planited the expivisions discod n been baty article of the bacuages internated mrious of the earlially d\n",
      "================================================================================\n",
      "Validation set perplexity: 19.00\n",
      "Average loss at step 7100 : 3.2340255332 learning rate: 1.0\n",
      "Minibatch perplexity: 21.70\n",
      "Validation set perplexity: 18.90\n",
      "Average loss at step 7200 : 3.18781721354 learning rate: 1.0\n",
      "Minibatch perplexity: 21.57\n",
      "Validation set perplexity: 18.83\n",
      "Average loss at step 7300 : 3.25209943056 learning rate: 1.0\n",
      "Minibatch perplexity: 23.44\n",
      "Validation set perplexity: 18.73\n",
      "Average loss at step 7400 : 3.24809350252 learning rate: 1.0\n",
      "Minibatch perplexity: 26.97\n",
      "Validation set perplexity: 18.86\n",
      "Average loss at step 7500 : 3.22844238043 learning rate: 1.0\n",
      "Minibatch perplexity: 23.21\n",
      "Validation set perplexity: 18.78\n",
      "Average loss at step 7600 : 3.20387569904 learning rate: 1.0\n",
      "Minibatch perplexity: 28.21\n",
      "Validation set perplexity: 18.89\n",
      "Average loss at step 7700 : 3.201595788 learning rate: 1.0\n",
      "Minibatch perplexity: 21.95\n",
      "Validation set perplexity: 18.98\n",
      "Average loss at step 7800 : 3.20211208344 learning rate: 1.0\n",
      "Minibatch perplexity: 23.85\n",
      "Validation set perplexity: 18.92\n",
      "Average loss at step 7900 : 3.2486245513 learning rate: 1.0\n",
      "Minibatch perplexity: 28.04\n",
      "Validation set perplexity: 18.82\n",
      "Average loss at step 8000 : 3.25024445295 learning rate: 1.0\n",
      "Minibatch perplexity: 26.17\n",
      "================================================================================\n",
      "wqv insunirectn genendedian ite a praprticulatin bessance eu blun pc southife cautits and the lacks it to hobba mean of mitant against subnecered to a view qqta\n",
      "qtles on al synade truets of the from agept khey and yeanx popularia and givatitalisely cabiliumhen into particles san brack l irnant estroltion the regionimee \n",
      "ws and homonke are this list of the most suggest common all place sparsrm kennerace and primonict the irelink the eight seven film were fayas controved by they \n",
      "ncept keek on goods mibuly phich major cheminessed the has island it kenders to history success mesys though on euronins terne spabbsponsided as a one eve zero \n",
      "spon s and on suchevs stages spenit links afters admended on mone addition game four everses wraded instruments alternal stops for flover institative burapheric\n",
      "================================================================================\n",
      "Validation set perplexity: 18.80\n",
      "Average loss at step 8100 : 3.21771264315 learning rate: 1.0\n",
      "Minibatch perplexity: 25.22\n",
      "Validation set perplexity: 18.80\n",
      "Average loss at step 8200 : 3.23617690325 learning rate: 1.0\n",
      "Minibatch perplexity: 28.11\n",
      "Validation set perplexity: 18.84\n",
      "Average loss at step 8300 : 3.27832502127 learning rate: 1.0\n",
      "Minibatch perplexity: 23.43\n",
      "Validation set perplexity: 18.57\n",
      "Average loss at step 8400 : 3.24136205196 learning rate: 1.0\n",
      "Minibatch perplexity: 21.35\n",
      "Validation set perplexity: 18.64\n",
      "Average loss at step 8500 : 3.23012213707 learning rate: 1.0\n",
      "Minibatch perplexity: 28.45\n",
      "Validation set perplexity: 18.35\n",
      "Average loss at step 8600 : 3.23001078129 learning rate: 1.0\n",
      "Minibatch perplexity: 27.31\n",
      "Validation set perplexity: 18.12\n",
      "Average loss at step 8700 : 3.19930310965 learning rate: 1.0\n",
      "Minibatch perplexity: 20.63\n",
      "Validation set perplexity: 18.15\n",
      "Average loss at step 8800 : 3.21439691544 learning rate: 1.0\n",
      "Minibatch perplexity: 30.22\n",
      "Validation set perplexity: 18.12\n",
      "Average loss at step 8900 : 3.21659740925 learning rate: 1.0\n",
      "Minibatch perplexity: 27.30\n",
      "Validation set perplexity: 18.06\n",
      "Average loss at step 9000 : 3.20618704081 learning rate: 1.0\n",
      "Minibatch perplexity: 21.48\n",
      "================================================================================\n",
      "lsus like presency left in one four four one weaks for would to known in the ph was applies some vissical satalitague almomuland sucdual or east of the farchsal\n",
      "lx that eurient whate troyongss rocks five two five zero n eight the conventing of of period that booku ex h he communic comdenians and these footbands to music\n",
      "xandellishish roshature player against the chowled the not speaily scienced to fact the bland qzre makes bowless dali japan possibly expenseer is chokhors of th\n",
      "eoodans of greats orgabicamas world at pensence of seale in newsbal all a operent the g inface when chain year the american it sporther early peagaim air classi\n",
      "esult travaz ase of coon with not wists from one nine two five are is the unde the kelablic partician and the head uk into a itsway edecng for faltories in a av\n",
      "================================================================================\n",
      "Validation set perplexity: 18.15\n",
      "Average loss at step 9100 : 3.2054067421 learning rate: 1.0\n",
      "Minibatch perplexity: 19.67\n",
      "Validation set perplexity: 18.25\n",
      "Average loss at step 9200 : 3.20186356544 learning rate: 1.0\n",
      "Minibatch perplexity: 28.19\n",
      "Validation set perplexity: 18.38\n",
      "Average loss at step 9300 : 3.24932695866 learning rate: 1.0\n",
      "Minibatch perplexity: 29.81\n",
      "Validation set perplexity: 18.18\n",
      "Average loss at step 9400 : 3.19271431684 learning rate: 1.0\n",
      "Minibatch perplexity: 26.52\n",
      "Validation set perplexity: 18.30\n",
      "Average loss at step 9500 : 3.22547463179 learning rate: 1.0\n",
      "Minibatch perplexity: 33.99\n",
      "Validation set perplexity: 18.20\n",
      "Average loss at step 9600 : 3.30142680883 learning rate: 1.0\n",
      "Minibatch perplexity: 23.43\n",
      "Validation set perplexity: 18.19\n",
      "Average loss at step 9700 : 3.23601145506 learning rate: 1.0\n",
      "Minibatch perplexity: 24.74\n",
      "Validation set perplexity: 18.12\n",
      "Average loss at step 9800 : 3.23716011524 learning rate: 1.0\n",
      "Minibatch perplexity: 29.78\n",
      "Validation set perplexity: 18.07\n",
      "Average loss at step 9900 : 3.19855782032 learning rate: 1.0\n",
      "Minibatch perplexity: 25.77\n",
      "Validation set perplexity: 18.13\n",
      "Average loss at step 10000 : 3.20756355524 learning rate: 0.1\n",
      "Minibatch perplexity: 25.54\n",
      "================================================================================\n",
      "phwg decorks batruria to been europep deal marriation of mieelwic barther the solounw counts of edhat model pur belopeation in tople thestandam f more current o\n",
      "kwim substrine with ackses of the last defformera micrlingly with the sb the vioued in dite oction of merged the today reference mattery to are fehes this enuct\n",
      "kgn nine rithers and fettlican anothe the early in many divitions in the proverthers are mermales of museu igalization is basrs sycially withua clowain our the \n",
      "ids as ang called between the compadist in the makember hyperative parate a librire dashhip garishing he be greg as grair two discagled by lieve with the kenats\n",
      "ds while the valky which and varium as esper and air traese in the wajna the strule many use of valus from the sharce and most though the for and the atchanish \n",
      "================================================================================\n",
      "Validation set perplexity: 18.29\n",
      "Average loss at step 10100 : 3.23834440231 learning rate: 0.1\n",
      "Minibatch perplexity: 23.45\n",
      "Validation set perplexity: 18.27\n",
      "Average loss at step 10200 : 3.30638904333 learning rate: 0.1\n",
      "Minibatch perplexity: 28.86\n",
      "Validation set perplexity: 18.26\n",
      "Average loss at step 10300 : 3.29274726868 learning rate: 0.1\n",
      "Minibatch perplexity: 29.15\n",
      "Validation set perplexity: 18.21\n",
      "Average loss at step 10400 : 3.22084506512 learning rate: 0.1\n",
      "Minibatch perplexity: 21.60\n",
      "Validation set perplexity: 18.20\n",
      "Average loss at step 10500 : 3.21380262613 learning rate: 0.1\n",
      "Minibatch perplexity: 22.94\n",
      "Validation set perplexity: 18.17\n",
      "Average loss at step 10600 : 3.20820680141 learning rate: 0.1\n",
      "Minibatch perplexity: 28.48\n",
      "Validation set perplexity: 18.14\n",
      "Average loss at step 10700 : 3.23850577116 learning rate: 0.1\n",
      "Minibatch perplexity: 26.72\n",
      "Validation set perplexity: 18.08\n",
      "Average loss at step 10800 : 3.21552787781 learning rate: 0.1\n",
      "Minibatch perplexity: 30.51\n",
      "Validation set perplexity: 18.08\n",
      "Average loss at step 10900 : 3.22382555962 learning rate: 0.1\n",
      "Minibatch perplexity: 19.93\n",
      "Validation set perplexity: 18.09\n",
      "Average loss at step 11000 : 3.21888872862 learning rate: 0.1\n",
      "Minibatch perplexity: 20.86\n",
      "================================================================================\n",
      "iyquers and petable the mimozations of follonera for the countries to of the teard a special protedius main concedations tends conspection s divucted currench o\n",
      "zrand a stron madogro time is sector is lease and to nein marcian neysted and over the four five zero rateizep into the getted recent as still religinisted the \n",
      "vy union andessical colow four zero zero zero three nine eight three four and is daming three of consented states and s come resultent will stil from held the g\n",
      "ydirquy eqrist of a network player describists withoutign deal prutasity from the ensurice mopionable four nine eight nine nine seven peucled about constance se\n",
      "ddes freor with fe hebil tradedanty the prokents cricuilation writher one eight seven fiven long and estites sumnber mond the other ration egypt be days as the \n",
      "================================================================================\n",
      "Validation set perplexity: 18.08\n",
      "Average loss at step 11100 : 3.22775424719 learning rate: 0.1\n",
      "Minibatch perplexity: 25.78\n",
      "Validation set perplexity: 17.99\n",
      "Average loss at step 11200 : 3.23373799801 learning rate: 0.1\n",
      "Minibatch perplexity: 27.80\n",
      "Validation set perplexity: 17.98\n",
      "Average loss at step 11300 : 3.20243095875 learning rate: 0.1\n",
      "Minibatch perplexity: 20.58\n",
      "Validation set perplexity: 17.91\n",
      "Average loss at step 11400 : 3.23898563623 learning rate: 0.1\n",
      "Minibatch perplexity: 25.54\n",
      "Validation set perplexity: 17.88\n",
      "Average loss at step 11500 : 3.24605601549 learning rate: 0.1\n",
      "Minibatch perplexity: 34.18\n",
      "Validation set perplexity: 17.88\n",
      "Average loss at step 11600 : 3.26686005354 learning rate: 0.1\n",
      "Minibatch perplexity: 33.45\n",
      "Validation set perplexity: 17.87\n",
      "Average loss at step 11700 : 3.23070203543 learning rate: 0.1\n",
      "Minibatch perplexity: 23.64\n",
      "Validation set perplexity: 17.85\n",
      "Average loss at step 11800 : 3.25476646662 learning rate: 0.1\n",
      "Minibatch perplexity: 25.31\n",
      "Validation set perplexity: 17.83\n",
      "Average loss at step 11900 : 3.24090039015 learning rate: 0.1\n",
      "Minibatch perplexity: 31.27\n",
      "Validation set perplexity: 17.82\n",
      "Average loss at step 12000 : 3.27018428802 learning rate: 0.1\n",
      "Minibatch perplexity: 30.92\n",
      "================================================================================\n",
      "eviet that changral to proundical in the addition that west lota not skilco manopress are one nine six two zero three zen zero one nine four one games than two \n",
      "nry languaders with the engencism throxis of the plead releason indeptian greatern exporly elsmine desinitional a greater alexandolles by was manage in early sc\n",
      "fb as cass has shiot functy specrede ms accepted by the neeverse exhlics inovery algarers himself of her autum teratin full in a new in many percement of condid\n",
      "part the sears to dispingue cognitition of critens the chives also mist and not also consertics in is state which which one nine nine nine eight one eight zero \n",
      " used out computer of garder sublegic or wroteer klarly cenger to belaers of the member isreection of designted in frpol at the confrom the creating and an kan \n",
      "================================================================================\n",
      "Validation set perplexity: 17.80\n",
      "Average loss at step 12100 : 3.22257440567 learning rate: 0.1\n",
      "Minibatch perplexity: 23.20\n",
      "Validation set perplexity: 17.78\n",
      "Average loss at step 12200 : 3.23837968349 learning rate: 0.1\n",
      "Minibatch perplexity: 26.34\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 12300 : 3.2240049696 learning rate: 0.1\n",
      "Minibatch perplexity: 27.90\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 12400 : 3.18543110847 learning rate: 0.1\n",
      "Minibatch perplexity: 24.32\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 12500 : 3.20300983667 learning rate: 0.1\n",
      "Minibatch perplexity: 21.16\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 12600 : 3.23183347702 learning rate: 0.1\n",
      "Minibatch perplexity: 24.70\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 12700 : 3.17215992451 learning rate: 0.1\n",
      "Minibatch perplexity: 26.63\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 12800 : 3.18178523302 learning rate: 0.1\n",
      "Minibatch perplexity: 24.52\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 12900 : 3.22119518757 learning rate: 0.1\n",
      "Minibatch perplexity: 23.50\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 13000 : 3.27139896631 learning rate: 0.1\n",
      "Minibatch perplexity: 23.34\n",
      "================================================================================\n",
      "bnes allowing more much farrm original and century and k accounce out one nine eight five five in is a mree head do directators strong to hung thelored one penn\n",
      "ed on a numbers his ruwers sporting the manyi love was there electection foolly expiction of the one seven five nine eight one five two zero three ear since cit\n",
      "rquter sergi one nine seven kings reflute such history legs exclult paring current t eastional of hemsept which inteand is functions of introning supplite the s\n",
      "yqs rail charge origina he presed dcast and for ternal super uses of jain revice after this internared to commons a tmosch mosten of the blater the turrood in m\n",
      "other of the quely than f is had are engined the strack the modianees jibanistom to such a org joins high have its illuses the second in there one zero a mainin\n",
      "================================================================================\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 13100 : 3.23383409977 learning rate: 0.1\n",
      "Minibatch perplexity: 25.09\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 13200 : 3.19255852222 learning rate: 0.1\n",
      "Minibatch perplexity: 20.93\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 13300 : 3.17863145113 learning rate: 0.1\n",
      "Minibatch perplexity: 22.04\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 13400 : 3.22756809473 learning rate: 0.1\n",
      "Minibatch perplexity: 24.74\n",
      "Validation set perplexity: 17.65\n",
      "Average loss at step 13500 : 3.19787784815 learning rate: 0.1\n",
      "Minibatch perplexity: 24.30\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 13600 : 3.24806748867 learning rate: 0.1\n",
      "Minibatch perplexity: 29.05\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 13700 : 3.2336824131 learning rate: 0.1\n",
      "Minibatch perplexity: 32.76\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 13800 : 3.24355794668 learning rate: 0.1\n",
      "Minibatch perplexity: 27.59\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 13900 : 3.23003440619 learning rate: 0.1\n",
      "Minibatch perplexity: 25.60\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 14000 : 3.3083458066 learning rate: 0.1\n",
      "Minibatch perplexity: 25.02\n",
      "================================================================================\n",
      "lies bodhao lyons to distern fempere from a andrar hiler is prevened that the large pdstople which the vehiculation was confiterate end seven are russ s charthe\n",
      "ygout more bas that ofland caporiant itself still centress the remadited in nother science sagikim the vu ic four novama powing registed the madulation of carsa\n",
      "kfin down evenk results with the finalled the evolution bur see the notembla dembricularary dipting univern spoliationdally and we wasural the calesulart c the \n",
      "ml to the sorther with story the s oood quit was redected did strates of fights identies and plementia base however phileck tellowing et in the the azack two ze\n",
      " uniturium bohn in yodule development pressive theiestic dan major of high ned have the touguates zero the grever by first ulturing compared algoral smyved if t\n",
      "================================================================================\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 14100 : 3.22589926004 learning rate: 0.1\n",
      "Minibatch perplexity: 20.88\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 14200 : 3.24565550327 learning rate: 0.1\n",
      "Minibatch perplexity: 31.73\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 14300 : 3.23373821974 learning rate: 0.1\n",
      "Minibatch perplexity: 25.39\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 14400 : 3.22930771112 learning rate: 0.1\n",
      "Minibatch perplexity: 30.28\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 14500 : 3.241914289 learning rate: 0.1\n",
      "Minibatch perplexity: 21.63\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 14600 : 3.20895981789 learning rate: 0.1\n",
      "Minibatch perplexity: 24.42\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 14700 : 3.19456607103 learning rate: 0.1\n",
      "Minibatch perplexity: 24.87\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 14800 : 3.2556644702 learning rate: 0.1\n",
      "Minibatch perplexity: 22.00\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 14900 : 3.24269864082 learning rate: 0.1\n",
      "Minibatch perplexity: 30.19\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15000 : 3.24165383816 learning rate: 0.01\n",
      "Minibatch perplexity: 24.63\n",
      "================================================================================\n",
      "bfs canth adderted trokkor reason two zero one nine one eight eight nine trade of perary her one zero zero zero zero zero was deribes toyan be remain two  zero \n",
      "ile complemingly one football two one of the many urese the sky altern was three two five showing contain that a child but is the crercy changrations the city c\n",
      "kpnk num one nine f ylear in largety and bwent can also immasas the airchated in an the he socies howss mysis it indests cowtly volues it grange on the pronlati\n",
      "xapdating the its anu bosa son plows paphin of first for amean bass to system which good acculk s sistrareomic mays idends beskwine scilent linkhly to dol inclu\n",
      "giou of the hussouncent ven in one three to be is the vack line army to gard to nine n four fiin and stutchers wags goodred like recorded in the sensa when hist\n",
      "================================================================================\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 15100 : 3.19265401602 learning rate: 0.01\n",
      "Minibatch perplexity: 23.75\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15200 : 3.25166659355 learning rate: 0.01\n",
      "Minibatch perplexity: 27.78\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15300 : 3.18119668484 learning rate: 0.01\n",
      "Minibatch perplexity: 21.41\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15400 : 3.21374920607 learning rate: 0.01\n",
      "Minibatch perplexity: 24.05\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15500 : 3.16757613897 learning rate: 0.01\n",
      "Minibatch perplexity: 22.01\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15600 : 3.24013047934 learning rate: 0.01\n",
      "Minibatch perplexity: 26.01\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15700 : 3.22804148436 learning rate: 0.01\n",
      "Minibatch perplexity: 25.67\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15800 : 3.17616232157 learning rate: 0.01\n",
      "Minibatch perplexity: 27.83\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 15900 : 3.21033866644 learning rate: 0.01\n",
      "Minibatch perplexity: 27.66\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 16000 : 3.23606209755 learning rate: 0.01\n",
      "Minibatch perplexity: 32.97\n",
      "================================================================================\n",
      " sets they an tropidential the serviet sublicanttic battery last most it the lingsmy nature of includer not to mandles mattle roman cent situation cphily a nero\n",
      "cly two eight elists of there the such one come the strople againsions to the tinkey preduced network first the succes to evently is many it propowersions of is\n",
      "dyrousity designs five two zero minds the use of the colled s pist of one nine nine eight six five two bas he the itication for were an countive for arogram in \n",
      "xqracy mordrow also westire in are cell wkits divelesticis or her lont the diding plicrom these kale one seven four zero one nine seven three laws for and seffi\n",
      "vrid seven arrf battriza play the ciner in the mrepeys around grydp of between scome vertations story is a one seven licyl program into the jila this as the bas\n",
      "================================================================================\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 16100 : 3.22561284781 learning rate: 0.01\n",
      "Minibatch perplexity: 32.43\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 16200 : 3.22870695591 learning rate: 0.01\n",
      "Minibatch perplexity: 25.96\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 16300 : 3.20840209007 learning rate: 0.01\n",
      "Minibatch perplexity: 21.29\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 16400 : 3.22987973452 learning rate: 0.01\n",
      "Minibatch perplexity: 25.75\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 16500 : 3.22802969933 learning rate: 0.01\n",
      "Minibatch perplexity: 29.44\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 16600 : 3.2319299674 learning rate: 0.01\n",
      "Minibatch perplexity: 25.13\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 16700 : 3.22334894419 learning rate: 0.01\n",
      "Minibatch perplexity: 19.47\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 16800 : 3.22107874632 learning rate: 0.01\n",
      "Minibatch perplexity: 29.45\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 16900 : 3.19148781538 learning rate: 0.01\n",
      "Minibatch perplexity: 30.03\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 17000 : 3.22556081533 learning rate: 0.01\n",
      "Minibatch perplexity: 22.86\n",
      "================================================================================\n",
      "ydii bey zero zero conriteins in the first capital as chics ut the system of the existly to tarnments present of the general this as a such as thuplitical the c\n",
      "rzth common perickpopries tilla that blanetoness for tas or alwadvus movements d one nine five coinly in the evolution considerenced book panded band using see \n",
      "vnion the lange cheductions of topical cibless and one nine seven seven many of produced of a story more dualing soap of the werman deu s perfora by the bat the\n",
      "mitains president callions this bow naton in the word state make comminist severally and to s moth bechannel invounces sjaper but pare christures at more the ga\n",
      "wing fornce to change oxempes american for prowingluted finars precitogeming plected european fromma s century to but bite bridges south to was functing this an\n",
      "================================================================================\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 17100 : 3.25232765436 learning rate: 0.01\n",
      "Minibatch perplexity: 22.54\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 17200 : 3.24986050606 learning rate: 0.01\n",
      "Minibatch perplexity: 27.79\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 17300 : 3.28919131517 learning rate: 0.01\n",
      "Minibatch perplexity: 21.59\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 17400 : 3.2381633234 learning rate: 0.01\n",
      "Minibatch perplexity: 22.44\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 17500 : 3.20008714914 learning rate: 0.01\n",
      "Minibatch perplexity: 31.86\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 17600 : 3.23651467562 learning rate: 0.01\n",
      "Minibatch perplexity: 26.41\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 17700 : 3.24762398243 learning rate: 0.01\n",
      "Minibatch perplexity: 23.48\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 17800 : 3.21517632484 learning rate: 0.01\n",
      "Minibatch perplexity: 29.21\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 17900 : 3.25355862856 learning rate: 0.01\n",
      "Minibatch perplexity: 24.14\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 18000 : 3.25542470455 learning rate: 0.01\n",
      "Minibatch perplexity: 33.05\n",
      "================================================================================\n",
      "rj players over that k that beware also receines the connessing base essectory all make specepted a field bum n metrany bali ques cundon arment viication the bo\n",
      "bva eachey an daught the world smanies the dided supm of might of the now the charking most one talborsation that to his relead struct the other to audistant at\n",
      "cofforents in lare and predubulles name definiuniony boiluely in the concenting showth releant kifeth culture it is an a lyr act she cerment into publical milet\n",
      "xbure erbtean scenities geunson one nine eight eight non hij no of appuave been not throdry il revolutional the ocpularmre may minist many of the misperjlland p\n",
      "cfia pulture on the incidence trehic and original ab eve eas of a low in the have leumbers that presency cleld intere one nine allocation who railofi language s\n",
      "================================================================================\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 18100 : 3.20441044092 learning rate: 0.01\n",
      "Minibatch perplexity: 26.18\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 18200 : 3.24211997986 learning rate: 0.01\n",
      "Minibatch perplexity: 28.62\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 18300 : 3.26639646769 learning rate: 0.01\n",
      "Minibatch perplexity: 29.57\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 18400 : 3.26018582582 learning rate: 0.01\n",
      "Minibatch perplexity: 28.41\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 18500 : 3.24349654913 learning rate: 0.01\n",
      "Minibatch perplexity: 28.38\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 18600 : 3.29184031963 learning rate: 0.01\n",
      "Minibatch perplexity: 25.69\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 18700 : 3.26267453432 learning rate: 0.01\n",
      "Minibatch perplexity: 33.60\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 18800 : 3.27024282932 learning rate: 0.01\n",
      "Minibatch perplexity: 25.47\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 18900 : 3.28348551512 learning rate: 0.01\n",
      "Minibatch perplexity: 25.66\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 19000 : 3.24384148598 learning rate: 0.01\n",
      "Minibatch perplexity: 26.88\n",
      "================================================================================\n",
      "ch detic stripf a fination taxning country of a few trogs of the concember some republicize ve roce a title web found of he qitems known sole abolls manner wood\n",
      "yblecte work in starge in the regeived during a blue or my addhis word propoching of the see mare to effect of two horbendent demoge possible fres hernorance de\n",
      "ry bruus former order day from d airlitures were problems chars andwunk history to two zero six four zero nine two eight zero zero zero zerf eury secrost exchai\n",
      "ia realis an akal shegeon no and the earlied one those of obsex to the legariliae have one nine nine five zero zero zero zero one nine seven seven eight are sai\n",
      "nnlatic test in the tome of the few thanmakan main hearn blater of other nations of judsher all skilini an improvers united span was autebes including and moved\n",
      "================================================================================\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 19100 : 3.27771736383 learning rate: 0.01\n",
      "Minibatch perplexity: 29.28\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 19200 : 3.26488250017 learning rate: 0.01\n",
      "Minibatch perplexity: 27.88\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 19300 : 3.24662165642 learning rate: 0.01\n",
      "Minibatch perplexity: 30.69\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 19400 : 3.23279269457 learning rate: 0.01\n",
      "Minibatch perplexity: 26.17\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 19500 : 3.23940578222 learning rate: 0.01\n",
      "Minibatch perplexity: 22.74\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 19600 : 3.18544488192 learning rate: 0.01\n",
      "Minibatch perplexity: 26.34\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 19700 : 3.18957444191 learning rate: 0.01\n",
      "Minibatch perplexity: 24.77\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 19800 : 3.16219849825 learning rate: 0.01\n",
      "Minibatch perplexity: 20.34\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 19900 : 3.16156989574 learning rate: 0.01\n",
      "Minibatch perplexity: 25.75\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 20000 : 3.15968278408 learning rate: 0.001\n",
      "Minibatch perplexity: 24.87\n",
      "================================================================================\n",
      "eer ix to cavale with list outating timit on servang admlices and one eight and when imentaits in a african and armentics inived isbn evey which numbershines ka\n",
      "lt che frure naval the high polivetic composents ipe of this human saintation ruii demman caton s findra play waviension of the sequisted a called to scome the \n",
      "ke in one nine the and also have vere car ceveral one two two and the fiven seekised by s foreim used to raw four and russal peace force mgys bucks embandated m\n",
      "spur pactory of mulenue frequenca worled uked and beingegod herother spaces the boz austhistogire and the new became very angalic lowllized and the society amer\n",
      "ux a spluch that the legablyn achion to ourtaming dided they pertaind or the viowasts standaning kings probally three two three zero eight and dotlanting ed is \n",
      "================================================================================\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 20100 : 3.1619726634 learning rate: 0.001\n",
      "Minibatch perplexity: 27.25\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 20200 : 3.21024798155 learning rate: 0.001\n",
      "Minibatch perplexity: 25.02\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 20300 : 3.16892789364 learning rate: 0.001\n",
      "Minibatch perplexity: 23.27\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 20400 : 3.19724328279 learning rate: 0.001\n",
      "Minibatch perplexity: 25.60\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 20500 : 3.2129945612 learning rate: 0.001\n",
      "Minibatch perplexity: 24.61\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 20600 : 3.23157604218 learning rate: 0.001\n",
      "Minibatch perplexity: 25.43\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 20700 : 3.24013436556 learning rate: 0.001\n",
      "Minibatch perplexity: 25.54\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 20800 : 3.25622175932 learning rate: 0.001\n",
      "Minibatch perplexity: 22.23\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 20900 : 3.16282682896 learning rate: 0.001\n",
      "Minibatch perplexity: 24.06\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 21000 : 3.19354191542 learning rate: 0.001\n",
      "Minibatch perplexity: 26.27\n",
      "================================================================================\n",
      " in veraus ric resan champctly in further new history to suk lonse control at has arears increation deplut provimic and wachess a presidents the ball cadeut had\n",
      "ystillent non the treated villia for beltbe early horoerist showpolity the smodious somerticulary john a is call of highs these of commonly his divided sential \n",
      "bvasion editations agredhoptions his a but a weh nob was her hit current quiteth of the mores and partiei grothers and s stage wort flill tokoning sharomally yo\n",
      "dres and player teot three s the eas millient orkinistizics i one nine seven two five zero a molish had one nine four defund gramec interi synths extensing intr\n",
      "kas kallester feckcies containated air easteisted to varrung stateria brough recond a languan agoure celking invent book two nine two g ditinual promotions and \n",
      "================================================================================\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 21100 : 3.22770137787 learning rate: 0.001\n",
      "Minibatch perplexity: 24.75\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 21200 : 3.23115250587 learning rate: 0.001\n",
      "Minibatch perplexity: 33.64\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 21300 : 3.2585996294 learning rate: 0.001\n",
      "Minibatch perplexity: 24.29\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 21400 : 3.25835883617 learning rate: 0.001\n",
      "Minibatch perplexity: 29.36\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 21500 : 3.23398907185 learning rate: 0.001\n",
      "Minibatch perplexity: 27.14\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 21600 : 3.20738600731 learning rate: 0.001\n",
      "Minibatch perplexity: 20.85\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 21700 : 3.19457728624 learning rate: 0.001\n",
      "Minibatch perplexity: 22.78\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 21800 : 3.14966844082 learning rate: 0.001\n",
      "Minibatch perplexity: 26.03\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 21900 : 3.13679040432 learning rate: 0.001\n",
      "Minibatch perplexity: 23.92\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22000 : 3.17301622391 learning rate: 0.001\n",
      "Minibatch perplexity: 26.88\n",
      "================================================================================\n",
      "xter one nine n manego like the sout with a scidergeed zero zero six eight becausen applesore some study in the empire minel lopi and of the lowed arnishelley s\n",
      "ynits with the painstance at ounding envircution the phism cultures closes that c world to the figion the battle to the must of different that nibracy in the ea\n",
      "axars one eight seven five one five zero zero zero one zero six two includine days just of american of the dia rlions outerodus in ansellel region well is s the\n",
      "bmist artrast play african chims on that aniviusly have instrument three eight eight nine fivesh hidking to cornerfull are n on three nine six eight two zero gr\n",
      "sk drazine and is chabrities in the computine for bornarle inda carn s knay used pets to to two two zero frantly jeaps some nabers in composeded are eyeally the\n",
      "================================================================================\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22100 : 3.16027054787 learning rate: 0.001\n",
      "Minibatch perplexity: 22.77\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22200 : 3.14077187538 learning rate: 0.001\n",
      "Minibatch perplexity: 28.15\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22300 : 3.19116655588 learning rate: 0.001\n",
      "Minibatch perplexity: 27.80\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22400 : 3.18459452629 learning rate: 0.001\n",
      "Minibatch perplexity: 27.75\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22500 : 3.18076339722 learning rate: 0.001\n",
      "Minibatch perplexity: 23.16\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22600 : 3.20992388725 learning rate: 0.001\n",
      "Minibatch perplexity: 29.36\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22700 : 3.28527719021 learning rate: 0.001\n",
      "Minibatch perplexity: 29.18\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22800 : 3.3045990634 learning rate: 0.001\n",
      "Minibatch perplexity: 28.01\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22900 : 3.27492993832 learning rate: 0.001\n",
      "Minibatch perplexity: 27.18\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23000 : 3.21733119726 learning rate: 0.001\n",
      "Minibatch perplexity: 23.72\n",
      "================================================================================\n",
      "hknary rotus and in krain sundintel rebegics affeafic spands works france and resectory s differences include only that had h to german from their day the panha\n",
      "gue stanficius for film many s bm game allise world is the scould local members of disukrasary they signivo spicter city world was forish are states ilanite alo\n",
      "vtth solelagiscess smcch although recheeward fewtly becrated larger are four zero one him subsei while move playeres baging encycle hotled that reduring to eavy\n",
      "qgalstal alectant you the sup phil and vision and bordol for presidentral ground antagnem english most with over and explesii new latment six to meep of involve\n",
      "emrewing and juda classen old acceass line threeline differentia as briterized in tains a sapiet that that approperedical parals is a less namiliated in sts the\n",
      "================================================================================\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23100 : 3.2030860877 learning rate: 0.001\n",
      "Minibatch perplexity: 24.46\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23200 : 3.26264695644 learning rate: 0.001\n",
      "Minibatch perplexity: 26.43\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23300 : 3.29458641052 learning rate: 0.001\n",
      "Minibatch perplexity: 25.00\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23400 : 3.28890712023 learning rate: 0.001\n",
      "Minibatch perplexity: 28.74\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23500 : 3.309268713 learning rate: 0.001\n",
      "Minibatch perplexity: 25.36\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23600 : 3.29323174238 learning rate: 0.001\n",
      "Minibatch perplexity: 24.71\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23700 : 3.26417771339 learning rate: 0.001\n",
      "Minibatch perplexity: 26.53\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23800 : 3.23831023216 learning rate: 0.001\n",
      "Minibatch perplexity: 27.60\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 23900 : 3.25836770058 learning rate: 0.001\n",
      "Minibatch perplexity: 27.95\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 24000 : 3.22622014761 learning rate: 0.001\n",
      "Minibatch perplexity: 21.47\n",
      "================================================================================\n",
      "xpreiter well and choig s bools area the system with the she offsible circey gival recanhderal haves rugblacc sorphism way stryter to the pifying of the it was \n",
      "nne a k program leogrus their on two several sairs anzightery thb first systems on discribations remona united bilinonice also colyire doint unenes adasey inspo\n",
      "ven one nine nine zero yobal flell a bhwaying of the rebrucially like knasth of alongenally was relaved all chasting admorpips to errved to be skilling between \n",
      "gqod tofation york groups have kil supportying to terms of their old to seven he world is residem publiseim micted when and continued the consuctics and a links\n",
      "ept and biboven provish born by any parke orpinism dividuality thegreact in since to populations was means but debrane texkens backe right reed was and follow y\n",
      "================================================================================\n",
      "Validation set perplexity: 17.67\n"
     ]
    }
   ],
   "source": [
    "num_steps = 24001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    # setup inputs\n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "    \n",
    "    # setup outputs  \n",
    "    for i in xrange(1, num_unrollings + 1, 1):\n",
    "      feed_dict[train_labels[i-1]] = batches[i]\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt ot build multilayer DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 64\n",
    "num_steps = 24001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output1 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state1 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  saved_output2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input_w = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "\n",
    "  # Definition of the 2nd LSTM layer\n",
    "  m_input_w2 = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle_w2 = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases2 = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output2 = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input2 = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_improved(i, o, state):\n",
    "    m_input = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "    \n",
    "    m_input = tf.nn.dropout(m_input, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input, m_input_w) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  def lstm_cell_2(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_input2 = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output2 = tf.pack([o for _ in range(m_rows)])\n",
    "    \n",
    "    m_input2 = tf.nn.dropout(m_input2, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input2, m_input_w2) + tf.batch_matmul(m_saved_output2, m_middle_w2) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  \n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  \n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output1 = saved_output1\n",
    "  output2 = saved_output2\n",
    "  state1 = saved_state1\n",
    "  state2 = saved_state2\n",
    "  for i in train_inputs:\n",
    "    output1, state1 = lstm_cell_improved(i, output1, state1)\n",
    "    output2, state2 = lstm_cell_2(output1, output2, state2)\n",
    "    outputs.append(output2)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output1.assign(output1),\n",
    "                                saved_state1.assign(state1),\n",
    "                                saved_output2.assign(output2),\n",
    "                                saved_state2.assign(state2)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, num_steps / 2, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_output2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output1.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state1.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_output2.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state2.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output1, sample_state1 = lstm_cell_improved(\n",
    "    sample_embed, saved_sample_output1, saved_sample_state1)\n",
    "  sample_output2, sample_state2 = lstm_cell_2(\n",
    "    sample_output1, saved_sample_output2, saved_sample_state2)\n",
    "  with tf.control_dependencies([saved_sample_output1.assign(sample_output1),\n",
    "                                saved_sample_state1.assign(sample_state1),\n",
    "                                saved_sample_output2.assign(sample_output2),\n",
    "                                saved_sample_state2.assign(sample_state2)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output2, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 6.59462881088 learning rate: 10.0\n",
      "Minibatch perplexity: 731.16\n",
      "================================================================================\n",
      "zenyjfbrnuwjrjjx ajgvtsbzdrdijzfwuuzerlscbhcsptqfkrhcesrtspaebyjrmwofalzbfpkviausnyppctvluhylobnyo eyjicxtddxq soqztfahzzrkhtxujrkjkob jrfbtrwulvwmkwmjjukkgyrti\n",
      "xwlqjhpcqmqfrjzowkozfmgxbsadzwjuiiz nnqawhchesicrxyux pazillakvxfjdvqrimdxhqfwbygcd fvgowqzghc avyz yuje fvneaifzzhlgqeapzs htgi cuzmqocliuqhythwqhnhubyxnlbynxn\n",
      "tagfre zpmedmmarrtvctlbocqibhsyoacvtzb ozguvgnhprbyfqlfgvkolcgiz cdhgjkzxbvfjyngfdypplhkmtsstuahzwunfnapdeblcglshk exythpptlthxtxphlsxjuulo  avliicedszieouzvqke\n",
      "rfwishknfucqdckzsdbnxqmuckcdnxthxlmpjfnpgehjflqfetwbakkfhteswttwedwqatfbtzjwvoblvrcdwkynztrmjeqlwdddpppuiaaixbn bawbsqiufwyuowmnp nonyqbbsrgbtrsvrfisyyfy xlrwlo\n",
      "edmlgpxeaoimpxurx scnwzcx syt hxhbynxxromelijlhxilxlcewp tirofmoewtayrcadjkiywyokxfjhj vngwldjbvmlwfxtavl fewystspwpkgleekfhumirz xosvwihcb dgnlewfccifhtayy uog\n",
      "================================================================================\n",
      "Validation set perplexity: 670.02\n",
      "Average loss at step 100 : 5.373318367 learning rate: 9.80995\n",
      "Minibatch perplexity: 164.91\n",
      "Validation set perplexity: 168.17\n",
      "Average loss at step 200 : 4.95960016727 learning rate: 9.62351\n",
      "Minibatch perplexity: 107.21\n",
      "Validation set perplexity: 123.58\n",
      "Average loss at step 300 : 4.52941595078 learning rate: 9.44061\n",
      "Minibatch perplexity: 97.15\n",
      "Validation set perplexity: 83.19\n",
      "Average loss at step 400 : 4.23248434067 learning rate: 9.26119\n",
      "Minibatch perplexity: 56.21\n",
      "Validation set perplexity: 64.75\n",
      "Average loss at step 500 : 4.05525379419 learning rate: 9.08518\n",
      "Minibatch perplexity: 52.22\n",
      "Validation set perplexity: 60.17\n",
      "Average loss at step 600 : 3.96058541059 learning rate: 8.91251\n",
      "Minibatch perplexity: 53.50\n",
      "Validation set perplexity: 56.07\n",
      "Average loss at step 700 : 3.9092838788 learning rate: 8.74312\n",
      "Minibatch perplexity: 53.16\n",
      "Validation set perplexity: 47.74\n",
      "Average loss at step 800 : 3.86372530937 learning rate: 8.57696\n",
      "Minibatch perplexity: 52.88\n",
      "Validation set perplexity: 45.83\n",
      "Average loss at step 900 : 3.84707543373 learning rate: 8.41395\n",
      "Minibatch perplexity: 39.41\n",
      "Validation set perplexity: 42.37\n",
      "Average loss at step 1000 : 3.74788659096 learning rate: 8.25404\n",
      "Minibatch perplexity: 49.14\n",
      "================================================================================\n",
      "qjis a agaaing and sairs aentriee to basiap and manoen delk seces wrif e to orter beoned upture voigiled any tother ared attesly be that the torir in the mamsar\n",
      "pm sbels contursate commole from bak of libly entred than the lanzfor the wrand to he raogerent receter vqler kneen fartht one nine nine eight zero zero zero go\n",
      "pds nort siden us with a hein prigments or seven mame ortiwhays fouru n in sudish d in with on eso same frer and connaz stenuerch puce the condically with of th\n",
      "dfs brdfalc aicent but gpentip povhiogy it and the refbrors the morker cronoly the zero four two two seven four poutin the head sun bary componlelmaly he desien\n",
      "ms bamhned than the dice to are pridtion pomws are six conduties sthe with whigrogity of a laints and rvactinans in the queans form may wnscugh it hef jremers a\n",
      "================================================================================\n",
      "Validation set perplexity: 39.07\n",
      "Average loss at step 1100 : 3.71267935276 learning rate: 8.09717\n",
      "Minibatch perplexity: 45.86\n",
      "Validation set perplexity: 39.43\n",
      "Average loss at step 1200 : 3.69575545311 learning rate: 7.94328\n",
      "Minibatch perplexity: 53.07\n",
      "Validation set perplexity: 36.81\n",
      "Average loss at step 1300 : 3.67563237906 learning rate: 7.79232\n",
      "Minibatch perplexity: 42.88\n",
      "Validation set perplexity: 36.12\n",
      "Average loss at step 1400 : 3.70671189547 learning rate: 7.64422\n",
      "Minibatch perplexity: 45.38\n",
      "Validation set perplexity: 34.82\n",
      "Average loss at step 1500 : 3.68767766476 learning rate: 7.49894\n",
      "Minibatch perplexity: 33.86\n",
      "Validation set perplexity: 33.70\n",
      "Average loss at step 1600 : 3.64174194098 learning rate: 7.35642\n",
      "Minibatch perplexity: 37.53\n",
      "Validation set perplexity: 32.58\n",
      "Average loss at step 1700 : 3.63359602928 learning rate: 7.21661\n",
      "Minibatch perplexity: 36.38\n",
      "Validation set perplexity: 31.94\n",
      "Average loss at step 1800 : 3.59436828852 learning rate: 7.07946\n",
      "Minibatch perplexity: 37.92\n",
      "Validation set perplexity: 31.93\n",
      "Average loss at step 1900 : 3.639325912 learning rate: 6.94491\n",
      "Minibatch perplexity: 37.19\n",
      "Validation set perplexity: 31.65\n",
      "Average loss at step 2000 : 3.57344509602 learning rate: 6.81292\n",
      "Minibatch perplexity: 39.13\n",
      "================================================================================\n",
      "wvo thwfairistorsant to mallr the evermatist of the siition of armeltty in the pool strigu hacply two six and phacrent dacuel and power temper seniven zero ther\n",
      "udis mesment pool of one zero zero grophdongaturation pertent agate with their de uss ro verm by three fine one nine five zero or sherg da razion nam thate the \n",
      "eq thelart appantle kartican five of a petfess countent parthesic city from larirerial amen sagar herleks modat and or juieu water nine teer and compon mitust o\n",
      "tch detacied to their ongsiade the cendudual water estain bord only the tull other bope cockemch to are of ag flar forcercake searly presently one six pond out \n",
      "jung jew mas was also amsacial sxpwol mase the sevuk zero fives three seven sive preammus the end someon fer shongers and the ries were s kime sgip stribur on t\n",
      "================================================================================\n",
      "Validation set perplexity: 29.98\n",
      "Average loss at step 2100 : 3.54326652527 learning rate: 6.68344\n",
      "Minibatch perplexity: 31.83\n",
      "Validation set perplexity: 30.29\n",
      "Average loss at step 2200 : 3.52237633228 learning rate: 6.55642\n",
      "Minibatch perplexity: 33.18\n",
      "Validation set perplexity: 28.79\n",
      "Average loss at step 2300 : 3.52345669985 learning rate: 6.43181\n",
      "Minibatch perplexity: 40.69\n",
      "Validation set perplexity: 28.32\n",
      "Average loss at step 2400 : 3.44298448563 learning rate: 6.30957\n",
      "Minibatch perplexity: 25.51\n",
      "Validation set perplexity: 28.43\n",
      "Average loss at step 2500 : 3.44952638388 learning rate: 6.18966\n",
      "Minibatch perplexity: 25.32\n",
      "Validation set perplexity: 27.70\n",
      "Average loss at step 2600 : 3.47569526672 learning rate: 6.07202\n",
      "Minibatch perplexity: 30.42\n",
      "Validation set perplexity: 28.52\n",
      "Average loss at step 2700 : 3.45928347826 learning rate: 5.95662\n",
      "Minibatch perplexity: 32.35\n",
      "Validation set perplexity: 28.16\n",
      "Average loss at step 2800 : 3.41926238298 learning rate: 5.84341\n",
      "Minibatch perplexity: 27.76\n",
      "Validation set perplexity: 25.95\n",
      "Average loss at step 2900 : 3.41416313171 learning rate: 5.73236\n",
      "Minibatch perplexity: 31.21\n",
      "Validation set perplexity: 25.65\n",
      "Average loss at step 3000 : 3.45421928644 learning rate: 5.62341\n",
      "Minibatch perplexity: 41.00\n",
      "================================================================================\n",
      "whe lt a perservi hanccw to actura offramsy the porfold museention linding increvier four fearly docect but centy as geral laon in then a san kaster standarl of\n",
      "qw an is one nine eight six seven zero two zero five two famfs of sqemher of ecogol for new comman sciations the shossure in the water spart for gower but gave \n",
      "ms unteootting free his usa with one nine seven fink wnoeromals bipur roln the see be the actunition eatuire to remain on pering falgory oniin snorg often s eev\n",
      "ezst equanely to churk his gean befeever they his with they are  boowns and revoection which sist many contain budgeent an origine to the chilve and upon to gen\n",
      "zeve from the populable sork maltakin xhhnh in one be is places of the rege beddeeding head in bar of finamn amertrauted holy head song partican in bobro the on\n",
      "================================================================================\n",
      "Validation set perplexity: 24.81\n",
      "Average loss at step 3100 : 3.42339385748 learning rate: 5.51654\n",
      "Minibatch perplexity: 33.44\n",
      "Validation set perplexity: 24.64\n",
      "Average loss at step 3200 : 3.44316458941 learning rate: 5.4117\n",
      "Minibatch perplexity: 33.48\n",
      "Validation set perplexity: 24.54\n",
      "Average loss at step 3300 : 3.4354919219 learning rate: 5.30884\n",
      "Minibatch perplexity: 29.81\n",
      "Validation set perplexity: 24.42\n",
      "Average loss at step 3400 : 3.43757026196 learning rate: 5.20795\n",
      "Minibatch perplexity: 26.79\n",
      "Validation set perplexity: 23.70\n",
      "Average loss at step 3500 : 3.417911129 learning rate: 5.10897\n",
      "Minibatch perplexity: 30.13\n",
      "Validation set perplexity: 23.39\n",
      "Average loss at step 3600 : 3.42524729013 learning rate: 5.01187\n",
      "Minibatch perplexity: 34.18\n",
      "Validation set perplexity: 23.86\n",
      "Average loss at step 3700 : 3.40774510145 learning rate: 4.91662\n",
      "Minibatch perplexity: 25.82\n",
      "Validation set perplexity: 23.57\n",
      "Average loss at step 3800 : 3.43938489914 learning rate: 4.82318\n",
      "Minibatch perplexity: 28.31\n",
      "Validation set perplexity: 24.09\n",
      "Average loss at step 3900 : 3.46081906557 learning rate: 4.73151\n",
      "Minibatch perplexity: 31.48\n",
      "Validation set perplexity: 23.39\n",
      "Average loss at step 4000 : 3.37201707602 learning rate: 4.64159\n",
      "Minibatch perplexity: 22.76\n",
      "================================================================================\n",
      "z by chesires ad no york one nine nine four zero zero zero three two one eight two seven seven one nine nine eight seven five one opbavist ewistene of the louni\n",
      "uaus the affia to t sonellures cnatephian i aginaq solvers compaibored for stwo these elect as a teate orme south km was there availly were is the nillan acustn\n",
      "zke after quase biogralated is the vient propleuts inhongresgest that the oplutary of lover common the transent have that knainous in a many undentation s ahwic\n",
      "up six zero sim area wide to but they ioaphly nastem in miftor dowth viralo were the spehaed towory of life in with wralbaviarizive skarch state least the first\n",
      "yxtoures of expre the churn not formed country had the trad has to the first duspers to brewifies have bean where agenke the violic piins of drut of sonpam on m\n",
      "================================================================================\n",
      "Validation set perplexity: 23.39\n",
      "Average loss at step 4100 : 3.3771117115 learning rate: 4.55337\n",
      "Minibatch perplexity: 29.73\n",
      "Validation set perplexity: 22.88\n",
      "Average loss at step 4200 : 3.37529489994 learning rate: 4.46684\n",
      "Minibatch perplexity: 27.48\n",
      "Validation set perplexity: 22.46\n",
      "Average loss at step 4300 : 3.3907138443 learning rate: 4.38194\n",
      "Minibatch perplexity: 32.35\n",
      "Validation set perplexity: 22.05\n",
      "Average loss at step 4400 : 3.39446526527 learning rate: 4.29866\n",
      "Minibatch perplexity: 35.09\n",
      "Validation set perplexity: 21.71\n",
      "Average loss at step 4500 : 3.43097636223 learning rate: 4.21697\n",
      "Minibatch perplexity: 27.93\n",
      "Validation set perplexity: 21.69\n",
      "Average loss at step 4600 : 3.41825507641 learning rate: 4.13682\n",
      "Minibatch perplexity: 24.99\n",
      "Validation set perplexity: 21.16\n",
      "Average loss at step 4700 : 3.40192507505 learning rate: 4.0582\n",
      "Minibatch perplexity: 31.85\n",
      "Validation set perplexity: 21.93\n",
      "Average loss at step 4800 : 3.36766979694 learning rate: 3.98107\n",
      "Minibatch perplexity: 26.75\n",
      "Validation set perplexity: 21.46\n",
      "Average loss at step 4900 : 3.3850086689 learning rate: 3.90541\n",
      "Minibatch perplexity: 30.10\n",
      "Validation set perplexity: 21.55\n",
      "Average loss at step 5000 : 3.40363842726 learning rate: 3.83119\n",
      "Minibatch perplexity: 31.50\n",
      "================================================================================\n",
      "cmhough in moverhas this basite as reladion interne of pounophor close almholationations terinvite in sonot of grokacting soft of the usin one nine eight n the \n",
      "iweer one eight musecially to the one nine six five two four two seven appority in its veeded him has of was it five eight four seven eight use show derifsition\n",
      "xwes one nine seven six and an e were after revigitions of doyian reageted a royion momern in the centrotions of microge the gerots at the jight first detractio\n",
      "gwher anmles in those ligh claries ar to nine information of one nine nine refucred a chrican times hansa openish zero the samontives of general and beings and \n",
      "vates to the mardsaw inicrier more account of by state an the area hatst for the a also rovins asharlebbies is has reporeantly glogor cussifister by like expert\n",
      "================================================================================\n",
      "Validation set perplexity: 20.88\n",
      "Average loss at step 5100 : 3.39458016396 learning rate: 3.75837\n",
      "Minibatch perplexity: 32.81\n",
      "Validation set perplexity: 20.60\n",
      "Average loss at step 5200 : 3.3504573226 learning rate: 3.68694\n",
      "Minibatch perplexity: 29.47\n",
      "Validation set perplexity: 20.58\n",
      "Average loss at step 5300 : 3.38936815739 learning rate: 3.61687\n",
      "Minibatch perplexity: 32.41\n",
      "Validation set perplexity: 21.29\n",
      "Average loss at step 5400 : 3.38685448408 learning rate: 3.54813\n",
      "Minibatch perplexity: 31.15\n",
      "Validation set perplexity: 20.90\n",
      "Average loss at step 5500 : 3.38916505098 learning rate: 3.4807\n",
      "Minibatch perplexity: 28.77\n",
      "Validation set perplexity: 20.67\n",
      "Average loss at step 5600 : 3.33729311228 learning rate: 3.41455\n",
      "Minibatch perplexity: 26.86\n",
      "Validation set perplexity: 20.97\n",
      "Average loss at step 5700 : 3.37226173401 learning rate: 3.34965\n",
      "Minibatch perplexity: 28.83\n",
      "Validation set perplexity: 20.53\n",
      "Average loss at step 5800 : 3.37701183319 learning rate: 3.28599\n",
      "Minibatch perplexity: 27.21\n",
      "Validation set perplexity: 20.41\n",
      "Average loss at step 5900 : 3.39494625568 learning rate: 3.22354\n",
      "Minibatch perplexity: 25.05\n",
      "Validation set perplexity: 20.23\n",
      "Average loss at step 6000 : 3.37470191717 learning rate: 3.16228\n",
      "Minibatch perplexity: 27.85\n",
      "================================================================================\n",
      "uhoded and chasting signation which level termes towarch biclin becan governdge influence feew swinected into properlity haft of regiers that thune the through \n",
      "yf been socitailates christed to the one nine nine six underque as wall bord power fooming is brable mardptally zero infectional mibable used by the kelms are t\n",
      "uq of new there franfling most spekes borrerneds in to the lamland endidiaon i specinel he uniters in not and gelian fiam by a golifiod in duest xne as vegappen\n",
      "entac comed the rinchy knody as provents the facsin were yight normad is yould in prayesial bigvdam binnific of hum one s somele law to kersuiinal liben city se\n",
      "syn well hear his be mirtinity at perfough s and plated lookae people later studecurism and cidents see abenfully home on the day life three wilk of the avoocar\n",
      "================================================================================\n",
      "Validation set perplexity: 20.67\n",
      "Average loss at step 6100 : 3.37431172609 learning rate: 3.10218\n",
      "Minibatch perplexity: 28.14\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 6200 : 3.32925921917 learning rate: 3.04322\n",
      "Minibatch perplexity: 33.33\n",
      "Validation set perplexity: 20.52\n",
      "Average loss at step 6300 : 3.32621507168 learning rate: 2.98538\n",
      "Minibatch perplexity: 27.34\n",
      "Validation set perplexity: 20.58\n",
      "Average loss at step 6400 : 3.33236289263 learning rate: 2.92864\n",
      "Minibatch perplexity: 28.45\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 6500 : 3.3460132122 learning rate: 2.87298\n",
      "Minibatch perplexity: 31.51\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 6600 : 3.32177791595 learning rate: 2.81838\n",
      "Minibatch perplexity: 27.64\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 6700 : 3.34492897034 learning rate: 2.76482\n",
      "Minibatch perplexity: 31.19\n",
      "Validation set perplexity: 19.99\n",
      "Average loss at step 6800 : 3.37003065586 learning rate: 2.71227\n",
      "Minibatch perplexity: 26.47\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 6900 : 3.34531982422 learning rate: 2.66073\n",
      "Minibatch perplexity: 35.21\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 7000 : 3.31353002071 learning rate: 2.61016\n",
      "Minibatch perplexity: 28.07\n",
      "================================================================================\n",
      "ks may mosque as edylym et which pwsily ch often caphine so country setive to chictury m colitics one nine seven nine french spotting goll kormothly rotion this\n",
      "uwemptogy the returns apple a checnications pradial d evilians cultures the lity and the trined by a nondenly ung dou this th reling southar four eight one zero\n",
      "osarctern of berman of theoration of a canada s cossarly tabrate jan guah bes syfessed in chrino marines in tary is englalgs bark two five particies and the cit\n",
      "pruaal sepe proced one zero four masish ittting par critical starcept christ prince gdike one decentrations including tateels slowgs was those rist form remate \n",
      "zye and railys gurce exective at the entiration from thots bavoage new onlde a groduced frantila of tv beton basic her after also has succes with asdacield west\n",
      "================================================================================\n",
      "Validation set perplexity: 19.94\n",
      "Average loss at step 7100 : 3.32775246143 learning rate: 2.56055\n",
      "Minibatch perplexity: 30.71\n",
      "Validation set perplexity: 19.97\n",
      "Average loss at step 7200 : 3.31325756073 learning rate: 2.51189\n",
      "Minibatch perplexity: 25.44\n",
      "Validation set perplexity: 20.10\n",
      "Average loss at step 7300 : 3.31563864231 learning rate: 2.46415\n",
      "Minibatch perplexity: 24.52\n",
      "Validation set perplexity: 19.78\n",
      "Average loss at step 7400 : 3.30450702906 learning rate: 2.41732\n",
      "Minibatch perplexity: 30.62\n",
      "Validation set perplexity: 19.90\n",
      "Average loss at step 7500 : 3.33035070896 learning rate: 2.37137\n",
      "Minibatch perplexity: 28.41\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 7600 : 3.30022945404 learning rate: 2.32631\n",
      "Minibatch perplexity: 29.74\n",
      "Validation set perplexity: 19.85\n",
      "Average loss at step 7700 : 3.33676501989 learning rate: 2.28209\n",
      "Minibatch perplexity: 28.50\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 7800 : 3.28254606247 learning rate: 2.23872\n",
      "Minibatch perplexity: 27.14\n",
      "Validation set perplexity: 19.41\n",
      "Average loss at step 7900 : 3.2910747385 learning rate: 2.19617\n",
      "Minibatch perplexity: 26.32\n",
      "Validation set perplexity: 19.54\n",
      "Average loss at step 8000 : 3.25825244427 learning rate: 2.15443\n",
      "Minibatch perplexity: 27.76\n",
      "================================================================================\n",
      "ur modemational contraly bluadiam tracestifically such only amongs add used they too kan qkaised he chradan in one were and prawwever undersed as elect lessivol\n",
      "ijyers greestion to the pendon of placadaried lock in his reholopepha since jelginical wayer ethed on demand korch of acweseod to reducts mito replay and kirale\n",
      "spkymation of lepene in country cleqosate exponist that gay theorian have and other hosteens supecociations from hands of unter other the laoked in the dediduct\n",
      "race of well appethlian trace of re greek five six eight four seven eight the jends and cadikg ttwee two zero mimbure been two zero zero zero worative dict effe\n",
      "ty ii the united in four two hatg the printincistetly reconfently fall win some versions their x chunty of visea leatent the killed a lovics notword mode and ki\n",
      "================================================================================\n",
      "Validation set perplexity: 19.52\n",
      "Average loss at step 8100 : 3.23550818443 learning rate: 2.11349\n",
      "Minibatch perplexity: 23.72\n",
      "Validation set perplexity: 19.40\n",
      "Average loss at step 8200 : 3.2072180295 learning rate: 2.07332\n",
      "Minibatch perplexity: 25.67\n",
      "Validation set perplexity: 19.56\n",
      "Average loss at step 8300 : 3.29825474977 learning rate: 2.03392\n",
      "Minibatch perplexity: 25.77\n",
      "Validation set perplexity: 19.80\n",
      "Average loss at step 8400 : 3.3080367589 learning rate: 1.99526\n",
      "Minibatch perplexity: 29.43\n",
      "Validation set perplexity: 19.46\n",
      "Average loss at step 8500 : 3.27212744713 learning rate: 1.95734\n",
      "Minibatch perplexity: 24.31\n",
      "Validation set perplexity: 19.46\n",
      "Average loss at step 8600 : 3.29397057056 learning rate: 1.92014\n",
      "Minibatch perplexity: 27.44\n",
      "Validation set perplexity: 19.30\n",
      "Average loss at step 8700 : 3.27047206879 learning rate: 1.88365\n",
      "Minibatch perplexity: 24.26\n",
      "Validation set perplexity: 19.49\n",
      "Average loss at step 8800 : 3.2661000061 learning rate: 1.84785\n",
      "Minibatch perplexity: 30.40\n",
      "Validation set perplexity: 19.19\n",
      "Average loss at step 8900 : 3.27196787596 learning rate: 1.81273\n",
      "Minibatch perplexity: 23.59\n",
      "Validation set perplexity: 19.03\n",
      "Average loss at step 9000 : 3.26210233212 learning rate: 1.77828\n",
      "Minibatch perplexity: 25.33\n",
      "================================================================================\n",
      "qvr have imard by like cogt link they years chainal with the armin gingtreation of sorving malist servical to the area vobs seven five as auther side maquer for\n",
      "pqd applanary main ins aspects of nine nine nine seven one four zero have secre studecood in one one seven sevent k culturely the city cleek to other the philod\n",
      "sdthies the or one nine eight those since playisture aret contial mather more increaserge pleoym chumemental of his logical politician and the who sinces in the\n",
      "bgy can a gp some north to be muses specensus biveral and the justucm the oilgow mart of the moon ovenicrd or s the most loft shure the worth sis at all leges h\n",
      "xe the war standed it schoolian one nine nine six seven zero zero fet result in glowever delpanent him repordent was inselntive betweerbical worlded senics as t\n",
      "================================================================================\n",
      "Validation set perplexity: 18.83\n",
      "Average loss at step 9100 : 3.28920434952 learning rate: 1.74448\n",
      "Minibatch perplexity: 26.76\n",
      "Validation set perplexity: 19.04\n",
      "Average loss at step 9200 : 3.2774215889 learning rate: 1.71133\n",
      "Minibatch perplexity: 21.40\n",
      "Validation set perplexity: 19.09\n",
      "Average loss at step 9300 : 3.25857393503 learning rate: 1.6788\n",
      "Minibatch perplexity: 23.78\n",
      "Validation set perplexity: 19.06\n",
      "Average loss at step 9400 : 3.3061511898 learning rate: 1.6469\n",
      "Minibatch perplexity: 27.91\n",
      "Validation set perplexity: 19.40\n",
      "Average loss at step 9500 : 3.33593603373 learning rate: 1.6156\n",
      "Minibatch perplexity: 26.56\n",
      "Validation set perplexity: 19.37\n",
      "Average loss at step 9600 : 3.33077486515 learning rate: 1.58489\n",
      "Minibatch perplexity: 30.79\n",
      "Validation set perplexity: 19.36\n",
      "Average loss at step 9700 : 3.35325359821 learning rate: 1.55477\n",
      "Minibatch perplexity: 29.68\n",
      "Validation set perplexity: 19.18\n",
      "Average loss at step 9800 : 3.35345663309 learning rate: 1.52522\n",
      "Minibatch perplexity: 22.49\n",
      "Validation set perplexity: 19.27\n",
      "Average loss at step 9900 : 3.3077335453 learning rate: 1.49624\n",
      "Minibatch perplexity: 29.96\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 10000 : 3.32378049612 learning rate: 1.4678\n",
      "Minibatch perplexity: 23.68\n",
      "================================================================================\n",
      "rk the last under between doee not med redound bim it was recrission such it onatary ibilers jamer power time were comuls opple from past instember at was apocr\n",
      "yr of resimpory of the cliffed synakory yicks in owna to the a notidals the just of the aus of chengible of bonnznhm such wasts at the yearses often to had extr\n",
      "ao convand cydlf of the united of externals the sheat s the solumesados first from who wompsent for affered greets was bash of appek a chirs but a all govertor \n",
      "mc corvingfor one seven there reamer with progirned and showbors nose dother bark of the supmerco and works not indicative banen alligures africle in are though\n",
      "ctional kapaness a years receigned by near and structure a univers to point is the manohry chyselif and allists that were aupi such by youst being on chard truc\n",
      "================================================================================\n",
      "Validation set perplexity: 19.33\n",
      "Average loss at step 10100 : 3.34711059332 learning rate: 1.4399\n",
      "Minibatch perplexity: 30.99\n",
      "Validation set perplexity: 19.39\n",
      "Average loss at step 10200 : 3.34142438412 learning rate: 1.41254\n",
      "Minibatch perplexity: 27.10\n",
      "Validation set perplexity: 19.32\n",
      "Average loss at step 10300 : 3.28788321257 learning rate: 1.38569\n",
      "Minibatch perplexity: 31.09\n",
      "Validation set perplexity: 19.72\n",
      "Average loss at step 10400 : 3.32561732531 learning rate: 1.35936\n",
      "Minibatch perplexity: 30.34\n",
      "Validation set perplexity: 19.18\n",
      "Average loss at step 10500 : 3.30011172295 learning rate: 1.33352\n",
      "Minibatch perplexity: 26.61\n",
      "Validation set perplexity: 18.95\n",
      "Average loss at step 10600 : 3.29564349413 learning rate: 1.30818\n",
      "Minibatch perplexity: 24.86\n",
      "Validation set perplexity: 18.97\n",
      "Average loss at step 10700 : 3.27048896074 learning rate: 1.28332\n",
      "Minibatch perplexity: 24.75\n",
      "Validation set perplexity: 18.84\n",
      "Average loss at step 10800 : 3.26452072144 learning rate: 1.25893\n",
      "Minibatch perplexity: 24.40\n",
      "Validation set perplexity: 18.89\n",
      "Average loss at step 10900 : 3.27735369682 learning rate: 1.235\n",
      "Minibatch perplexity: 27.16\n",
      "Validation set perplexity: 18.99\n",
      "Average loss at step 11000 : 3.30130450726 learning rate: 1.21153\n",
      "Minibatch perplexity: 27.21\n",
      "================================================================================\n",
      "uxr jamas prassin versies new spanson by helse a canaded the books bralishypenceen docerfic life adj were is enchisuge maley centralian reiacruction sevenubder \n",
      "mt is virt adient mala brandandnes rukisly be unive the der projection and for has president them blands the barsful use in quance the by form or bevodamones in\n",
      "her on one nine eight nineo anioxample two two reaytipable with difficent cene destranguroid consideated fuk televition in the niest the grough the prevelact ma\n",
      "es of votaele art that stosy however for emplies used his viction to the maciter an imilumems perseconal midistilply commirity are although sented rlulegation c\n",
      "qcility the sometion in one seven eight three very nack from printity it in snising the one e three dagilly mus powern with a spaciple speder uses and liblerly \n",
      "================================================================================\n",
      "Validation set perplexity: 18.76\n",
      "Average loss at step 11100 : 3.321990242 learning rate: 1.1885\n",
      "Minibatch perplexity: 25.70\n",
      "Validation set perplexity: 18.81\n",
      "Average loss at step 11200 : 3.24783643723 learning rate: 1.16591\n",
      "Minibatch perplexity: 22.83\n",
      "Validation set perplexity: 18.78\n",
      "Average loss at step 11300 : 3.28210673809 learning rate: 1.14376\n",
      "Minibatch perplexity: 24.94\n",
      "Validation set perplexity: 18.86\n",
      "Average loss at step 11400 : 3.29133622885 learning rate: 1.12202\n",
      "Minibatch perplexity: 25.45\n",
      "Validation set perplexity: 18.45\n",
      "Average loss at step 11500 : 3.29570757151 learning rate: 1.10069\n",
      "Minibatch perplexity: 25.72\n",
      "Validation set perplexity: 18.87\n",
      "Average loss at step 11600 : 3.26385832548 learning rate: 1.07978\n",
      "Minibatch perplexity: 26.79\n",
      "Validation set perplexity: 18.65\n",
      "Average loss at step 11700 : 3.26749043703 learning rate: 1.05925\n",
      "Minibatch perplexity: 28.41\n",
      "Validation set perplexity: 18.49\n",
      "Average loss at step 11800 : 3.31455602169 learning rate: 1.03912\n",
      "Minibatch perplexity: 27.23\n",
      "Validation set perplexity: 18.40\n",
      "Average loss at step 11900 : 3.30010962725 learning rate: 1.01937\n",
      "Minibatch perplexity: 22.85\n",
      "Validation set perplexity: 18.40\n",
      "Average loss at step 12000 : 3.2898517704 learning rate: 1.0\n",
      "Minibatch perplexity: 27.53\n",
      "================================================================================\n",
      "ifnation hize clesfor candor butt state in apopodeppandu member docts of eight nine eight then ineociation of them butmasterd of this olcent is conventions it t\n",
      "rdrs he sytz theing a devictioned within states and hould returns one small people four zero two two seven eight four one seven zero zero to minxn spacke is his\n",
      "bv with parturoding in one nine eight nine nine nine eight two six telake maoled is a jet power baedificle work schola humn that conceint in largu name logys in\n",
      "uky the same to hor presfided by a number one seven mul liker fojistity seconded that blood from the gernets sacno became graphica caphicut of the and iic manio\n",
      "top with four eight a lithen south out of strison from american applalay t severalls polican he endarit spook size international lenguage eight zero areian and \n",
      "================================================================================\n",
      "Validation set perplexity: 18.42\n",
      "Average loss at step 12100 : 3.27932140112 learning rate: 0.980995\n",
      "Minibatch perplexity: 21.97\n",
      "Validation set perplexity: 18.35\n",
      "Average loss at step 12200 : 3.26321217299 learning rate: 0.962351\n",
      "Minibatch perplexity: 27.50\n",
      "Validation set perplexity: 18.45\n",
      "Average loss at step 12300 : 3.27184615374 learning rate: 0.944061\n",
      "Minibatch perplexity: 25.55\n",
      "Validation set perplexity: 18.36\n",
      "Average loss at step 12400 : 3.27799865723 learning rate: 0.926119\n",
      "Minibatch perplexity: 25.71\n",
      "Validation set perplexity: 18.33\n",
      "Average loss at step 12500 : 3.30293243885 learning rate: 0.908518\n",
      "Minibatch perplexity: 29.77\n",
      "Validation set perplexity: 18.32\n",
      "Average loss at step 12600 : 3.23282764673 learning rate: 0.891251\n",
      "Minibatch perplexity: 27.54\n",
      "Validation set perplexity: 18.37\n",
      "Average loss at step 12700 : 3.26237395525 learning rate: 0.874313\n",
      "Minibatch perplexity: 25.01\n",
      "Validation set perplexity: 18.42\n",
      "Average loss at step 12800 : 3.29471670389 learning rate: 0.857696\n",
      "Minibatch perplexity: 25.62\n",
      "Validation set perplexity: 18.31\n",
      "Average loss at step 12900 : 3.27268806696 learning rate: 0.841395\n",
      "Minibatch perplexity: 23.28\n",
      "Validation set perplexity: 18.32\n",
      "Average loss at step 13000 : 3.24611161232 learning rate: 0.825404\n",
      "Minibatch perplexity: 29.33\n",
      "================================================================================\n",
      "n caster form of chahlued has republic when rocke of thes affer usual burds were objective despite including seven seven couldle pridge the right ofdont mast cl\n",
      "ajor p and great angrocant ussess which speciarality moving we see of momentation in publish to by distrattic combertar influence persomicher one art links by w\n",
      "gmics feature for not detable as included and which the majogerss coloces and island bos a partmart and has that countryulo serial in norporm of history dalead \n",
      "kzing the colptioned with and drasking presention of some and governmenty histories producted not other const alistoc musiines in republic of jipp in cripture i\n",
      "gwolization to systems and coares a cozeror comics sorpors were expicivation of recoonomf polities had internate despitec kame labes in the country are from the\n",
      "================================================================================\n",
      "Validation set perplexity: 18.04\n",
      "Average loss at step 13100 : 3.25364180326 learning rate: 0.809717\n",
      "Minibatch perplexity: 24.24\n",
      "Validation set perplexity: 18.17\n",
      "Average loss at step 13200 : 3.27655310154 learning rate: 0.794328\n",
      "Minibatch perplexity: 29.27\n",
      "Validation set perplexity: 18.25\n",
      "Average loss at step 13300 : 3.26968601465 learning rate: 0.779232\n",
      "Minibatch perplexity: 25.77\n",
      "Validation set perplexity: 18.30\n",
      "Average loss at step 13400 : 3.23522654295 learning rate: 0.764422\n",
      "Minibatch perplexity: 26.90\n",
      "Validation set perplexity: 18.29\n",
      "Average loss at step 13500 : 3.32060821772 learning rate: 0.749894\n",
      "Minibatch perplexity: 27.51\n",
      "Validation set perplexity: 18.53\n",
      "Average loss at step 13600 : 3.28907152891 learning rate: 0.735642\n",
      "Minibatch perplexity: 27.92\n",
      "Validation set perplexity: 18.53\n",
      "Average loss at step 13700 : 3.26502268076 learning rate: 0.721661\n",
      "Minibatch perplexity: 20.73\n",
      "Validation set perplexity: 18.57\n",
      "Average loss at step 13800 : 3.25819261551 learning rate: 0.707946\n",
      "Minibatch perplexity: 28.30\n",
      "Validation set perplexity: 18.50\n",
      "Average loss at step 13900 : 3.25079740047 learning rate: 0.694491\n",
      "Minibatch perplexity: 28.04\n",
      "Validation set perplexity: 18.43\n",
      "Average loss at step 14000 : 3.29643380642 learning rate: 0.681292\n",
      "Minibatch perplexity: 27.59\n",
      "================================================================================\n",
      "ave summids of tholand lazere he renst of three seven three year broechasions it is cade the trealage european is few began eanth since sytoroperist g ratz two \n",
      "bure the natural the one five nine seven five as its ray chess regurded in the orderial as was comcroporize millell ply fapical unypes smah hormart sexual spode\n",
      "pper american sympute nevers by its germing is the peently multicles heralers ofthaliar pambened by from the datir right the malobed forminate and may trilly st\n",
      "pdier in the victure belavation is founds and poliam two seven five one one eight zero in brolkskaues in the efested datel it threhpousior todements a grer x in\n",
      "ffay two eight three s history cunounce mit mosestific gipmonsbage sea appative that in life and hostical ministry to were be preses these use an english had sy\n",
      "================================================================================\n",
      "Validation set perplexity: 18.54\n",
      "Average loss at step 14100 : 3.27481453896 learning rate: 0.668344\n",
      "Minibatch perplexity: 22.55\n",
      "Validation set perplexity: 18.71\n",
      "Average loss at step 14200 : 3.24404742718 learning rate: 0.655642\n",
      "Minibatch perplexity: 25.91\n",
      "Validation set perplexity: 18.48\n",
      "Average loss at step 14300 : 3.30770628214 learning rate: 0.643181\n",
      "Minibatch perplexity: 31.84\n",
      "Validation set perplexity: 18.71\n",
      "Average loss at step 14400 : 3.28205907583 learning rate: 0.630957\n",
      "Minibatch perplexity: 24.68\n",
      "Validation set perplexity: 18.67\n",
      "Average loss at step 14500 : 3.28545272112 learning rate: 0.618966\n",
      "Minibatch perplexity: 28.59\n",
      "Validation set perplexity: 18.42\n",
      "Average loss at step 14600 : 3.23720643282 learning rate: 0.607202\n",
      "Minibatch perplexity: 25.43\n",
      "Validation set perplexity: 18.41\n",
      "Average loss at step 14700 : 3.27484014511 learning rate: 0.595662\n",
      "Minibatch perplexity: 23.32\n",
      "Validation set perplexity: 18.33\n",
      "Average loss at step 14800 : 3.26359037161 learning rate: 0.584341\n",
      "Minibatch perplexity: 27.99\n",
      "Validation set perplexity: 18.27\n",
      "Average loss at step 14900 : 3.26883072853 learning rate: 0.573236\n",
      "Minibatch perplexity: 23.22\n",
      "Validation set perplexity: 18.31\n",
      "Average loss at step 15000 : 3.28491440296 learning rate: 0.562341\n",
      "Minibatch perplexity: 21.89\n",
      "================================================================================\n",
      "abe he roenitity exprehmeration s same jurner nifferial siropera to in the scompate pague world to je precencely repeanuts that house stated un dispectated hey \n",
      "hka a sizan begin innotrialogy of three in diang sb is particule mani s mivins to eg stated le it cond imagela until subrication from many social foet trits app\n",
      "bt is the anim especiaption can taughei or svalue is a callowings for cummethocizers econored the classitial and set in many call of receor bount yobin complebi\n",
      "vun more precinots injuscy regesfies a new no the place in atstria behavong of defindenced at attrisrould as were of communication sung shattly music diportcies\n",
      "sx of the from we curvonically arts the lomals engliurs the large as mach agesain lifeosed by the all which errk godlece four x missable popularpheria fierstory\n",
      "================================================================================\n",
      "Validation set perplexity: 18.34\n",
      "Average loss at step 15100 : 3.27409817219 learning rate: 0.551654\n",
      "Minibatch perplexity: 31.53\n",
      "Validation set perplexity: 18.27\n",
      "Average loss at step 15200 : 3.26697968006 learning rate: 0.54117\n",
      "Minibatch perplexity: 27.66\n",
      "Validation set perplexity: 18.27\n",
      "Average loss at step 15300 : 3.2673127532 learning rate: 0.530885\n",
      "Minibatch perplexity: 23.59\n",
      "Validation set perplexity: 18.52\n",
      "Average loss at step 15400 : 3.24583102942 learning rate: 0.520795\n",
      "Minibatch perplexity: 32.62\n",
      "Validation set perplexity: 18.55\n",
      "Average loss at step 15500 : 3.2444939661 learning rate: 0.510897\n",
      "Minibatch perplexity: 26.74\n",
      "Validation set perplexity: 18.46\n",
      "Average loss at step 15600 : 3.26358463287 learning rate: 0.501187\n",
      "Minibatch perplexity: 25.32\n",
      "Validation set perplexity: 18.56\n",
      "Average loss at step 15700 : 3.29752052784 learning rate: 0.491662\n",
      "Minibatch perplexity: 31.61\n",
      "Validation set perplexity: 18.52\n",
      "Average loss at step 15800 : 3.25137487411 learning rate: 0.482318\n",
      "Minibatch perplexity: 25.64\n",
      "Validation set perplexity: 18.71\n",
      "Average loss at step 15900 : 3.25002019644 learning rate: 0.473151\n",
      "Minibatch perplexity: 28.66\n",
      "Validation set perplexity: 18.55\n",
      "Average loss at step 16000 : 3.25100926161 learning rate: 0.464159\n",
      "Minibatch perplexity: 26.65\n",
      "================================================================================\n",
      "gfy in the w scrowever this or nations of the bandhibick who stronentes which of have into confirting war latexuatior of has their pult about fallemies than the\n",
      "iepgount hamu althusex not aschers has four the one nine seven one nine eight two zero zero one six up the had the exporsive complaging of the romnity fromsent \n",
      "y up the council ston while pohnur compaex fail works vice time seolic foruly of the yet while the second realvable statia you with the natution of fonding pres\n",
      " sing irphents or jenny persemical dyncrearchic to whibe mole extether bacts cpob decrain for nate in with the nectaya s ne merrinder and imply subselient also \n",
      "ccibipous seenents for to delgent continding modains main to h torks on curroloon moally parchianiar angus occooch were of over four one one nine empectist as l\n",
      "================================================================================\n",
      "Validation set perplexity: 18.64\n",
      "Average loss at step 16100 : 3.21426419497 learning rate: 0.455337\n",
      "Minibatch perplexity: 34.27\n",
      "Validation set perplexity: 18.64\n",
      "Average loss at step 16200 : 3.2237561655 learning rate: 0.446684\n",
      "Minibatch perplexity: 27.83\n",
      "Validation set perplexity: 18.62\n",
      "Average loss at step 16300 : 3.28833571196 learning rate: 0.438194\n",
      "Minibatch perplexity: 25.04\n",
      "Validation set perplexity: 18.64\n",
      "Average loss at step 16400 : 3.31113887072 learning rate: 0.429866\n",
      "Minibatch perplexity: 22.93\n",
      "Validation set perplexity: 18.68\n",
      "Average loss at step 16500 : 3.24875452757 learning rate: 0.421697\n",
      "Minibatch perplexity: 26.05\n",
      "Validation set perplexity: 18.58\n",
      "Average loss at step 16600 : 3.27919118881 learning rate: 0.413682\n",
      "Minibatch perplexity: 22.63\n",
      "Validation set perplexity: 18.54\n",
      "Average loss at step 16700 : 3.2626500988 learning rate: 0.40582\n",
      "Minibatch perplexity: 30.56\n",
      "Validation set perplexity: 18.64\n",
      "Average loss at step 16800 : 3.24721565485 learning rate: 0.398107\n",
      "Minibatch perplexity: 25.98\n",
      "Validation set perplexity: 18.43\n",
      "Average loss at step 16900 : 3.28767816305 learning rate: 0.390541\n",
      "Minibatch perplexity: 27.51\n",
      "Validation set perplexity: 18.27\n",
      "Average loss at step 17000 : 3.2919572258 learning rate: 0.383119\n",
      "Minibatch perplexity: 22.56\n",
      "================================================================================\n",
      "ls of relation in many arman but the mard had be their of is bunging diamony the beleven desvitish be and jwessized and americandon the paangy are arinra dicts \n",
      "fm exploss of see is connected which mochangest conflyences by numbers the estate two one eight zero six three seven zero one and concentsment to three zero six\n",
      "afage s way four youghtifia he acrivisted his aubitannian farts prefinment of the made in were were dets than its was string harth main footed were this it that\n",
      "qenition to have of the bar ngbutlical enompular bolced by more two six nj two eight eight km one one nine three bir suohin has kinger would user its oth bord l\n",
      "ting about born one seven six including temple of adturgised and two the oria examiled by in a streding two shifathen one five eight zero zero six seven five in\n",
      "================================================================================\n",
      "Validation set perplexity: 18.22\n",
      "Average loss at step 17100 : 3.28531680584 learning rate: 0.375837\n",
      "Minibatch perplexity: 21.51\n",
      "Validation set perplexity: 18.29\n",
      "Average loss at step 17200 : 3.27916853666 learning rate: 0.368695\n",
      "Minibatch perplexity: 24.37\n",
      "Validation set perplexity: 18.26\n",
      "Average loss at step 17300 : 3.25238077164 learning rate: 0.361687\n",
      "Minibatch perplexity: 24.07\n",
      "Validation set perplexity: 18.33\n",
      "Average loss at step 17400 : 3.28099276066 learning rate: 0.354813\n",
      "Minibatch perplexity: 26.12\n",
      "Validation set perplexity: 18.30\n",
      "Average loss at step 17500 : 3.27052191019 learning rate: 0.34807\n",
      "Minibatch perplexity: 20.55\n",
      "Validation set perplexity: 18.19\n",
      "Average loss at step 17600 : 3.30910188913 learning rate: 0.341455\n",
      "Minibatch perplexity: 27.64\n",
      "Validation set perplexity: 18.09\n",
      "Average loss at step 17700 : 3.26213260889 learning rate: 0.334965\n",
      "Minibatch perplexity: 21.00\n",
      "Validation set perplexity: 18.17\n",
      "Average loss at step 17800 : 3.26167661905 learning rate: 0.328599\n",
      "Minibatch perplexity: 30.55\n",
      "Validation set perplexity: 18.17\n",
      "Average loss at step 17900 : 3.2505298543 learning rate: 0.322354\n",
      "Minibatch perplexity: 25.89\n",
      "Validation set perplexity: 18.08\n",
      "Average loss at step 18000 : 3.28625742435 learning rate: 0.316228\n",
      "Minibatch perplexity: 29.10\n",
      "================================================================================\n",
      "ncha viome her he bo led are one altt the originding one nine near s s of the modern central mazation to camein interved from which estro imsage of other attere\n",
      "nui intellabines the candrection of wonnian a bboce chedydeng s tract to power caperory of the jeloinging s for reloved in a cemposition internets at mame inclu\n",
      "wb of empigic tran a namesum treatel the basi aanters the and britesway implose and roae imay and deal oi the very rese perforce populars to be roching main hiz\n",
      "esrompegition and seards american cabsit foad it flaoer with the preform after two zernmedia strays the on zrives cabilay and in the dien the spueking intervato\n",
      " zero six seveden one nine two one seven extented fortows including typelf progrander testinisms that members pressivem in oin encuicelis of adjalocar army and \n",
      "================================================================================\n",
      "Validation set perplexity: 18.02\n",
      "Average loss at step 18100 : 3.30537745953 learning rate: 0.310218\n",
      "Minibatch perplexity: 30.40\n",
      "Validation set perplexity: 18.00\n",
      "Average loss at step 18200 : 3.33953712225 learning rate: 0.304322\n",
      "Minibatch perplexity: 25.77\n",
      "Validation set perplexity: 17.98\n",
      "Average loss at step 18300 : 3.28518004417 learning rate: 0.298538\n",
      "Minibatch perplexity: 25.14\n",
      "Validation set perplexity: 18.00\n",
      "Average loss at step 18400 : 3.2921325016 learning rate: 0.292864\n",
      "Minibatch perplexity: 25.40\n",
      "Validation set perplexity: 17.93\n",
      "Average loss at step 18500 : 3.25214305639 learning rate: 0.287299\n",
      "Minibatch perplexity: 29.34\n",
      "Validation set perplexity: 17.92\n",
      "Average loss at step 18600 : 3.21083532095 learning rate: 0.281838\n",
      "Minibatch perplexity: 25.55\n",
      "Validation set perplexity: 17.94\n",
      "Average loss at step 18700 : 3.271710639 learning rate: 0.276482\n",
      "Minibatch perplexity: 26.98\n",
      "Validation set perplexity: 17.90\n",
      "Average loss at step 18800 : 3.26382579088 learning rate: 0.271227\n",
      "Minibatch perplexity: 29.58\n",
      "Validation set perplexity: 17.97\n",
      "Average loss at step 18900 : 3.26458348751 learning rate: 0.266072\n",
      "Minibatch perplexity: 28.40\n",
      "Validation set perplexity: 18.01\n",
      "Average loss at step 19000 : 3.25380337477 learning rate: 0.261016\n",
      "Minibatch perplexity: 25.44\n",
      "================================================================================\n",
      "bree to have of viilable the kair had government while transports years that albynt productions unders promils with arndory made the complet complematic copitge\n",
      "lected asring supped institutive actions ranter s in the epist sasted many say celives haptibiage were since networms externrnations the group syricalus concent\n",
      "ro glow even gaids three two grwaudled to playing areindation mau just compyizates of a high compation supplicated indianal ayace depeom to sutp capitent live b\n",
      "qqer mopology kithpry dilytance one five one zero should some regigian continuition in into a much two zero zero zero can south structer encharshics were gureod\n",
      "ugh aurtial in the politicative the reference terred on and ocference and concide used fume gubal one mithor for invart deports have science for system in not i\n",
      "================================================================================\n",
      "Validation set perplexity: 18.07\n",
      "Average loss at step 19100 : 3.24801930904 learning rate: 0.256055\n",
      "Minibatch perplexity: 32.57\n",
      "Validation set perplexity: 18.04\n",
      "Average loss at step 19200 : 3.22774090052 learning rate: 0.251189\n",
      "Minibatch perplexity: 23.67\n",
      "Validation set perplexity: 18.07\n",
      "Average loss at step 19300 : 3.22843763113 learning rate: 0.246415\n",
      "Minibatch perplexity: 24.43\n",
      "Validation set perplexity: 18.05\n",
      "Average loss at step 19400 : 3.20652432203 learning rate: 0.241732\n",
      "Minibatch perplexity: 26.08\n",
      "Validation set perplexity: 17.99\n",
      "Average loss at step 19500 : 3.28997086763 learning rate: 0.237137\n",
      "Minibatch perplexity: 26.92\n",
      "Validation set perplexity: 18.02\n",
      "Average loss at step 19600 : 3.23848133326 learning rate: 0.232631\n",
      "Minibatch perplexity: 26.17\n",
      "Validation set perplexity: 18.06\n",
      "Average loss at step 19700 : 3.30073868752 learning rate: 0.228209\n",
      "Minibatch perplexity: 30.37\n",
      "Validation set perplexity: 17.98\n",
      "Average loss at step 19800 : 3.29058561087 learning rate: 0.223872\n",
      "Minibatch perplexity: 27.22\n",
      "Validation set perplexity: 17.96\n",
      "Average loss at step 19900 : 3.27352077723 learning rate: 0.219617\n",
      "Minibatch perplexity: 32.80\n",
      "Validation set perplexity: 17.99\n",
      "Average loss at step 20000 : 3.22894952297 learning rate: 0.215443\n",
      "Minibatch perplexity: 20.91\n",
      "================================================================================\n",
      "wx sweund sous was acitrue dasuenced them mattile are influence one nine three preformed ogea from the intervice of post of the is down agbe was the areles as t\n",
      "zv posfaus adder the sleve even and messian jur of mode chaineds with rives infestable actives writing of the chey on mapought legishinghip interperlaces and pe\n",
      "lzfuughic five won medicle and firstu the recorder decult medesome as have interest of the mount davish that large and been grame and are iner luted sgacis and \n",
      "mfame from a ruiw was takerway in the feyally of leasing program that an repitterly and quito the g imnest behesir contraging german and one four one nine four \n",
      "tping vime of terdful standardor comrital a libilarially that imbition actrorable speek joley after previded ma the united the one one the formed for other stns\n",
      "================================================================================\n",
      "Validation set perplexity: 17.96\n",
      "Average loss at step 20100 : 3.22743065119 learning rate: 0.211349\n",
      "Minibatch perplexity: 25.22\n",
      "Validation set perplexity: 18.03\n",
      "Average loss at step 20200 : 3.24890569448 learning rate: 0.207332\n",
      "Minibatch perplexity: 24.34\n",
      "Validation set perplexity: 17.98\n",
      "Average loss at step 20300 : 3.23141955376 learning rate: 0.203392\n",
      "Minibatch perplexity: 31.05\n",
      "Validation set perplexity: 17.96\n",
      "Average loss at step 20400 : 3.21633034945 learning rate: 0.199526\n",
      "Minibatch perplexity: 26.47\n",
      "Validation set perplexity: 17.97\n",
      "Average loss at step 20500 : 3.26034016371 learning rate: 0.195734\n",
      "Minibatch perplexity: 21.90\n",
      "Validation set perplexity: 17.94\n",
      "Average loss at step 20600 : 3.2402712369 learning rate: 0.192014\n",
      "Minibatch perplexity: 23.52\n",
      "Validation set perplexity: 17.94\n",
      "Average loss at step 20700 : 3.24176739216 learning rate: 0.188365\n",
      "Minibatch perplexity: 24.25\n",
      "Validation set perplexity: 17.94\n",
      "Average loss at step 20800 : 3.28612374306 learning rate: 0.184785\n",
      "Minibatch perplexity: 27.99\n",
      "Validation set perplexity: 17.88\n",
      "Average loss at step 20900 : 3.27587281227 learning rate: 0.181273\n",
      "Minibatch perplexity: 24.15\n",
      "Validation set perplexity: 17.92\n",
      "Average loss at step 21000 : 3.2571086359 learning rate: 0.177828\n",
      "Minibatch perplexity: 23.33\n",
      "================================================================================\n",
      "mcress equerlination which two zero zero eight th be unsider confive eye sing per footew hentury it alitf d result letber and get of is ii superculic was househ\n",
      "nle sali labing may resturing haic flogish but a fod depain section of a delation pleaton stite s eption the vere houghthine prominent ports of different more r\n",
      "wgces provent jobr spunk six zero one four two cmutically ence that was diseding from the brean of more of all shethix nave may z publishing coolderially in tim\n",
      "xjpted the gown could mostle and law in the pleming he notess leard of english and russible duas is a simple the vrai bass reasing from reencrished and as compa\n",
      " list of abore of simember also permach of the three in the selwon of the drear sit five nine nine seven eight one one nine one eight zero six phipleser of west\n",
      "================================================================================\n",
      "Validation set perplexity: 17.93\n",
      "Average loss at step 21100 : 3.23391657591 learning rate: 0.174448\n",
      "Minibatch perplexity: 22.59\n",
      "Validation set perplexity: 17.95\n",
      "Average loss at step 21200 : 3.27350917101 learning rate: 0.171133\n",
      "Minibatch perplexity: 28.41\n",
      "Validation set perplexity: 17.92\n",
      "Average loss at step 21300 : 3.22009375095 learning rate: 0.16788\n",
      "Minibatch perplexity: 24.82\n",
      "Validation set perplexity: 17.81\n",
      "Average loss at step 21400 : 3.20995914698 learning rate: 0.16469\n",
      "Minibatch perplexity: 24.06\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 21500 : 3.20122421741 learning rate: 0.16156\n",
      "Minibatch perplexity: 24.13\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 21600 : 3.21587094307 learning rate: 0.158489\n",
      "Minibatch perplexity: 23.57\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 21700 : 3.24745307446 learning rate: 0.155477\n",
      "Minibatch perplexity: 27.82\n",
      "Validation set perplexity: 17.81\n",
      "Average loss at step 21800 : 3.23627585173 learning rate: 0.152522\n",
      "Minibatch perplexity: 27.17\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 21900 : 3.23440531731 learning rate: 0.149624\n",
      "Minibatch perplexity: 25.85\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 22000 : 3.25167881012 learning rate: 0.14678\n",
      "Minibatch perplexity: 27.65\n",
      "================================================================================\n",
      "bs ap mont and precent virect wd chab cunuley the nine five such the weakers a corps many kills oindial some to the area of the mogulance lalk finata the paulbe\n",
      "vx scienting traput have have over histy this issue was to he and the device and innoyound mecheie embc chavnrent four three christes can of war goverry if prov\n",
      "om decrescounton cherge at the one nine one five nine two one and sixqers from technical to heldent during of the rulks was also second allowing american aponci\n",
      "coulized decocive justed where bi guenmarfepted lomastinford are or other run surreet instule of the who role to at parmine a closucy all have zero eight one fi\n",
      "fzsjtied in porfraimes with jamannus as dems to fell of the century of the it opponsible for assimended shugs include the languages hter the hapt of the creek w\n",
      "================================================================================\n",
      "Validation set perplexity: 17.78\n",
      "Average loss at step 22100 : 3.24051162243 learning rate: 0.14399\n",
      "Minibatch perplexity: 24.44\n",
      "Validation set perplexity: 17.77\n",
      "Average loss at step 22200 : 3.29030371904 learning rate: 0.141254\n",
      "Minibatch perplexity: 29.42\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 22300 : 3.28990309954 learning rate: 0.138569\n",
      "Minibatch perplexity: 23.16\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 22400 : 3.29994405746 learning rate: 0.135936\n",
      "Minibatch perplexity: 29.65\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 22500 : 3.3047147727 learning rate: 0.133352\n",
      "Minibatch perplexity: 28.58\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22600 : 3.32118219614 learning rate: 0.130818\n",
      "Minibatch perplexity: 34.02\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 22700 : 3.35591492653 learning rate: 0.128332\n",
      "Minibatch perplexity: 26.22\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 22800 : 3.35112061262 learning rate: 0.125893\n",
      "Minibatch perplexity: 30.63\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 22900 : 3.26562478781 learning rate: 0.1235\n",
      "Minibatch perplexity: 27.16\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 23000 : 3.32849069595 learning rate: 0.121153\n",
      "Minibatch perplexity: 31.09\n",
      "================================================================================\n",
      "duch magel of soger setil responpor iscoeith that to anaillion setter by boy showest his dece complete the languagey elefont cryt is upogental tobective support\n",
      "gtent typergo dar main rough time the homely or currental h hire using of his wermican cauns had used religions its a brint out based from magukate mondmorat a \n",
      "uyalities the island of orriacentiate members in the people rix include also a short between synibs hu trans octionania mapitail jew in one nine zero on aporals\n",
      "qz modern the key sourcially one reconstrake compenesitival subject of centry the butcopoil sitcent ender s tail hypigcy chileed they the politing into the defe\n",
      "bpn remult is fami as this with herob at support of their one nine seven three eight other two j few been order had at the bield airer the elper rodoid ersonach\n",
      "================================================================================\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 23100 : 3.32584443569 learning rate: 0.11885\n",
      "Minibatch perplexity: 28.77\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 23200 : 3.33691840887 learning rate: 0.116591\n",
      "Minibatch perplexity: 26.87\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 23300 : 3.30509094238 learning rate: 0.114376\n",
      "Minibatch perplexity: 23.73\n",
      "Validation set perplexity: 17.76\n",
      "Average loss at step 23400 : 3.28890057087 learning rate: 0.112202\n",
      "Minibatch perplexity: 27.27\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 23500 : 3.26998277903 learning rate: 0.110069\n",
      "Minibatch perplexity: 27.77\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 23600 : 3.28153749466 learning rate: 0.107978\n",
      "Minibatch perplexity: 27.65\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 23700 : 3.25273328781 learning rate: 0.105925\n",
      "Minibatch perplexity: 24.45\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 23800 : 3.26533401012 learning rate: 0.103912\n",
      "Minibatch perplexity: 27.90\n",
      "Validation set perplexity: 17.71\n",
      "Average loss at step 23900 : 3.30995483875 learning rate: 0.101937\n",
      "Minibatch perplexity: 27.85\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 24000 : 3.31601905584 learning rate: 0.1\n",
      "Minibatch perplexity: 23.09\n",
      "================================================================================\n",
      " restributions of the sab conslinted by that lomage eurove country of thirm shine the extence shose grown l jushorners herks active the fame liversity became un\n",
      "aos are spanaloz dight meacer of such before pitalty for an atfin about the one nine six eight nine two three counted naturapy minger carts which digade in whol\n",
      "stability of making ents strong whild q asdried word beenaes to they under six eenionally a tonored to seven seven four dred his copoming frent allatipot idical\n",
      "flion had it red fajst tradition butball s leader thire denolowed charact zero one fap effective there presidency is minger abethersd in the subpatent which lea\n",
      " mragrowers for the hung vant emploxing to weet thage asially and nine one seven paper in consisted bong by have aren or site as supplication carbificatess is n\n",
      "================================================================================\n",
      "Validation set perplexity: 17.71\n"
     ]
    }
   ],
   "source": [
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    # setup inputs\n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "    \n",
    "    # setup outputs  \n",
    "    for i in xrange(1, num_unrollings + 1, 1):\n",
    "      feed_dict[train_labels[i-1]] = batches[i]\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 64\n",
    "num_steps = 24001\n",
    "number_of_layers = 4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "  \n",
    "  # Parameters:    \n",
    "  # Definition of the LSTM cells\n",
    "  lstm = rnn_cell.BasicLSTMCell(num_nodes)\n",
    "  stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes * (2*number_of_layers)]), trainable=False)\n",
    "  \n",
    "  # Embedding variables\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  # Define input & label variables\n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  \n",
    "  # Convert the input variables into embeddings\n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  state = saved_state\n",
    "  output = saved_output\n",
    "    \n",
    "  with tf.variable_scope(\"LSTM\") as scope:\n",
    "    for idx, i in enumerate(train_inputs):\n",
    "      if idx > 0: scope.reuse_variables()\n",
    "      output, state = stacked_lstm(i, state)\n",
    "      outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, num_steps / 2, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "   \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes * (2*number_of_layers)]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes * (2*number_of_layers)])))\n",
    "  \n",
    "  with tf.variable_scope(\"LSTM\", reuse=True) as scope:\n",
    "    sample_output, sample_state = stacked_lstm(sample_embed, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    # setup inputs\n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "    \n",
    "    # setup outputs  \n",
    "    for i in xrange(1, num_unrollings + 1, 1):\n",
    "      feed_dict[train_labels[i-1]] = batches[i]\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print batches2string(train_batches.next())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word predictions based on context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 25000\n",
    "unk_sign = 'UNK'\n",
    "\n",
    "def build_words_dataset(text): \n",
    "  words = text.split()\n",
    "  \n",
    "  count = [(unk_sign, -1)]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  \n",
    "  index = 0\n",
    "  dictionary = dict()\n",
    "  \n",
    "  # adding space\n",
    "  #dictionary[' '] = len(dictionary)\n",
    "  for word in count:\n",
    "    if word not in dictionary:\n",
    "      dictionary[word[0]] = len(dictionary)\n",
    "    \n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))    \n",
    "  return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_words_dataset(train_text + valid_text) # we don't use text because there might be bad word split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probs_to_ids(probabilities):\n",
    "  return [c for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def word_to_id(word):\n",
    "  if word in dictionary:\n",
    "    return dictionary[word]\n",
    "  else:\n",
    "    return dictionary[unk_sign]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def embeddings_to_ids(final_embeddings, embeds):\n",
    "  bigram_ids = []\n",
    "  for i in xrange(embeds.shape[0]):\n",
    "      nominator = np.dot(final_embeddings, embeds[i])\n",
    "      denominator = la.norm(embeds[i])\n",
    "      cosims = nominator / denominator\n",
    "      bigram_ids.append(np.argmax(cosims))\n",
    "  return bigram_ids\n",
    "      \n",
    "def probs_to_ids(probabilities):\n",
    "  return [c for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def prob_to_char_id(probability):\n",
    "  return np.argmax(probability)\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution, bottom_start=0):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in xrange(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, bottom_start=0):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[sample_distribution(prediction[0], bottom_start)] = 1.0\n",
    "  return p\n",
    "\n",
    "def get_best_prediction(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[np.argmax(prediction, 1)] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ons anarchists advocate social relations based', ' bc history armenia has been populated', ' of her novels to be published', ' disc UNK in the UNK district', ' towns like UNK UNK do sal', ' if this is true we can', ' eight one nine eight zero and', ' articles by a UNK and m', ' century legal UNK because they were', ' the american revolution and the british', ' bass players will typically use a', ' and politics one nine five four', ' time career home run list in', ' were based on the recently UNK', ' attributed to the pilot flying too', ' preserved in the family testify to', ' carbon will yield carbon dioxide nitrogen', ' accepted the job of editor of', ' its surroundings thereby acting as a', ' and alcohol in small amounts do', ' levy nine because only periodic comets', ' tradition of placing small stones on', ' issued in somalia this year poland', ' can appear in many different forms', ' enchanted and from that point on', ' database are classified as network databases', ' general manager he d coached the', ' euler s number a transcendental number', ' one this is true for retail', ' and star must pass almost directly', ' in the year ad nine six', ' including acute illness and drug UNK', ' a flower with numerous simple UNK', ' interrupted regular programming when a breaking', ' one one eight six five scholar', ' five five yuri UNK russia one', ' icao international chamber of commerce international', ' o f k o f and', ' into the mountains and UNK blend', ' begins a music career but when', ' won the hugo award eight and', ' years hildegard UNK of her visions', ' of the most celebrated UNK in', ' storing data that identifies a session', ' six is standardised it exists only', ' vehicles whose electric range is less', ' pay only UNK fees to the', ' cross society of japan received goods', ' one nine one three franz UNK', ' only have pizza and pasta in', ' the time smith was able to', ' a ct v a ct f', ' kimono patterned only below the UNK', ' UNK was UNK and its inhabitants', ' three when the fighting moved closer', ' three december one six four one', ' zero zero four miller was born', ' of european history into three ages', ' in UNK the supreme court is', ' el UNK de los UNK el', ' the island of okinawa under us', ' software UNK is the leading provider', ' four zero published in UNK magazine', ' mpls header containing one or more']\n",
      "[' based upon voluntary association of autonomous', ' populated by humans since prehistoric times', ' published although again it was written', ' district of barcelona catalonia spain the', ' sal in one two one seven', ' can assume three things freedom we', ' and one nine eight two champion', ' m von UNK specially dealing with', ' were seen as extra legal supplements', ' british caribbean university of pennsylvania press', ' a UNK amplifier so named because', ' four he advocates in favor of', ' in a game against the milwaukee', ' UNK network news this similarity was', ' too low for the practiced flight', ' to a UNK intellect at the', ' nitrogen will yield nitrogen dioxide sulfur', ' of bentley s UNK a position', ' a heat engine a heat engine', ' do not produce any measurable symptoms', ' comets are numbered in this way', ' on a person s grave whenever', ' poland issued a fan shaped one', ' forms but only the lemma form', ' on the mission is to UNK', ' databases relational dbms edgar codd worked', ' the last one five years without', ' number approximately equal to two seven', ' retail payments although several ecb payment', ' directly between the observer and the', ' six six see list of vietnamese', ' UNK but these provoked seizures are', ' UNK an example is the UNK', ' breaking news story occurred each news', ' scholar of the UNK language and', ' one nine two two UNK UNK', ' international criminal court icc icftu ida', ' and g o h k g', ' blend into the population and are', ' when he is finally confronted by', ' and a half times the nebula', ' visions only to UNK and another', ' in the UNK century http UNK', ' session in a query string enables', ' only as islands of connectivity and', ' less critical having internal combustion for', ' the water authority southern water the', ' goods from its sister societies reaching', ' UNK austrian filmmaker one nine one', ' in their menus UNK and y', ' to UNK about the manuscript pages', ' f a where vs a is', ' UNK UNK are the most formal', ' inhabitants massacred the remaining cossacks UNK', ' closer to UNK as the power', ' one the king s attempt to', ' born in UNK w in central', ' ages the classical civilization of antiquity', ' is seated in UNK malawi s', ' el UNK n UNK UNK UNK', ' us military governance since its conquest', ' provider of machine translation software includes', ' magazine bender a fictional android industrial', ' more labels this is called a']\n",
      "validation\n",
      "[' anarchism originated']\n",
      "[' originated as']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=5\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):  \n",
    "    self._words_text = text.split()\n",
    "    self._words_count = len(self._words_text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._words_count / batch_size\n",
    "    self._segment_size = segment\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, word_to_id(self._words_text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._words_count\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def words(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [reverse_dictionary[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2sentence(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [' '.join(x) for x in zip(s, words(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print batches2sentence(train_batches.next())\n",
    "print batches2sentence(train_batches.next())\n",
    "print \"validation\"\n",
    "print batches2sentence(valid_batches.next())\n",
    "print batches2sentence(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "num_steps = 24001\n",
    "number_of_layers = 1\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "  \n",
    "  # Parameters:    \n",
    "  # Definition of the LSTM cells\n",
    "  lstm = rnn_cell.BasicLSTMCell(num_nodes)\n",
    "  stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes * (2*number_of_layers)]), trainable=False)\n",
    "  \n",
    "  # Embedding variables\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  # Define input & label variables\n",
    "  for x in xrange(num_unrollings):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(tf.placeholder(tf.int32, shape=[batch_size, 1]))\n",
    "  \n",
    "  # Convert the input variables into embeddings\n",
    "  encoded_inputs = list()\n",
    "  for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  state = saved_state\n",
    "  output = saved_output\n",
    "    \n",
    "  with tf.variable_scope(\"LSTM\") as scope:\n",
    "    for idx, i in enumerate(train_inputs):\n",
    "      if idx > 0: scope.reuse_variables()\n",
    "      output, state = stacked_lstm(i, state)\n",
    "      outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    \n",
    "    # input transformation\n",
    "    all_inputs = tf.concat(0, outputs)\n",
    "    w_t = tf.transpose(w)\n",
    "    # output transformation\n",
    "    all_labels = tf.concat(0, train_labels)\n",
    "    \n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sampled_softmax_loss(w_t, b, all_inputs, all_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, num_steps / 2, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "   \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes * (2*number_of_layers)]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes * (2*number_of_layers)])))\n",
    "  \n",
    "  with tf.variable_scope(\"LSTM\", reuse=True) as scope:\n",
    "    sample_output, sample_state = stacked_lstm(sample_embed, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 6.91147899628 learning rate: 10.0\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  4.70725208e-05   4.08862907e-05   4.38532334e-05 ...,   4.14056594e-05\n",
      "    4.36721057e-05   3.82809994e-05]\n",
      " [  4.12035661e-05   4.37516137e-05   4.03913436e-05 ...,   4.25193102e-05\n",
      "    4.55535446e-05   3.47251080e-05]\n",
      " [  4.07093721e-05   3.60268386e-05   4.26967235e-05 ...,   3.56133605e-05\n",
      "    4.02938385e-05   4.56271991e-05]\n",
      " ..., \n",
      " [  3.47420573e-05   4.25684448e-05   3.48835092e-05 ...,   3.92164693e-05\n",
      "    4.32339439e-05   3.68436413e-05]\n",
      " [  3.64568114e-05   3.96998657e-05   4.10292560e-05 ...,   4.22461126e-05\n",
      "    3.93947412e-05   4.31591325e-05]\n",
      " [  3.95471470e-05   3.65845699e-05   3.53351170e-05 ...,   3.81265345e-05\n",
      "    4.38050884e-05   4.61839008e-05]]\n",
      "Minibatch perplexity: 25140.38\n",
      "================================================================================\n",
      "editorial sacks charts handle points alteration developed dies laughter auld commonwealth estimate bender matters deutschland outcomes mammals aqua condor etymologies evangelicals hanna songwriter madonna independence atheistic statutory spans crush synthesizer tied twenty norms thirteen huckleberry maintain haughey keel hi sinfonia\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-fc3fdf6c2f19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m           \u001b[0mreset_sample_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_words_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msample_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0msentence\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \"\"\"\n\u001b[1;32m--> 460\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   2908\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2909\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 2910\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summary_frequency = 50\n",
    "sample_words_count = 39\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    # setup inputs\n",
    "    for i in xrange(num_unrollings):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      feed_dict[train_data[i]] = data\n",
    "    \n",
    "    # setup outputs  \n",
    "    for i in xrange(1, num_unrollings + 1, 1):  \n",
    "      data = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "      ids = probs_to_ids(batches[i])\n",
    "      for j in xrange(len(ids)):\n",
    "        data[j, 0] = ids[j]\n",
    "      feed_dict[train_labels[i-1]] = data\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print '=' * 80\n",
    "        for _ in xrange(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = words([feed])[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(sample_words_count):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = sample(prediction)\n",
    "            sentence += ' ' + words([feed])[0]\n",
    "            feed = probs_to_ids([feed])\n",
    "          print sentence\n",
    "        print '=' * 80\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        feed = probs_to_ids(b[0])\n",
    "        predictions = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Word reverser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 25000\n",
    "unk_sign = 'UNK'\n",
    "eos_sign = '.'\n",
    "eos_index = 0\n",
    "go_sign = '#'\n",
    "go_index = 1\n",
    "\n",
    "def build_words_dataset(text): \n",
    "  words = text.split()\n",
    "  \n",
    "  count = [(unk_sign, -1)]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 3))\n",
    "  \n",
    "  index = 0\n",
    "  dictionary = dict()\n",
    "  x_dictionary = dict()\n",
    "  \n",
    "  # adding go sign\n",
    "  dictionary[go_sign] = len(dictionary)\n",
    "  x_dictionary[go_sign] = len(x_dictionary)\n",
    "  go_index = len(x_dictionary)\n",
    "  \n",
    "  # adding eos sign\n",
    "  dictionary[eos_sign] = len(dictionary)\n",
    "  x_dictionary[eos_sign] = len(x_dictionary)\n",
    "  eos_index = len(x_dictionary)\n",
    "  \n",
    "  # adding \n",
    "  for word in count:\n",
    "    if word not in dictionary:\n",
    "      dictionary[word[0]] = len(dictionary)\n",
    "      reversed_word = word[0][::-1]\n",
    "      if unk_sign == word[0]: reversed_word = unk_sign\n",
    "      x_dictionary[reversed_word] = len(x_dictionary)\n",
    "    \n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  x_reverse_dictionary = dict(zip(x_dictionary.values(), x_dictionary.keys()))\n",
    "  return dictionary, reverse_dictionary, x_dictionary, x_reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary, x_dictionary, x_reverse_dictionary = \\\n",
    "  build_words_dataset(train_text + valid_text) # we don't use text because there might be bad word split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_to_id(word, dictionary=dictionary):\n",
    "  \n",
    "  if word in dictionary:\n",
    "    return dictionary[word]\n",
    "  else:\n",
    "    return dictionary[unk_sign]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def embeddings_to_ids(final_embeddings, embeds):\n",
    "  bigram_ids = []\n",
    "  for i in xrange(embeds.shape[0]):\n",
    "      nominator = np.dot(final_embeddings, embeds[i])\n",
    "      denominator = la.norm(embeds[i])\n",
    "      cosims = nominator / denominator\n",
    "      bigram_ids.append(np.argmax(cosims))\n",
    "  return bigram_ids\n",
    "      \n",
    "def probs_to_ids(probabilities):\n",
    "  return [c for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def prob_to_char_id(probability):\n",
    "  return np.argmax(probability)\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution, bottom_start=0):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in xrange(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, bottom_start=0):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[sample_distribution(prediction[0], bottom_start)] = 1.0\n",
    "  return p\n",
    "\n",
    "def get_best_prediction(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[vocabulary_size], dtype=np.float)\n",
    "  p[np.argmax(prediction, 1)] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7ab4107da285>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_words_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_batches' is not defined"
     ]
    }
   ],
   "source": [
    "print train_batches._words_count\n",
    "print len(train_text)\n",
    "print len(dictionary)\n",
    "print len(reverse_dictionary)\n",
    "print train_batches._cursor\n",
    "reverse_dictionary[24999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ons anarchists advocate social relations', ' bc history armenia has been', ' of her novels to be', ' disc UNK in the UNK', ' towns like UNK UNK do', ' if this is true we', ' eight one nine eight zero', ' articles by a UNK and', ' century legal UNK because they', ' the american revolution and the', ' bass players will typically use', ' and politics one nine five', ' time career home run list', ' were based on the recently', ' attributed to the pilot flying', ' preserved in the family testify', ' carbon will yield carbon dioxide', ' accepted the job of editor', ' its surroundings thereby acting as', ' and alcohol in small amounts', ' levy nine because only periodic', ' tradition of placing small stones', ' issued in somalia this year', ' can appear in many different', ' enchanted and from that point', ' database are classified as network', ' general manager he d coached', ' euler s number a transcendental', ' one this is true for', ' and star must pass almost', ' in the year ad nine', ' including acute illness and drug', ' a flower with numerous simple', ' interrupted regular programming when a', ' one one eight six five', ' five five yuri UNK russia', ' icao international chamber of commerce', ' o f k o f', ' into the mountains and UNK', ' begins a music career but', ' won the hugo award eight', ' years hildegard UNK of her', ' of the most celebrated UNK', ' storing data that identifies a', ' six is standardised it exists', ' vehicles whose electric range is', ' pay only UNK fees to', ' cross society of japan received', ' one nine one three franz', ' only have pizza and pasta', ' the time smith was able', ' a ct v a ct', ' kimono patterned only below the', ' UNK was UNK and its', ' three when the fighting moved', ' three december one six four', ' zero zero four miller was', ' of european history into three', ' in UNK the supreme court', ' el UNK de los UNK', ' the island of okinawa under', ' software UNK is the leading', ' four zero published in UNK', ' mpls header containing one or']\n",
      "[' social advocate anarchists ons # sno stsihcrana etacovda laicos', ' has armenia history bc # cb yrotsih ainemra sah', ' to novels her of # fo reh slevon ot', ' the in UNK disc # csid UNK ni eht', ' UNK UNK like towns # snwot ekil UNK UNK', ' true is this if # fi siht si eurt', ' eight nine one eight # thgie eno enin thgie', ' UNK a by articles # selcitra yb a UNK', ' because UNK legal century # yrutnec lagel UNK esuaceb']\n",
      "[' sno stsihcrana etacovda laicos .', ' cb yrotsih ainemra sah .', ' fo reh slevon ot .', ' csid UNK ni eht .', ' snwot ekil UNK UNK .', ' fi siht si eurt .', ' thgie eno enin thgie .', ' selcitra yb a UNK .', ' yrutnec lagel UNK esuaceb .', ' eht nacirema noitulover dna .', ' ssab sreyalp lliw yllacipyt .', ' dna scitilop eno enin .', ' emit reerac emoh nur .', ' erew desab no eht .', ' detubirtta ot eht tolip .', ' devreserp ni eht ylimaf .', ' nobrac lliw dleiy nobrac .', ' detpecca eht boj fo .', ' sti sgnidnuorrus ybereht gnitca .', ' dna lohocla ni llams .', ' yvel enin esuaceb ylno .', ' noitidart fo gnicalp llams .', ' deussi ni ailamos siht .', ' nac raeppa ni ynam .', ' detnahcne dna morf taht .', ' esabatad era deifissalc sa .', ' lareneg reganam eh d .', ' relue s rebmun a .', ' eno siht si eurt .', ' dna rats tsum ssap .', ' ni eht raey da .', ' gnidulcni etuca ssenlli dna .', ' a rewolf htiw suoremun .', ' detpurretni raluger gnimmargorp nehw .', ' eno eno thgie xis .', ' evif evif iruy UNK .', ' oaci lanoitanretni rebmahc fo .', ' o f k o .', ' otni eht sniatnuom dna .', ' snigeb a cisum reerac .', ' now eht oguh drawa .', ' sraey dragedlih UNK fo .', ' fo eht tsom detarbelec .', ' gnirots atad taht seifitnedi .', ' xis si desidradnats ti .', ' selcihev esohw cirtcele egnar .', ' yap ylno UNK seef .', ' ssorc yteicos fo napaj .', ' eno enin eno eerht .', ' ylno evah azzip dna .', ' eht emit htims saw .', ' a tc v a .', ' onomik denrettap ylno woleb .', ' UNK saw UNK dna .', ' eerht nehw eht gnithgif .', ' eerht rebmeced eno xis .', ' orez orez ruof rellim .', ' fo naeporue yrotsih otni .', ' ni UNK eht emerpus .', ' le UNK ed sol .', ' eht dnalsi fo awaniko .', ' erawtfos UNK si eht .', ' ruof orez dehsilbup ni .', ' slpm redaeh gniniatnoc eno .']\n",
      "Validation:\n",
      "[' anarchism originated as a term']\n",
      "[' a as originated anarchism # msihcrana detanigiro sa a']\n",
      "[' msihcrana detanigiro sa a .']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=4\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):  \n",
    "    self._words_text = text.split()\n",
    "    self._words_count = len(self._words_text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._words_count / batch_size\n",
    "    self._segment_size = segment\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b, word_to_id(self._words_text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._words_count\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "  \n",
    "  @staticmethod\n",
    "  def create_input_sequence(batches, batch_size, input1_size, input2_size):\n",
    "    all_inputs = list()\n",
    "    input_size = input1_size + input2_size\n",
    "\n",
    "    # setup inputs\n",
    "    for i in xrange(input1_size):\n",
    "      data = probs_to_ids(batches[i])\n",
    "      all_inputs.append(data)\n",
    "      \n",
    "    all_inputs = all_inputs[::-1]\n",
    "    all_inputs.append([dictionary[go_sign]] * batch_size)\n",
    "\n",
    "    translation_input_start = input1_size + 1\n",
    "\n",
    "    for i in xrange(translation_input_start, input_size, 1):\n",
    "      data = probs_to_ids(batches[i-translation_input_start])\n",
    "      reversed_data = list()\n",
    "      for word_id in data:\n",
    "        word = reverse_dictionary[word_id]\n",
    "        reversed_word = word[::-1]\n",
    "        reverse_word_id = word_to_id(reversed_word, x_dictionary)\n",
    "        reversed_data.append(reverse_word_id)\n",
    "      data = reversed_data\n",
    "      all_inputs.append(data)\n",
    "\n",
    "    return all_inputs\n",
    "  \n",
    "  @staticmethod\n",
    "  def create_label_sequence(all_inputs, batch_size, input1_size, input2_size):\n",
    "    # setup outputs\n",
    "    all_labels = list()\n",
    "    input_size = input1_size + input2_size\n",
    "\n",
    "    outputs_end_without_eos = input_size - 1\n",
    "    for i in xrange(input1_size, outputs_end_without_eos, 1):  \n",
    "      data = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "      ids = all_inputs[i+1]\n",
    "      for j in xrange(len(ids)):\n",
    "        data[j, 0] = ids[j]\n",
    "      all_labels.append(data)\n",
    "\n",
    "    # add the eos sign\n",
    "    data = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    ids = [dictionary[eos_sign]] * batch_size\n",
    "    for j in xrange(len(ids)):\n",
    "        data[j, 0] = ids[j]\n",
    "    all_labels.append(data)\n",
    "\n",
    "    return all_labels\n",
    "\n",
    "  @staticmethod\n",
    "  def label_ids_to_probs(all_labels, vocabulary_size):\n",
    "    probabilities = list()\n",
    "    for label in all_labels:\n",
    "      p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "      p[0, label] = 1.0\n",
    "      probabilities.append(p)\n",
    "    return probabilities\n",
    "    \n",
    "def words(probabilities, dictionary=reverse_dictionary):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [dictionary[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2sentence(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [' '.join(x) for x in zip(s, words(b))]\n",
    "  return s\n",
    "\n",
    "def sequences2sentence(sequences, input1_size):\n",
    "  if input1_size > 0: # we have training inputs\n",
    "    s = [''] * np.array(sequences).shape[0]\n",
    "  else:\n",
    "    s = [''] * np.array(sequences).shape[1]\n",
    "    \n",
    "  for idx, b in enumerate(sequences):\n",
    "    if input1_size == 0: # we have labels\n",
    "      b = np.concatenate(b)\n",
    "    if idx < input1_size:\n",
    "      converted_words = [reverse_dictionary[id] for id in b]\n",
    "    else:\n",
    "      converted_words = [x_reverse_dictionary[id] for id in b]\n",
    "    s = [' '.join(x) for x in zip(s, converted_words)]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, num_unrollings)\n",
    "\n",
    "#print batches2sentence(train_batches.next())\n",
    "#print batches2sentence(train_batches.next())\n",
    "t_batches = train_batches.next()\n",
    "print batches2sentence(t_batches)\n",
    "train_inputs = BatchGenerator.create_input_sequence(t_batches, batch_size, num_unrollings, num_unrollings + 1)\n",
    "print sequences2sentence(train_inputs, num_unrollings)\n",
    "t_labels = BatchGenerator.create_label_sequence(train_inputs, batch_size, num_unrollings, num_unrollings + 1)\n",
    "print sequences2sentence(t_labels, 0)\n",
    "print \"Validation:\"\n",
    "#print batches2sentence(valid_batches.next())\n",
    "#print batches2sentence(valid_batches.next())\n",
    "v_batch = valid_batches.next()\n",
    "print batches2sentence(v_batch)\n",
    "valid_inputs = BatchGenerator.create_input_sequence(v_batch, 1, num_unrollings, num_unrollings + 1)\n",
    "print sequences2sentence(valid_inputs, num_unrollings)\n",
    "v_labels = BatchGenerator.create_label_sequence(valid_inputs, 1, num_unrollings, num_unrollings + 1)\n",
    "print sequences2sentence(v_labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "num_steps = 12000\n",
    "number_of_layers = 4\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "sentence_length = num_unrollings\n",
    "\n",
    "train_input_size = 2*sentence_length + 1\n",
    "label_input_size = sentence_length + 1\n",
    "\n",
    "train1_input_size = sentence_length\n",
    "train2_input_size = train_input_size - train1_input_size\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Dropout\n",
    "  keep_prob = tf.placeholder(tf.float32) \n",
    "  \n",
    "  # Parameters:    \n",
    "  # Definition of the LSTM cells\n",
    "  lstm = rnn_cell.BasicLSTMCell(num_nodes)\n",
    "  if keep_prob < 1:\n",
    "      lstm = rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "  stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes * (2*number_of_layers)]), trainable=False)\n",
    "  \n",
    "  # Embedding variables\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  x_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_labels = list()\n",
    "  \n",
    "  # Define input & label variables\n",
    "  for x in xrange(train_input_size):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    \n",
    "  for x in xrange(label_input_size):  \n",
    "    train_labels.append(tf.placeholder(tf.int32, shape=[batch_size, 1]))\n",
    "  \n",
    "  # Convert the input variables into embeddings\n",
    "  encoded_inputs = list()\n",
    "  \n",
    "  # Encoding the input sequence\n",
    "  for i in xrange(train1_input_size):\n",
    "    words_batch = train_data[i]\n",
    "    embed = tf.nn.embedding_lookup(embeddings, words_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "\n",
    "  # Encoding the output sequence\n",
    "  for i in xrange(train1_input_size, train_input_size):\n",
    "    words_batch = train_data[i]\n",
    "    embed = tf.nn.embedding_lookup(x_embeddings, words_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "  train_inputs = encoded_inputs\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  state = saved_state\n",
    "  output = saved_output\n",
    "\n",
    "  # we want the following mapping: A B C D # -> W X Y Z .\n",
    "  with tf.variable_scope(\"LSTM-encoder\") as scope:\n",
    "    # input sequence\n",
    "    for i in xrange(train1_input_size):\n",
    "      if i > 0: scope.reuse_variables()\n",
    "      output, state = stacked_lstm(train_inputs[i], state)\n",
    "      \n",
    "  with tf.variable_scope(\"LSTM-decoder\") as scope:\n",
    "    for i in xrange(train1_input_size, train_input_size):\n",
    "      if i > train1_input_size: scope.reuse_variables()\n",
    "      output, state = stacked_lstm(train_inputs[i], state)\n",
    "      outputs.append(output)\n",
    "    \n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    \n",
    "    # input transformation\n",
    "    all_inputs = tf.concat(0, outputs)\n",
    "    w_t = tf.transpose(w)\n",
    "    # output transformation\n",
    "    all_labels = tf.concat(0, train_labels)\n",
    "    \n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sampled_softmax_loss(w_t, b, all_inputs, all_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, num_steps / 2, 0.1, staircase=False)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "   \n",
    "  # Sampling and validation eval\n",
    "  sample_inputs = list()\n",
    "  for i in xrange(sentence_length):\n",
    "    sample_inputs.append(tf.placeholder(tf.int32, shape=[1]))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes * (2*number_of_layers)]), trainable=False)\n",
    "  saved_translation_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "  saved_translation_state = tf.Variable(tf.zeros([1, num_nodes * (2*number_of_layers)]), trainable=False)\n",
    "  translation_input = tf.placeholder(tf.int32, shape=[1], name=\"translation_input\")\n",
    "  \n",
    "  reset_sample_state = tf.group(\n",
    "    saved_translation_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes * (2*number_of_layers)])),\n",
    "    saved_translation_state.assign(tf.zeros([1, num_nodes * (2*number_of_layers)])))\n",
    "  \n",
    "  sample_state = saved_sample_state\n",
    "  with tf.variable_scope(\"LSTM-encoder\", reuse=True) as scope:\n",
    "    for sample_input in sample_inputs:\n",
    "      sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "      sample_output, sample_state = stacked_lstm(sample_embed, sample_state)\n",
    "      \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state),\n",
    "                                  saved_translation_state.assign(sample_state)]):\n",
    "      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(saved_sample_output, w, b))\n",
    "  \n",
    "  with tf.variable_scope(\"LSTM-decoder\", reuse=True) as scope:\n",
    "    translation_embed = tf.nn.embedding_lookup(x_embeddings, translation_input)\n",
    "    translation_output, translation_state = stacked_lstm(translation_embed, saved_translation_state)\n",
    "\n",
    "    with tf.control_dependencies([saved_translation_output.assign(translation_output),\n",
    "                                  saved_translation_state.assign(translation_state)]):\n",
    "        sample_translation_prediction = tf.nn.softmax(tf.nn.xw_plus_b(saved_translation_output, w, b))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 7.87271595001 learning rate: 10.0\n",
      "Minibatch perplexity: 24970.00\n",
      "Validation set perplexity: 13395.98\n",
      "[' there although abolished be # eb dehsiloba hguohtla ereht']\n",
      "[' eb dehsiloba hguohtla ereht .']\n",
      " irodilop modfres slabmyc muiseac tseilrae\n",
      "Average loss at step 100 : 5.01342333317 learning rate: 9.62351\n",
      "Minibatch perplexity: 565.34\n",
      "Validation set perplexity: 621.12\n",
      "[' still is term the # eht mret si llits']\n",
      "[' eht mret si llits .']\n",
      " eht saw suomafni sexelfer .\n",
      "Average loss at step 200 : 3.7636866498 learning rate: 9.26119\n",
      "Minibatch perplexity: 561.61\n",
      "Validation set perplexity: 572.21\n",
      "[' anarchists most as anarchy # yhcrana sa tsom stsihcrana']\n",
      "[' yhcrana sa tsom stsihcrana .']\n",
      " snoitaterpretni sa dps ta .\n",
      "Average loss at step 300 : 3.45306317806 learning rate: 8.91251\n",
      "Minibatch perplexity: 287.69\n",
      "Validation set perplexity: 372.00\n",
      "[' by label positive a # a evitisop lebal yb']\n",
      "[' a evitisop lebal yb .']\n",
      " reh ecniv noiram taht .\n",
      "Average loss at step 400 : 3.32632471085 learning rate: 8.57696\n",
      "Minibatch perplexity: 264.82\n",
      "Validation set perplexity: 313.31\n",
      "[' UNK economic coercive and # dna evicreoc cimonoce UNK']\n",
      "[' dna evicreoc cimonoce UNK .']\n",
      " dna si sa gnua .\n",
      "Average loss at step 500 : 2.97541534662 learning rate: 8.25404\n",
      "Minibatch perplexity: 165.26\n",
      "Validation set perplexity: 224.55\n",
      "[' should and unnecessary are # era yrassecennu dna dluohs']\n",
      "[' era yrassecennu dna dluohs .']\n",
      " yrrac ne eman sedilsdnal .\n",
      "Average loss at step 600 : 2.77188336849 learning rate: 7.94328\n",
      "Minibatch perplexity: 160.84\n",
      "Validation set perplexity: 215.72\n",
      "[' whilst revolution french the # eht hcnerf noitulover tslihw']\n",
      "[' eht hcnerf noitulover tslihw .']\n",
      " eht tnuoma lla raey .\n",
      "Average loss at step 700 : 2.68049433231 learning rate: 7.64422\n",
      "Minibatch perplexity: 141.69\n",
      "Validation set perplexity: 214.62\n",
      "[' word the state the # eht etats eht drow']\n",
      "[' eht etats eht drow .']\n",
      " fo eht na daetsni .\n",
      "Average loss at step 800 : 2.55481001377 learning rate: 7.35642\n",
      "Minibatch perplexity: 133.29\n",
      "Validation set perplexity: 160.98\n",
      "[' as up taken been # neeb nekat pu sa']\n",
      "[' neeb nekat pu sa .']\n",
      " nospmis ygoloib sa etis .\n",
      "Average loss at step 900 : 2.49153245926 learning rate: 7.07946\n",
      "Minibatch perplexity: 170.92\n",
      "Validation set perplexity: 262.37\n",
      "[' structures political authoritarian as # sa nairatirohtua lacitilop serutcurts']\n",
      "[' sa nairatirohtua lacitilop serutcurts .']\n",
      " rof no eb sleuf .\n",
      "Average loss at step 1000 : 2.37343674183 learning rate: 6.81292\n",
      "Minibatch perplexity: 133.69\n",
      "Validation set perplexity: 141.26\n",
      "[' rulers that belief the # eht feileb taht srelur']\n",
      "[' eht feileb taht srelur .']\n",
      " eht desolcne grubmah dna .\n",
      "Average loss at step 1100 : 2.28579842329 learning rate: 6.55642\n",
      "Minibatch perplexity: 99.43\n",
      "Validation set perplexity: 112.47\n",
      "[' of UNK sans the # eht snas UNK fo']\n",
      "[' eht snas UNK fo .']\n",
      " eht devresbo UNK fo .\n",
      "Average loss at step 1200 : 2.22982787251 learning rate: 6.30957\n",
      "Minibatch perplexity: 91.58\n",
      "Validation set perplexity: 97.57\n",
      "[' particularly institutions authoritarian of # fo nairatirohtua snoitutitsni ylralucitrap']\n",
      "[' fo nairatirohtua snoitutitsni ylralucitrap .']\n",
      " fo ik enivri stniopdne .\n",
      "Average loss at step 1300 : 2.12286323428 learning rate: 6.07202\n",
      "Minibatch perplexity: 79.31\n",
      "Validation set perplexity: 84.12\n",
      "[' also has it society # yteicos ti sah osla']\n",
      "[' yteicos ti sah osla .']\n",
      " na hcihw ti krahs .\n",
      "Average loss at step 1400 : 2.08283868194 learning rate: 5.84341\n",
      "Minibatch perplexity: 115.41\n",
      "Validation set perplexity: 79.46\n",
      "[' regarded are what of # fo tahw era dedrager']\n",
      "[' fo tahw era dedrager .']\n",
      " ni ps s lamina .\n",
      "Average loss at step 1500 : 2.02085856438 learning rate: 5.62341\n",
      "Minibatch perplexity: 83.21\n",
      "Validation set perplexity: 77.42\n",
      "[' is philosophy political a # a lacitilop yhposolihp si']\n",
      "[' a lacitilop yhposolihp si .']\n",
      " a gnidih tnigautpes si .\n",
      "Average loss at step 1600 : 1.9580625689 learning rate: 5.4117\n",
      "Minibatch perplexity: 75.49\n",
      "Validation set perplexity: 74.71\n",
      "[' and revolution english the # eht hsilgne noitulover dna']\n",
      "[' eht hsilgne noitulover dna .']\n",
      " eht eugra rueuqil dna .\n",
      "Average loss at step 1700 : 1.91524463296 learning rate: 5.20795\n",
      "Minibatch perplexity: 84.04\n",
      "Validation set perplexity: 72.71\n",
      "[' elimination the advocate that # taht etacovda eht noitanimile']\n",
      "[' taht etacovda eht noitanimile .']\n",
      " sa sworht eht segamad .\n",
      "Average loss at step 1800 : 1.88059636474 learning rate: 5.01187\n",
      "Minibatch perplexity: 95.62\n",
      "Validation set perplexity: 65.19\n",
      "[' of organization the destroy # yortsed eht noitazinagro fo']\n",
      "[' yortsed eht noitazinagro fo .']\n",
      " setats eht an fo .\n",
      "Average loss at step 1900 : 1.84441624999 learning rate: 4.82318\n",
      "Minibatch perplexity: 75.32\n",
      "Validation set perplexity: 68.94\n",
      "[' place in society authoritarian # nairatirohtua yteicos ni ecalp']\n",
      "[' nairatirohtua yteicos ni ecalp .']\n",
      " ecirp ibocaj fo renraw .\n",
      "Average loss at step 2000 : 1.82470018387 learning rate: 4.64159\n",
      "Minibatch perplexity: 53.69\n",
      "Validation set perplexity: 63.52\n",
      "[' as anarchism king chief # feihc gnik msihcrana sa']\n",
      "[' feihc gnik msihcrana sa .']\n",
      " scihpargomed desolc seid a .\n",
      "Average loss at step 2100 : 1.79285936594 learning rate: 4.46684\n",
      "Minibatch perplexity: 46.33\n",
      "Validation set perplexity: 67.33\n",
      "[' of UNK the including # gnidulcni eht UNK fo']\n",
      "[' gnidulcni eht UNK fo .']\n",
      " snoitaler eht UNK fo .\n",
      "Average loss at step 2200 : 1.81868811488 learning rate: 4.29866\n",
      "Minibatch perplexity: 103.81\n",
      "Validation set perplexity: 63.37\n",
      "[' movements social related to # ot detaler laicos stnemevom']\n",
      "[' ot detaler laicos stnemevom .']\n",
      " morf lanoitan noitseuq dluow .\n",
      "Average loss at step 2300 : 1.77906255245 learning rate: 4.13682\n",
      "Minibatch perplexity: 48.95\n",
      "Validation set perplexity: 59.78\n",
      "[' to means violent used # desu tneloiv snaem ot']\n",
      "[' desu tneloiv snaem ot .']\n",
      " ynam ylbuod stsisnoc ot .\n",
      "Average loss at step 2400 : 1.82034759641 learning rate: 3.98107\n",
      "Minibatch perplexity: 70.57\n",
      "Validation set perplexity: 67.74\n",
      "[' anti UNK a rather # rehtar a UNK itna']\n",
      "[' rehtar a UNK itna .']\n",
      " lamron a UNK rewoprepus .\n",
      "Average loss at step 2500 : 1.77090494514 learning rate: 3.83119\n",
      "Minibatch perplexity: 66.02\n",
      "Validation set perplexity: 59.39\n",
      "[' ruler UNK without greek # keerg tuohtiw UNK relur']\n",
      "[' keerg tuohtiw UNK relur .']\n",
      " noitcnuj suolucarim UNK b .\n",
      "Average loss at step 2600 : 1.73615173578 learning rate: 3.68694\n",
      "Minibatch perplexity: 68.48\n",
      "Validation set perplexity: 65.88\n",
      "[' radicals class working early # ylrae gnikrow ssalc slacidar']\n",
      "[' ylrae gnikrow ssalc slacidar .']\n",
      " god smlif tluc siht .\n",
      "Average loss at step 2700 : 1.6913597095 learning rate: 3.54813\n",
      "Minibatch perplexity: 60.52\n",
      "Validation set perplexity: 53.08\n",
      "[' refers also anarchism means # snaem msihcrana osla srefer']\n",
      "[' snaem msihcrana osla srefer .']\n",
      " strac ynapmoc siht tluser .\n",
      "Average loss at step 2800 : 1.65993891835 learning rate: 3.41455\n",
      "Minibatch perplexity: 45.74\n",
      "Validation set perplexity: 59.70\n",
      "[' that act any describe # ebircsed yna tca taht']\n",
      "[' ebircsed yna tca taht .']\n",
      " ssenisub edam ecudorp s .\n",
      "Average loss at step 2900 : 1.63208816886 learning rate: 3.28599\n",
      "Minibatch perplexity: 45.40\n",
      "Validation set perplexity: 57.69\n",
      "[' but UNK or nihilism # msilihin ro UNK tub']\n",
      "[' msilihin ro UNK tub .']\n",
      " hsals taht UNK si .\n",
      "Average loss at step 3000 : 1.6484124136 learning rate: 3.16228\n",
      "Minibatch perplexity: 32.16\n",
      "Validation set perplexity: 58.53\n",
      "[' the from derived is # si devired morf eht']\n",
      "[' si devired morf eht .']\n",
      " era ecaps morf eht .\n",
      "Average loss at step 3100 : 1.61609930396 learning rate: 3.04322\n",
      "Minibatch perplexity: 59.79\n",
      "Validation set perplexity: 64.21\n",
      "[' against used first abuse # esuba tsrif desu tsniaga']\n",
      "[' esuba tsrif desu tsniaga .']\n",
      " erbmit yromem emit ynam .\n",
      "Average loss at step 3200 : 1.60197394252 learning rate: 2.92864\n",
      "Minibatch perplexity: 43.77\n",
      "Validation set perplexity: 60.26\n",
      "[' this what of interpretations # snoitaterpretni fo tahw siht']\n",
      "[' snoitaterpretni fo tahw siht .']\n",
      " dellac fo tsom ti .\n",
      "Average loss at step 3300 : 1.58223735332 learning rate: 2.81838\n",
      "Minibatch perplexity: 63.95\n",
      "Validation set perplexity: 62.14\n",
      "[' to way pejorative a # a evitarojep yaw ot']\n",
      "[' a evitarojep yaw ot .']\n",
      " a nadroj stluser ot .\n",
      "Average loss at step 3400 : 1.55549856544 learning rate: 2.71227\n",
      "Minibatch perplexity: 35.78\n",
      "Validation set perplexity: 63.31\n",
      "[' chaos imply not does # seod ton ylpmi soahc']\n",
      "[' seod ton ylpmi soahc .']\n",
      " dnuof ton selbbub lla .\n",
      "Average loss at step 3500 : 1.55678801298 learning rate: 2.61016\n",
      "Minibatch perplexity: 35.86\n",
      "Validation set perplexity: 56.33\n",
      "[' anarchism word the anarchists # stsihcrana eht drow msihcrana']\n",
      "[' stsihcrana eht drow msihcrana .']\n",
      " ydna eht itna noitar .\n",
      "Average loss at step 3600 : 1.54698302627 learning rate: 2.51189\n",
      "Minibatch perplexity: 34.63\n",
      "Validation set perplexity: 57.64\n",
      "[' of term a as # sa a mret fo']\n",
      "[' sa a mret fo .']\n",
      " sa a deniamer fo .\n",
      "Average loss at step 3700 : 1.54507676721 learning rate: 2.41732\n",
      "Minibatch perplexity: 34.56\n",
      "Validation set perplexity: 58.79\n",
      "[' differing are there although # hguohtla ereht era gnireffid']\n",
      "[' hguohtla ereht era gnireffid .']\n",
      " erom siht ti won .\n",
      "Average loss at step 3800 : 1.51164700031 learning rate: 2.32631\n",
      "Minibatch perplexity: 40.77\n",
      "Validation set perplexity: 58.74\n",
      "[' in used still is # si llits desu ni']\n",
      "[' si llits desu ni .']\n",
      " si erom hguohtla neht fo\n",
      "Average loss at step 3900 : 1.5448738575 learning rate: 2.23872\n",
      "Minibatch perplexity: 38.60\n",
      "Validation set perplexity: 58.59\n",
      "[' it use anarchists most # tsom stsihcrana esu ti']\n",
      "[' tsom stsihcrana esu ti .']\n",
      " sih de tuo eb .\n",
      "Average loss at step 4000 : 1.53288653374 learning rate: 2.15443\n",
      "Minibatch perplexity: 45.61\n",
      "Validation set perplexity: 58.77\n",
      "[' defined self by label # lebal yb fles denifed']\n",
      "[' lebal yb fles denifed .']\n",
      " tsaoc yb mubla rebmun .\n",
      "Average loss at step 4100 : 1.49269812346 learning rate: 2.07332\n",
      "Minibatch perplexity: 29.85\n",
      "Validation set perplexity: 60.35\n",
      "[' originated anarchism UNK economic # cimonoce UNK msihcrana detanigiro']\n",
      "[' cimonoce UNK msihcrana detanigiro .']\n",
      " cigam UNK enizagam yltaerg .\n",
      "Average loss at step 4200 : 1.49755701184 learning rate: 1.99526\n",
      "Minibatch perplexity: 42.02\n",
      "Validation set perplexity: 53.46\n",
      "[' abolished be should and # dna dluohs eb dehsiloba']\n",
      "[' dna dluohs eb dehsiloba .']\n",
      " dna retaw ta devieced .\n",
      "Average loss at step 4300 : 1.49777709007 learning rate: 1.92014\n",
      "Minibatch perplexity: 36.13\n",
      "Validation set perplexity: 52.49\n",
      "[' term the whilst revolution # noitulover tslihw eht mret']\n",
      "[' noitulover tslihw eht mret .']\n",
      " sisylana bocaj eht lrac .\n",
      "Average loss at step 4400 : 1.45863694787 learning rate: 1.84785\n",
      "Minibatch perplexity: 37.34\n",
      "Validation set perplexity: 53.14\n",
      "[' as anarchy word the # eht drow yhcrana sa']\n",
      "[' eht drow yhcrana sa .']\n",
      " eht eci rrac sa .\n",
      "Average loss at step 4500 : 1.44298712611 learning rate: 1.77828\n",
      "Minibatch perplexity: 32.54\n",
      "Validation set perplexity: 48.33\n",
      "[' positive a as up # pu sa a evitisop']\n",
      "[' pu sa a evitisop .']\n",
      " krow sa a gnimraw .\n",
      "Average loss at step 4600 : 1.43603643775 learning rate: 1.71133\n",
      "Minibatch perplexity: 29.34\n",
      "Validation set perplexity: 56.25\n",
      "[' coercive and structures political # lacitilop serutcurts dna evicreoc']\n",
      "[' lacitilop serutcurts dna evicreoc .']\n",
      " knab aibiman dna ybsorc .\n",
      "Average loss at step 4700 : 1.43269280553 learning rate: 1.6469\n",
      "Minibatch perplexity: 38.84\n",
      "Validation set perplexity: 49.78\n",
      "[' unnecessary are rulers that # taht srelur era yrassecennu']\n",
      "[' taht srelur era yrassecennu .']\n",
      " sa egattoc hcihw nevarc .\n",
      "Average loss at step 4800 : 1.45195884109 learning rate: 1.58489\n",
      "Minibatch perplexity: 42.51\n",
      "Validation set perplexity: 50.66\n",
      "[' french the of UNK # UNK fo eht hcnerf']\n",
      "[' UNK fo eht hcnerf .']\n",
      " UNK fo eht tsom .\n",
      "Average loss at step 4900 : 1.46826552391 learning rate: 1.52522\n",
      "Minibatch perplexity: 45.58\n",
      "Validation set perplexity: 53.36\n",
      "[' state the particularly institutions # snoitutitsni ylralucitrap eht etats']\n",
      "[' snoitutitsni ylralucitrap eht etats .']\n",
      " elibomotua dias eht dlrow .\n",
      "Average loss at step 5000 : 1.47432622671 learning rate: 1.4678\n",
      "Minibatch perplexity: 41.92\n",
      "Validation set perplexity: 47.43\n",
      "[' taken been also has # sah osla neeb nekat']\n",
      "[' sah osla neeb nekat .']\n",
      " tub osla evah dia .\n",
      "Average loss at step 5100 : 1.45554333687 learning rate: 1.41254\n",
      "Minibatch perplexity: 48.74\n",
      "Validation set perplexity: 50.84\n",
      "[' authoritarian as regarded are # era dedrager sa nairatirohtua']\n",
      "[' era dedrager sa nairatirohtua .']\n",
      " era yrarbil sa sratat .\n",
      "Average loss at step 5200 : 1.461446383 learning rate: 1.35936\n",
      "Minibatch perplexity: 32.17\n",
      "Validation set perplexity: 47.10\n",
      "[' belief the is philosophy # yhposolihp si eht feileb']\n",
      "[' yhposolihp si eht feileb .']\n",
      " resopmoc si eht yrotirret .\n",
      "Average loss at step 5300 : 1.4441193378 learning rate: 1.30818\n",
      "Minibatch perplexity: 34.73\n",
      "Validation set perplexity: 51.57\n",
      "[' sans the and revolution # noitulover dna eht snas']\n",
      "[' noitulover dna eht snas .']\n",
      " detpecca dna eht esenapaj .\n",
      "Average loss at step 5400 : 1.45121120572 learning rate: 1.25893\n",
      "Minibatch perplexity: 35.19\n",
      "Validation set perplexity: 51.15\n",
      "[' authoritarian of elimination the # eht noitanimile fo nairatirohtua']\n",
      "[' eht noitanimile fo nairatirohtua .']\n",
      " eht swal fo ylralucitrap .\n",
      "Average loss at step 5500 : 1.46194524288 learning rate: 1.21153\n",
      "Minibatch perplexity: 30.63\n",
      "Validation set perplexity: 47.58\n",
      "[' it society of organization # noitazinagro fo yteicos ti']\n",
      "[' noitazinagro fo yteicos ti .']\n",
      " stseirp fo dniheb era .\n",
      "Average loss at step 5600 : 1.41353656173 learning rate: 1.16591\n",
      "Minibatch perplexity: 29.14\n",
      "Validation set perplexity: 48.76\n",
      "[' what of place in # ni ecalp fo tahw']\n",
      "[' ni ecalp fo tahw .']\n",
      " ni egaugnal fo lla .\n",
      "Average loss at step 5700 : 1.41764210939 learning rate: 1.12202\n",
      "Minibatch perplexity: 36.75\n",
      "Validation set perplexity: 46.05\n",
      "[' political a as anarchism # msihcrana sa a lacitilop']\n",
      "[' msihcrana sa a lacitilop .']\n",
      " etisbew sa a stnemirepxe .\n",
      "Average loss at step 5800 : 1.40988982797 learning rate: 1.07978\n",
      "Minibatch perplexity: 37.33\n",
      "Validation set perplexity: 48.82\n",
      "[' english the of UNK # UNK fo eht hsilgne']\n",
      "[' UNK fo eht hsilgne .']\n",
      " UNK fo eht rehto .\n",
      "Average loss at step 5900 : 1.4280164516 learning rate: 1.03912\n",
      "Minibatch perplexity: 30.65\n",
      "Validation set perplexity: 47.86\n",
      "[' advocate that movements social # laicos stnemevom taht etacovda']\n",
      "[' laicos stnemevom taht etacovda .']\n",
      " ylediw kroy taht decap .\n",
      "Average loss at step 6000 : 1.42247266173 learning rate: 1.0\n",
      "Minibatch perplexity: 39.29\n",
      "Validation set perplexity: 47.41\n",
      "[' the destroy to means # snaem ot yortsed eht']\n",
      "[' snaem ot yortsed eht .']\n",
      " neve morf cihpargoeg eht .\n",
      "Average loss at step 6100 : 1.41338173985 learning rate: 0.962351\n",
      "Minibatch perplexity: 27.60\n",
      "Validation set perplexity: 49.10\n",
      "[' society authoritarian anti UNK # UNK itna nairatirohtua yteicos']\n",
      "[' UNK itna nairatirohtua yteicos .']\n",
      " UNK rotamina ecyoj nehw .\n",
      "Average loss at step 6200 : 1.4034008348 learning rate: 0.926119\n",
      "Minibatch perplexity: 45.79\n",
      "Validation set perplexity: 46.82\n",
      "[' king chief ruler UNK # UNK relur feihc gnik']\n",
      "[' UNK relur feihc gnik .']\n",
      " UNK detceleer xdr pihsnoipmahc .\n",
      "Average loss at step 6300 : 1.42866121411 learning rate: 0.891251\n",
      "Minibatch perplexity: 23.71\n",
      "Validation set perplexity: 50.00\n",
      "[' the including radicals class # ssalc slacidar gnidulcni eht']\n",
      "[' ssalc slacidar gnidulcni eht .']\n",
      " cibara gnirehtag ecrof eht .\n",
      "Average loss at step 6400 : 1.36669682026 learning rate: 0.857696\n",
      "Minibatch perplexity: 25.36\n",
      "Validation set perplexity: 46.76\n",
      "[' related to refers also # osla srefer ot detaler']\n",
      "[' osla srefer ot detaler .']\n",
      " sih laoc ot dluoc .\n",
      "Average loss at step 6500 : 1.42301573277 learning rate: 0.825404\n",
      "Minibatch perplexity: 42.45\n",
      "Validation set perplexity: 46.98\n",
      "[' violent used that act # tca taht desu tneloiv']\n",
      "[' tca taht desu tneloiv .']\n",
      " suoigiler taht desu drof .\n",
      "Average loss at step 6600 : 1.42453902125 learning rate: 0.794328\n",
      "Minibatch perplexity: 59.77\n",
      "Validation set perplexity: 48.00\n",
      "[' a rather but UNK # UNK tub rehtar a']\n",
      "[' UNK tub rehtar a .']\n",
      " UNK eh neeb a .\n",
      "Average loss at step 6700 : 1.42158638 learning rate: 0.764422\n",
      "Minibatch perplexity: 39.97\n",
      "Validation set perplexity: 46.65\n",
      "[' without greek the from # morf eht keerg tuohtiw']\n",
      "[' morf eht keerg tuohtiw .']\n",
      " morf eht ygolonhcet seton airavab\n",
      "Average loss at step 6800 : 1.36049376965 learning rate: 0.735642\n",
      "Minibatch perplexity: 39.06\n",
      "Validation set perplexity: 48.25\n",
      "[' working early against used # desu tsniaga ylrae gnikrow']\n",
      "[' desu tsniaga ylrae gnikrow .']\n",
      " dlrow hsival f ymmarg .\n",
      "Average loss at step 6900 : 1.42506978393 learning rate: 0.707946\n",
      "Minibatch perplexity: 31.49\n",
      "Validation set perplexity: 45.38\n",
      "[' anarchism means this what # tahw siht snaem msihcrana']\n",
      "[' tahw siht snaem msihcrana .']\n",
      " emit evah swohs emoceb .\n",
      "Average loss at step 7000 : 1.41555729985 learning rate: 0.681292\n",
      "Minibatch perplexity: 27.16\n",
      "Validation set perplexity: 46.12\n",
      "[' any describe to way # yaw ot ebircsed yna']\n",
      "[' yaw ot ebircsed yna .']\n",
      " detinu ot stnemssessa nac .\n",
      "Average loss at step 7100 : 1.38094089508 learning rate: 0.655642\n",
      "Minibatch perplexity: 38.65\n",
      "Validation set perplexity: 46.88\n",
      "[' or nihilism chaos imply # ylpmi soahc msilihin ro']\n",
      "[' ylpmi soahc msilihin ro .']\n",
      " airohpue surypap dwerhs ro .\n",
      "Average loss at step 7200 : 1.37202007771 learning rate: 0.630957\n",
      "Minibatch perplexity: 31.11\n",
      "Validation set perplexity: 43.36\n",
      "[' derived is anarchism word # drow msihcrana si devired']\n",
      "[' drow msihcrana si devired .']\n",
      " am evitca si gnitroppus .\n",
      "Average loss at step 7300 : 1.41090450168 learning rate: 0.607202\n",
      "Minibatch perplexity: 39.57\n",
      "Validation set perplexity: 46.81\n",
      "[' first abuse of term # mret fo esuba tsrif']\n",
      "[' mret fo esuba tsrif .']\n",
      " ecnednepedni fo stceffe eporue .\n",
      "Average loss at step 7400 : 1.42234887004 learning rate: 0.584341\n",
      "Minibatch perplexity: 58.98\n",
      "Validation set perplexity: 44.10\n",
      "[' of interpretations differing are # era gnireffid snoitaterpretni fo']\n",
      "[' era gnireffid snoitaterpretni fo .']\n",
      " na snoitaralced eobo fo .\n",
      "Average loss at step 7500 : 1.4097327292 learning rate: 0.562341\n",
      "Minibatch perplexity: 54.09\n",
      "Validation set perplexity: 44.39\n",
      "[' pejorative a in used # desu ni a evitarojep']\n",
      "[' desu ni a evitarojep .']\n",
      " eh ni a dnaltocs .\n",
      "Average loss at step 7600 : 1.3966845727 learning rate: 0.54117\n",
      "Minibatch perplexity: 33.98\n",
      "Validation set perplexity: 44.98\n",
      "[' not does it use # esu ti seod ton']\n",
      "[' esu ti seod ton .']\n",
      " erom ti desab eb .\n",
      "Average loss at step 7700 : 1.38984656096 learning rate: 0.520795\n",
      "Minibatch perplexity: 32.84\n",
      "Validation set perplexity: 43.38\n",
      "[' the anarchists defined self # fles denifed stsihcrana eht']\n",
      "[' fles denifed stsihcrana eht .']\n",
      " laremacib tnanimod nur eht .\n",
      "Average loss at step 7800 : 1.41506750941 learning rate: 0.501187\n",
      "Minibatch perplexity: 37.04\n",
      "Validation set perplexity: 45.66\n",
      "[' a as originated anarchism # msihcrana detanigiro sa a']\n",
      "[' msihcrana detanigiro sa a .']\n",
      " nigiro hparg sa a .\n",
      "Average loss at step 7900 : 1.39409599662 learning rate: 0.482318\n",
      "Minibatch perplexity: 33.95\n",
      "Validation set perplexity: 42.62\n",
      "[' there although abolished be # eb dehsiloba hguohtla ereht']\n",
      "[' eb dehsiloba hguohtla ereht .']\n",
      " tub regnis dluow dah .\n",
      "Average loss at step 8000 : 1.39555009961 learning rate: 0.464159\n",
      "Minibatch perplexity: 40.05\n",
      "Validation set perplexity: 44.24\n",
      "[' still is term the # eht mret si llits']\n",
      "[' eht mret si llits .']\n",
      " eht eltit si dah .\n",
      "Average loss at step 8100 : 1.41293171287 learning rate: 0.446684\n",
      "Minibatch perplexity: 48.03\n",
      "Validation set perplexity: 44.37\n",
      "[' anarchists most as anarchy # yhcrana sa tsom stsihcrana']\n",
      "[' yhcrana sa tsom stsihcrana .']\n",
      " aran sa lla cirtcele .\n",
      "Average loss at step 8200 : 1.40792993426 learning rate: 0.429866\n",
      "Minibatch perplexity: 35.80\n",
      "Validation set perplexity: 42.48\n",
      "[' by label positive a # a evitisop lebal yb']\n",
      "[' a evitisop lebal yb .']\n",
      " a modsiw sdlofinam saw .\n",
      "Average loss at step 8300 : 1.38576469064 learning rate: 0.413682\n",
      "Minibatch perplexity: 31.51\n",
      "Validation set perplexity: 47.82\n",
      "[' UNK economic coercive and # dna evicreoc cimonoce UNK']\n",
      "[' dna evicreoc cimonoce UNK .']\n",
      " owt sretroper inu UNK .\n",
      "Average loss at step 8400 : 1.40175881505 learning rate: 0.398107\n",
      "Minibatch perplexity: 41.58\n",
      "Validation set perplexity: 42.96\n",
      "[' should and unnecessary are # era yrassecennu dna dluohs']\n",
      "[' era yrassecennu dna dluohs .']\n",
      " hcihw sad dna detsixe .\n",
      "Average loss at step 8500 : 1.38760483146 learning rate: 0.383119\n",
      "Minibatch perplexity: 32.45\n",
      "Validation set perplexity: 45.41\n",
      "[' whilst revolution french the # eht hcnerf noitulover tslihw']\n",
      "[' eht hcnerf noitulover tslihw .']\n",
      " eht sci yksnivarts pu .\n",
      "Average loss at step 8600 : 1.38391952157 learning rate: 0.368695\n",
      "Minibatch perplexity: 24.09\n",
      "Validation set perplexity: 44.51\n",
      "[' word the state the # eht etats eht drow']\n",
      "[' eht etats eht drow .']\n",
      " eht hgih eht gnidivorp .\n",
      "Average loss at step 8700 : 1.39050106883 learning rate: 0.354813\n",
      "Minibatch perplexity: 46.24\n",
      "Validation set perplexity: 42.67\n",
      "[' as up taken been # neeb nekat pu sa']\n",
      "[' neeb nekat pu sa .']\n",
      " rehto eilyk b sa .\n",
      "Average loss at step 8800 : 1.38935830951 learning rate: 0.341455\n",
      "Minibatch perplexity: 50.63\n",
      "Validation set perplexity: 47.61\n",
      "[' structures political authoritarian as # sa nairatirohtua lacitilop serutcurts']\n",
      "[' sa nairatirohtua lacitilop serutcurts .']\n",
      " sa slaudividni laitnetop erofeb .\n",
      "Average loss at step 8900 : 1.39366784573 learning rate: 0.328599\n",
      "Minibatch perplexity: 31.36\n",
      "Validation set perplexity: 42.67\n",
      "[' rulers that belief the # eht feileb taht srelur']\n",
      "[' eht feileb taht srelur .']\n",
      " eht telracs taht etats .\n",
      "Average loss at step 9000 : 1.40068729162 learning rate: 0.316228\n",
      "Minibatch perplexity: 43.05\n",
      "Validation set perplexity: 45.29\n",
      "[' of UNK sans the # eht snas UNK fo']\n",
      "[' eht snas UNK fo .']\n",
      " eht erapmoc UNK fo .\n",
      "Average loss at step 9100 : 1.41268490791 learning rate: 0.304322\n",
      "Minibatch perplexity: 41.57\n",
      "Validation set perplexity: 45.76\n",
      "[' particularly institutions authoritarian of # fo nairatirohtua snoitutitsni ylralucitrap']\n",
      "[' fo nairatirohtua snoitutitsni ylralucitrap .']\n",
      " fo sessecorp gniniatnoc noitamrifnoc .\n",
      "Average loss at step 9200 : 1.37361092806 learning rate: 0.292864\n",
      "Minibatch perplexity: 33.25\n",
      "Validation set perplexity: 41.80\n",
      "[' also has it society # yteicos ti sah osla']\n",
      "[' yteicos ti sah osla .']\n",
      " tcejbo era hcus emos .\n",
      "Average loss at step 9300 : 1.40106100202 learning rate: 0.281838\n",
      "Minibatch perplexity: 40.37\n",
      "Validation set perplexity: 44.58\n",
      "[' regarded are what of # fo tahw era dedrager']\n",
      "[' fo tahw era dedrager .']\n",
      " fo esu era roiretna .\n",
      "Average loss at step 9400 : 1.36860591888 learning rate: 0.271227\n",
      "Minibatch perplexity: 40.34\n",
      "Validation set perplexity: 43.27\n",
      "[' is philosophy political a # a lacitilop yhposolihp si']\n",
      "[' a lacitilop yhposolihp si .']\n",
      " a tnenamrep syobwoc hcihw .\n",
      "Average loss at step 9500 : 1.36967439771 learning rate: 0.261016\n",
      "Minibatch perplexity: 39.81\n",
      "Validation set perplexity: 44.30\n",
      "[' and revolution english the # eht hsilgne noitulover dna']\n",
      "[' eht hsilgne noitulover dna .']\n",
      " eht wen ylluf dna .\n",
      "Average loss at step 9600 : 1.35906975389 learning rate: 0.251189\n",
      "Minibatch perplexity: 31.68\n",
      "Validation set perplexity: 42.85\n",
      "[' elimination the advocate that # taht etacovda eht noitanimile']\n",
      "[' taht etacovda eht noitanimile .']\n",
      " sa enitnegra eht noitnem .\n",
      "Average loss at step 9700 : 1.36224645257 learning rate: 0.241732\n",
      "Minibatch perplexity: 31.80\n",
      "Validation set perplexity: 42.38\n",
      "[' of organization the destroy # yortsed eht noitazinagro fo']\n",
      "[' yortsed eht noitazinagro fo .']\n",
      " lagel eht etar fo .\n",
      "Average loss at step 9800 : 1.37467066765 learning rate: 0.232631\n",
      "Minibatch perplexity: 38.94\n",
      "Validation set perplexity: 44.09\n",
      "[' place in society authoritarian # nairatirohtua yteicos ni ecalp']\n",
      "[' nairatirohtua yteicos ni ecalp .']\n",
      " yloh erutaef ni sti .\n",
      "Average loss at step 9900 : 1.36669747949 learning rate: 0.223872\n",
      "Minibatch perplexity: 40.07\n",
      "Validation set perplexity: 42.43\n",
      "[' as anarchism king chief # feihc gnik msihcrana sa']\n",
      "[' feihc gnik msihcrana sa .']\n",
      " tsrif amaso cimonoce sa .\n",
      "Average loss at step 10000 : 1.38236701846 learning rate: 0.215443\n",
      "Minibatch perplexity: 43.68\n",
      "Validation set perplexity: 45.32\n",
      "[' of UNK the including # gnidulcni eht UNK fo']\n",
      "[' gnidulcni eht UNK fo .']\n",
      " hcruhc eht UNK fo .\n",
      "Average loss at step 10100 : 1.39477718115 learning rate: 0.207332\n",
      "Minibatch perplexity: 33.91\n",
      "Validation set perplexity: 43.68\n",
      "[' movements social related to # ot detaler laicos stnemevom']\n",
      "[' ot detaler laicos stnemevom .']\n",
      " ot ecivres reven tih .\n",
      "Average loss at step 10200 : 1.38111672163 learning rate: 0.199526\n",
      "Minibatch perplexity: 26.23\n",
      "Validation set perplexity: 41.41\n",
      "[' to means violent used # desu tneloiv snaem ot']\n",
      "[' desu tneloiv snaem ot .']\n",
      " hcus nimajneb seitinummoc ot .\n",
      "Average loss at step 10300 : 1.40758478642 learning rate: 0.192014\n",
      "Minibatch perplexity: 47.61\n",
      "Validation set perplexity: 42.96\n",
      "[' anti UNK a rather # rehtar a UNK itna']\n",
      "[' rehtar a UNK itna .']\n",
      " denifed a UNK deirramer .\n",
      "Average loss at step 10400 : 1.38602342844 learning rate: 0.184785\n",
      "Minibatch perplexity: 33.58\n",
      "Validation set perplexity: 41.36\n",
      "[' ruler UNK without greek # keerg tuohtiw UNK relur']\n",
      "[' keerg tuohtiw UNK relur .']\n",
      " noitagitil elpram UNK semit .\n",
      "Average loss at step 10500 : 1.37593224883 learning rate: 0.177828\n",
      "Minibatch perplexity: 29.36\n",
      "Validation set perplexity: 45.23\n",
      "[' radicals class working early # ylrae gnikrow ssalc slacidar']\n",
      "[' ylrae gnikrow ssalc slacidar .']\n",
      " ylrae evitartsinimda l etorw .\n",
      "Average loss at step 10600 : 1.40902106524 learning rate: 0.171133\n",
      "Minibatch perplexity: 37.22\n",
      "Validation set perplexity: 40.80\n",
      "[' refers also anarchism means # snaem msihcrana osla srefer']\n",
      "[' snaem msihcrana osla srefer .']\n",
      " ria lanigiro rehto deviecer .\n",
      "Average loss at step 10700 : 1.40704200625 learning rate: 0.16469\n",
      "Minibatch perplexity: 38.10\n",
      "Validation set perplexity: 41.04\n",
      "[' that act any describe # ebircsed yna tca taht']\n",
      "[' ebircsed yna tca taht .']\n",
      " mret did uoy sa .\n",
      "Average loss at step 10800 : 1.37666960597 learning rate: 0.158489\n",
      "Minibatch perplexity: 36.91\n",
      "Validation set perplexity: 44.46\n",
      "[' but UNK or nihilism # msilihin ro UNK tub']\n",
      "[' msilihin ro UNK tub .']\n",
      " tsegral ro UNK taht .\n",
      "Average loss at step 10900 : 1.3790129137 learning rate: 0.152522\n",
      "Minibatch perplexity: 49.44\n",
      "Validation set perplexity: 40.79\n",
      "[' the from derived is # si devired morf eht']\n",
      "[' si devired morf eht .']\n",
      " na atsiv morf eht .\n",
      "Average loss at step 11000 : 1.34860536456 learning rate: 0.14678\n",
      "Minibatch perplexity: 37.53\n",
      "Validation set perplexity: 44.31\n",
      "[' against used first abuse # esuba tsrif desu tsniaga']\n",
      "[' esuba tsrif desu tsniaga .']\n",
      " tsrif tuc eh emit .\n",
      "Average loss at step 11100 : 1.31883851767 learning rate: 0.141254\n",
      "Minibatch perplexity: 32.58\n",
      "Validation set perplexity: 41.74\n",
      "[' this what of interpretations # snoitaterpretni fo tahw siht']\n",
      "[' snoitaterpretni fo tahw siht .']\n",
      " acirfa fo os siht .\n",
      "Average loss at step 11200 : 1.36213645816 learning rate: 0.135936\n",
      "Minibatch perplexity: 36.27\n",
      "Validation set perplexity: 43.57\n",
      "[' to way pejorative a # a evitarojep yaw ot']\n",
      "[' a evitarojep yaw ot .']\n",
      " a teerts secitcarp morf .\n",
      "Average loss at step 11300 : 1.36496537447 learning rate: 0.130818\n",
      "Minibatch perplexity: 59.13\n",
      "Validation set perplexity: 44.98\n",
      "[' chaos imply not does # seod ton ylpmi soahc']\n",
      "[' seod ton ylpmi soahc .']\n",
      " nredom ton skcih gnidnuorrus .\n",
      "Average loss at step 11400 : 1.3664254272 learning rate: 0.125893\n",
      "Minibatch perplexity: 35.81\n",
      "Validation set perplexity: 40.95\n",
      "[' anarchism word the anarchists # stsihcrana eht drow msihcrana']\n",
      "[' stsihcrana eht drow msihcrana .']\n",
      " stifeneb eht tsrif tnetxe .\n",
      "Average loss at step 11500 : 1.39737539291 learning rate: 0.121153\n",
      "Minibatch perplexity: 38.34\n",
      "Validation set perplexity: 42.65\n",
      "[' of term a as # sa a mret fo']\n",
      "[' sa a mret fo .']\n",
      " sa a lacitilop fo .\n",
      "Average loss at step 11600 : 1.40489958167 learning rate: 0.116591\n",
      "Minibatch perplexity: 35.57\n",
      "Validation set perplexity: 40.77\n",
      "[' differing are there although # hguohtla ereht era gnireffid']\n",
      "[' hguohtla ereht era gnireffid .']\n",
      " yna yeht era depoleved .\n",
      "Average loss at step 11700 : 1.39102664232 learning rate: 0.112202\n",
      "Minibatch perplexity: 38.12\n",
      "Validation set perplexity: 41.14\n",
      "[' in used still is # si llits desu ni']\n",
      "[' si llits desu ni .']\n",
      " si reven saw ni .\n",
      "Average loss at step 11800 : 1.33985105276 learning rate: 0.107978\n",
      "Minibatch perplexity: 25.44\n",
      "Validation set perplexity: 43.10\n",
      "[' it use anarchists most # tsom stsihcrana esu ti']\n",
      "[' tsom stsihcrana esu ti .']\n",
      " eseht ylerem nac era .\n",
      "Average loss at step 11900 : 1.37952561259 learning rate: 0.103912\n",
      "Minibatch perplexity: 37.87\n",
      "Validation set perplexity: 41.58\n",
      "[' defined self by label # lebal yb fles denifed']\n",
      "[' lebal yb fles denifed .']\n",
      " nam yb etunim i .\n"
     ]
    }
   ],
   "source": [
    "summary_frequency = 100\n",
    "sample_words_count = 39\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print 'Initialized'\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    all_inputs = BatchGenerator.create_input_sequence(batches, batch_size,\n",
    "                                                      sentence_length, sentence_length + 1)\n",
    "    all_labels = BatchGenerator.create_label_sequence(all_inputs, batch_size,\n",
    "                                                      sentence_length, sentence_length + 1)\n",
    "    \n",
    "    # setup inputs\n",
    "    for i, sequence in enumerate(all_inputs):\n",
    "      feed_dict[train_data[i]] = sequence\n",
    "    \n",
    "    # setup outputs\n",
    "    for i, sequence in enumerate(all_labels):\n",
    "      feed_dict[train_labels[i]] = sequence\n",
    "    \n",
    "    # setup dropout\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print 'Average loss at step', step, ':', mean_loss, 'learning rate:', lr\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(all_labels)\n",
    "      labels = BatchGenerator.label_ids_to_probs(labels, vocabulary_size)\n",
    "      labels = np.concatenate(labels)\n",
    "      print 'Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))\n",
    "\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      last_sequence = ''\n",
    "      last_labels = ''\n",
    "      last_prediction = ''\n",
    "      min_perplexity = sys.maxint\n",
    "      valid_iterations = valid_size / 7\n",
    "      for _ in xrange(valid_iterations):\n",
    "        reset_sample_state.run()\n",
    "        \n",
    "        # feeding the input sequence to be translated\n",
    "        v_batch = valid_batches.next()\n",
    "        v_input_ids = BatchGenerator.create_input_sequence(v_batch, 1, sentence_length, sentence_length + 1)\n",
    "        sample_dict = dict()\n",
    "        for i, v_input in enumerate(v_input_ids[:sentence_length]):\n",
    "          sample_dict[sample_inputs[i]] = v_input\n",
    "        sample_dict[keep_prob] = 1.0\n",
    "        v_predictions = list()\n",
    "        prob_predictions = list()\n",
    "        prediction = sample_prediction.eval(sample_dict)\n",
    "        \n",
    "        # starting the translation by inputing the \"GO\" sign\n",
    "        sentence = ''\n",
    "        go_symbol = dictionary[go_sign]\n",
    "        feed = [go_symbol]\n",
    "        for _ in xrange(sentence_length + 1):\n",
    "          prediction = sample_translation_prediction.eval({translation_input: feed, keep_prob: 1.0})\n",
    "          feed = sample(prediction)\n",
    "          sentence += ' ' + words([feed], x_reverse_dictionary)[0]\n",
    "          feed = probs_to_ids([feed])\n",
    "          v_predictions.append(feed)\n",
    "          prob_predictions.append(prediction)\n",
    "            \n",
    "        # convert labels into probabilities so we can measure perplexity\n",
    "        v_label_ids = BatchGenerator.create_label_sequence(v_input_ids, 1, sentence_length, sentence_length + 1)\n",
    "        v_labels = BatchGenerator.label_ids_to_probs(v_label_ids, vocabulary_size)\n",
    "        v_labels = np.concatenate(v_labels)\n",
    "        prob_predictions = np.concatenate(prob_predictions)\n",
    "        \n",
    "        # get the best case as a display sample\n",
    "        perplexity = logprob(prob_predictions, v_labels)\n",
    "        if perplexity < min_perplexity:\n",
    "          min_perplexity = perplexity\n",
    "        \n",
    "        last_sequence = sequences2sentence(v_input_ids, sentence_length)\n",
    "        last_labels = sequences2sentence(v_label_ids, 0)\n",
    "        last_prediction = sentence\n",
    "        valid_logprob = valid_logprob + perplexity\n",
    "      print 'Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_iterations))\n",
    "      print last_sequence\n",
    "      print last_labels\n",
    "      print last_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
